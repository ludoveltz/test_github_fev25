{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqtNigGmfsNyUqHadidBYN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ludoveltz/test_github_fev25/blob/main/Daily_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uoM-dGzolVA",
        "outputId": "5d2547c1-e898-44c0-b6d2-4f824442bec4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š AperÃ§u du dataset:\n",
            "Dimensions du dataset: (8, 4)\n",
            "Nombre d'articles: 8\n",
            "\n",
            "ğŸ“‘ Colonnes prÃ©sentes:\n",
            "- article_id: 8 valeurs non-nulles, type: int64\n",
            "- article_title: 8 valeurs non-nulles, type: object\n",
            "- article_text: 8 valeurs non-nulles, type: object\n",
            "- source: 8 valeurs non-nulles, type: object\n",
            "\n",
            "ğŸ“ Exemple d'article:\n",
            "\n",
            "Titre: I do not have friends inÂ tennis, says Maria Sharapova\n",
            "Source: https://www.tennisworldusa.org/tennis/news/Maria_Sharapova/62220/i-do-not-have-friends-in-tennis-says-maria-sharapova/\n",
            "Texte (premiers 300 caractÃ¨res):\n",
            "Maria Sharapova has basically no friends as tennis players on the WTA Tour. The Russian player has no problems in openly speaking about it and in a recent interview she said: 'I don't really hide any feelings too much. I think everyone knows this is my job here. When I'm on the courts or when I'm on...\n",
            "\n",
            "ğŸ§¹ Dataset nettoyÃ©:\n",
            "- Colonnes conservÃ©es: ['article_text']\n",
            "- Nombre d'articles: 8\n",
            "\n",
            "ğŸ“ˆ Statistiques sur la longueur des articles:\n",
            "- Longueur moyenne: 1967 caractÃ¨res\n",
            "- Longueur minimale: 1080 caractÃ¨res\n",
            "- Longueur maximale: 4674 caractÃ¨res\n",
            "\n",
            "ğŸ’¾ Dataset nettoyÃ© sauvegardÃ© dans 'tennis_articles_cleaned.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    # Lecture du fichier avec l'encodage correct\n",
        "    df = pd.read_csv('tennis_articles.csv', encoding='latin1')\n",
        "\n",
        "    # Exploration initiale\n",
        "    print(\"\\nğŸ“Š AperÃ§u du dataset:\")\n",
        "    print(f\"Dimensions du dataset: {df.shape}\")\n",
        "    print(f\"Nombre d'articles: {len(df)}\")\n",
        "\n",
        "    print(\"\\nğŸ“‘ Colonnes prÃ©sentes:\")\n",
        "    for col in df.columns:\n",
        "        non_null = df[col].count()\n",
        "        dtype = df[col].dtype\n",
        "        print(f\"- {col}: {non_null} valeurs non-nulles, type: {dtype}\")\n",
        "\n",
        "    print(\"\\nğŸ“ Exemple d'article:\")\n",
        "    example = df.iloc[0]\n",
        "    print(f\"\\nTitre: {example['article_title']}\")\n",
        "    print(f\"Source: {example['source']}\")\n",
        "    print(f\"Texte (premiers 300 caractÃ¨res):\\n{example['article_text'][:300]}...\")\n",
        "\n",
        "    # Nettoyage - garder uniquement article_text\n",
        "    df_cleaned = df[['article_text']]\n",
        "\n",
        "    print(\"\\nğŸ§¹ Dataset nettoyÃ©:\")\n",
        "    print(f\"- Colonnes conservÃ©es: {df_cleaned.columns.tolist()}\")\n",
        "    print(f\"- Nombre d'articles: {len(df_cleaned)}\")\n",
        "\n",
        "    # Statistiques sur les articles\n",
        "    article_lengths = df_cleaned['article_text'].str.len()\n",
        "    print(\"\\nğŸ“ˆ Statistiques sur la longueur des articles:\")\n",
        "    print(f\"- Longueur moyenne: {article_lengths.mean():.0f} caractÃ¨res\")\n",
        "    print(f\"- Longueur minimale: {article_lengths.min():.0f} caractÃ¨res\")\n",
        "    print(f\"- Longueur maximale: {article_lengths.max():.0f} caractÃ¨res\")\n",
        "\n",
        "    # Sauvegarde du dataset nettoyÃ©\n",
        "    df_cleaned.to_csv('tennis_articles_cleaned.csv', index=False)\n",
        "    print(\"\\nğŸ’¾ Dataset nettoyÃ© sauvegardÃ© dans 'tennis_articles_cleaned.csv'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Erreur: {str(e)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "try:\n",
        "    # TÃ©lÃ©chargement complet des ressources NLTK nÃ©cessaires\n",
        "    print(\"â³ TÃ©lÃ©chargement des ressources NLTK...\")\n",
        "    nltk.download('punkt_tab')\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "    print(\"âœ… Ressources NLTK tÃ©lÃ©chargÃ©es\")\n",
        "\n",
        "    # Chargement du dataset nettoyÃ©\n",
        "    df = pd.read_csv('tennis_articles_cleaned.csv')\n",
        "\n",
        "    # Fonction de nettoyage du texte\n",
        "    def clean_text(text):\n",
        "        # Conversion en minuscules\n",
        "        text = text.lower()\n",
        "        # Suppression de la ponctuation\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        # Suppression des chiffres\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        # Suppression des espaces multiples\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    # Chargement des embeddings GloVe (utilisation du modÃ¨le 100d pour la rapiditÃ©)\n",
        "    print(\"\\nâ³ Chargement des embeddings GloVe...\")\n",
        "    embeddings_dict = {}\n",
        "    with open('glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_dict[word] = vector\n",
        "    print(\"âœ… Embeddings chargÃ©s\")\n",
        "\n",
        "    # PrÃ©traitement des articles\n",
        "    all_sentences = []\n",
        "    original_sentences = []\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    print(\"\\nğŸ”„ Traitement des articles...\")\n",
        "    for idx, article in enumerate(df['article_text'], 1):\n",
        "        print(f\"Traitement de l'article {idx}/{len(df)}...\")\n",
        "\n",
        "        try:\n",
        "            # Tokenization en phrases\n",
        "            sentences = sent_tokenize(article)\n",
        "            original_sentences.extend(sentences)\n",
        "\n",
        "            # Nettoyage et prÃ©traitement de chaque phrase\n",
        "            for sentence in sentences:\n",
        "                # Nettoyage\n",
        "                clean_sent = clean_text(sentence)\n",
        "                # Tokenization en mots\n",
        "                words = word_tokenize(clean_sent)\n",
        "                # Suppression des stop words\n",
        "                words = [word for word in words if word not in stop_words]\n",
        "                # Ajout Ã  la liste\n",
        "                all_sentences.append(words)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Erreur lors du traitement de l'article {idx}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"âœ… Nombre total de phrases traitÃ©es: {len(all_sentences)}\")\n",
        "\n",
        "    # Vectorisation des phrases\n",
        "    print(\"\\nâ³ Vectorisation des phrases...\")\n",
        "    sentence_vectors = []\n",
        "    for sentence in all_sentences:\n",
        "        if not sentence:\n",
        "            continue\n",
        "\n",
        "        word_vectors = []\n",
        "        for word in sentence:\n",
        "            if word in embeddings_dict:\n",
        "                word_vectors.append(embeddings_dict[word])\n",
        "\n",
        "        if word_vectors:\n",
        "            sentence_vector = np.mean(word_vectors, axis=0)\n",
        "            sentence_vectors.append(sentence_vector)\n",
        "        else:\n",
        "            sentence_vectors.append(np.zeros(100))\n",
        "\n",
        "    print(f\"âœ… Vectorisation terminÃ©e: {len(sentence_vectors)} vecteurs crÃ©Ã©s\")\n",
        "\n",
        "    # Sauvegarde des rÃ©sultats pour l'Ã©tape suivante\n",
        "    np.save('sentence_vectors.npy', np.array(sentence_vectors))\n",
        "    with open('original_sentences.txt', 'w', encoding='utf-8') as f:\n",
        "        for sent in original_sentences:\n",
        "            f.write(sent + '\\n')\n",
        "\n",
        "    print(\"\\nğŸ’¾ RÃ©sultats sauvegardÃ©s pour l'Ã©tape suivante\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Erreur gÃ©nÃ©rale: {str(e)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWfg3V0_qaBD",
        "outputId": "065654c6-c0fa-4c15-c668-019274f0f6d6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â³ TÃ©lÃ©chargement des ressources NLTK...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Ressources NLTK tÃ©lÃ©chargÃ©es\n",
            "\n",
            "â³ Chargement des embeddings GloVe...\n",
            "âœ… Embeddings chargÃ©s\n",
            "\n",
            "ğŸ”„ Traitement des articles...\n",
            "Traitement de l'article 1/8...\n",
            "Traitement de l'article 2/8...\n",
            "Traitement de l'article 3/8...\n",
            "Traitement de l'article 4/8...\n",
            "Traitement de l'article 5/8...\n",
            "Traitement de l'article 6/8...\n",
            "Traitement de l'article 7/8...\n",
            "Traitement de l'article 8/8...\n",
            "âœ… Nombre total de phrases traitÃ©es: 130\n",
            "\n",
            "â³ Vectorisation des phrases...\n",
            "âœ… Vectorisation terminÃ©e: 129 vecteurs crÃ©Ã©s\n",
            "\n",
            "ğŸ’¾ RÃ©sultats sauvegardÃ©s pour l'Ã©tape suivante\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "from scipy.spatial.distance import cosine\n",
        "import pickle\n",
        "\n",
        "try:\n",
        "    # Chargement des vecteurs\n",
        "    print(\"â³ Chargement des vecteurs...\")\n",
        "    sentence_vectors = np.load('sentence_vectors.npy')\n",
        "    print(f\"âœ… {len(sentence_vectors)} vecteurs chargÃ©s\")\n",
        "\n",
        "    # Calcul de la matrice de similaritÃ©\n",
        "    print(\"\\nâ³ Calcul de la matrice de similaritÃ©...\")\n",
        "    similarity_matrix = np.zeros((len(sentence_vectors), len(sentence_vectors)))\n",
        "\n",
        "    for i in range(len(sentence_vectors)):\n",
        "        for j in range(len(sentence_vectors)):\n",
        "            if i != j:\n",
        "                similarity = 1 - cosine(sentence_vectors[i], sentence_vectors[j])\n",
        "                similarity = max(0, similarity)\n",
        "                similarity_matrix[i][j] = similarity\n",
        "\n",
        "    print(\"âœ… Matrice de similaritÃ© calculÃ©e\")\n",
        "\n",
        "    # Statistiques de la matrice\n",
        "    print(\"\\nğŸ“Š Statistiques de similaritÃ©:\")\n",
        "    print(f\"- SimilaritÃ© moyenne: {np.mean(similarity_matrix):.3f}\")\n",
        "    print(f\"- SimilaritÃ© maximale: {np.max(similarity_matrix):.3f}\")\n",
        "    print(f\"- SimilaritÃ© minimale: {np.min(similarity_matrix):.3f}\")\n",
        "\n",
        "    # Construction du graphe avec seuil de similaritÃ©\n",
        "    print(\"\\nâ³ Construction du graphe avec seuil...\")\n",
        "    threshold = 0.7  # Seuil de similaritÃ© pour crÃ©er une arÃªte\n",
        "    graph = nx.Graph()\n",
        "\n",
        "    # Ajout des nÅ“uds\n",
        "    for i in range(len(sentence_vectors)):\n",
        "        graph.add_node(i)\n",
        "\n",
        "    # Ajout des arÃªtes avec poids au-dessus du seuil\n",
        "    for i in range(len(sentence_vectors)):\n",
        "        for j in range(i + 1, len(sentence_vectors)):\n",
        "            if similarity_matrix[i][j] > threshold:\n",
        "                graph.add_edge(i, j, weight=similarity_matrix[i][j])\n",
        "\n",
        "    # Statistiques du graphe\n",
        "    print(\"ğŸ“Š Statistiques du graphe:\")\n",
        "    print(f\"- Nombre de nÅ“uds: {graph.number_of_nodes()}\")\n",
        "    print(f\"- Nombre d'arÃªtes: {graph.number_of_edges()}\")\n",
        "    print(f\"- DensitÃ© du graphe: {nx.density(graph):.3f}\")\n",
        "\n",
        "    # Analyse des nÅ“uds centraux\n",
        "    print(\"\\nğŸ” Analyse des nÅ“uds centraux...\")\n",
        "    centrality = nx.degree_centrality(graph)\n",
        "    top_nodes = sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "\n",
        "    # Chargement des phrases originales\n",
        "    with open('original_sentences.txt', 'r', encoding='utf-8') as f:\n",
        "        original_sentences = f.readlines()\n",
        "\n",
        "    print(\"\\nğŸ“ˆ Top 5 des phrases les plus connectÃ©es:\")\n",
        "    for node_id, score in top_nodes:\n",
        "        print(f\"\\nScore de centralitÃ©: {score:.3f}\")\n",
        "        print(f\"Phrase: {original_sentences[node_id].strip()[:100]}...\")\n",
        "\n",
        "    # Sauvegarde des rÃ©sultats\n",
        "    print(\"\\nğŸ’¾ Sauvegarde des rÃ©sultats...\")\n",
        "    np.save('similarity_matrix.npy', similarity_matrix)\n",
        "    with open('sentence_graph.pkl', 'wb') as f:\n",
        "        pickle.dump(graph, f)\n",
        "    print(\"âœ… RÃ©sultats sauvegardÃ©s\")\n",
        "\n",
        "    # PrÃ©paration pour l'Ã©tape suivante (PageRank)\n",
        "    print(\"\\nğŸ”„ Le graphe est prÃªt pour l'application de PageRank\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Erreur: {str(e)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fuE2APEtLKB",
        "outputId": "65bf2e66-e2ec-4d3c-b04c-2fa26c116954"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â³ Chargement des vecteurs...\n",
            "âœ… 129 vecteurs chargÃ©s\n",
            "\n",
            "â³ Calcul de la matrice de similaritÃ©...\n",
            "âœ… Matrice de similaritÃ© calculÃ©e\n",
            "\n",
            "ğŸ“Š Statistiques de similaritÃ©:\n",
            "- SimilaritÃ© moyenne: 0.724\n",
            "- SimilaritÃ© maximale: 0.957\n",
            "- SimilaritÃ© minimale: 0.000\n",
            "\n",
            "â³ Construction du graphe avec seuil...\n",
            "ğŸ“Š Statistiques du graphe:\n",
            "- Nombre de nÅ“uds: 129\n",
            "- Nombre d'arÃªtes: 5504\n",
            "- DensitÃ© du graphe: 0.667\n",
            "\n",
            "ğŸ” Analyse des nÅ“uds centraux...\n",
            "\n",
            "ğŸ“ˆ Top 5 des phrases les plus connectÃ©es:\n",
            "\n",
            "Score de centralitÃ©: 0.938\n",
            "Phrase: So I'm not the one to strike up a conversation about the weather and know that in the next few minut...\n",
            "\n",
            "Score de centralitÃ©: 0.938\n",
            "Phrase: Â“It's a very pleasant atmosphere, I'd have to say, around the locker rooms....\n",
            "\n",
            "Score de centralitÃ©: 0.922\n",
            "Phrase: Roger Federer has revealed that organisers of the re-launched and condensed Davis Cup gave him three...\n",
            "\n",
            "Score de centralitÃ©: 0.922\n",
            "Phrase: Not because heÂ’d been out on a bender or anything Â— those days were in the past....\n",
            "\n",
            "Score de centralitÃ©: 0.922\n",
            "Phrase: HeÂ’d backed up his last-32 showingat Melbourne Park with a string of wins over elites including Fren...\n",
            "\n",
            "ğŸ’¾ Sauvegarde des rÃ©sultats...\n",
            "âœ… RÃ©sultats sauvegardÃ©s\n",
            "\n",
            "ğŸ”„ Le graphe est prÃªt pour l'application de PageRank\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "import pickle\n",
        "\n",
        "try:\n",
        "    # Chargement du graphe et des phrases originales\n",
        "    print(\"â³ Chargement des donnÃ©es...\")\n",
        "    with open('sentence_graph.pkl', 'rb') as f:\n",
        "        graph = pickle.load(f)\n",
        "\n",
        "    with open('original_sentences.txt', 'r', encoding='utf-8') as f:\n",
        "        original_sentences = f.readlines()\n",
        "    original_sentences = [s.strip() for s in original_sentences]\n",
        "\n",
        "    print(\"âœ… DonnÃ©es chargÃ©es\")\n",
        "\n",
        "    # Application de l'algorithme PageRank\n",
        "    print(\"\\nâ³ Application de PageRank...\")\n",
        "    pagerank_scores = nx.pagerank(graph, alpha=0.85)  # alpha est le facteur d'amortissement\n",
        "\n",
        "    # Tri des phrases selon leur score PageRank\n",
        "    ranked_sentences = [(score, idx) for idx, score in pagerank_scores.items()]\n",
        "    ranked_sentences.sort(reverse=True)\n",
        "\n",
        "    # ParamÃ¨tres du rÃ©sumÃ©\n",
        "    summary_size = 5  # Nombre de phrases pour le rÃ©sumÃ©\n",
        "\n",
        "    # Extraction des phrases les plus importantes\n",
        "    print(f\"\\nğŸ“ RÃ©sumÃ© gÃ©nÃ©rÃ© ({summary_size} phrases les plus importantes):\")\n",
        "    print(\"\\n-------------------\")\n",
        "\n",
        "    for i, (score, idx) in enumerate(ranked_sentences[:summary_size], 1):\n",
        "        print(f\"\\n{i}. Score PageRank: {score:.3f}\")\n",
        "        print(f\"Phrase: {original_sentences[idx]}\")\n",
        "\n",
        "    # Statistiques sur les scores PageRank\n",
        "    scores = [score for score, _ in ranked_sentences]\n",
        "    print(\"\\nğŸ“Š Statistiques PageRank:\")\n",
        "    print(f\"- Score moyen: {np.mean(scores):.3f}\")\n",
        "    print(f\"- Score maximal: {np.max(scores):.3f}\")\n",
        "    print(f\"- Score minimal: {np.min(scores):.3f}\")\n",
        "\n",
        "    # Sauvegarde du rÃ©sumÃ©\n",
        "    print(\"\\nğŸ’¾ Sauvegarde du rÃ©sumÃ©...\")\n",
        "    with open('tennis_articles_summary.txt', 'w', encoding='utf-8') as f:\n",
        "        f.write(\"RÃ©sumÃ© des articles de tennis\\n\")\n",
        "        f.write(\"===========================\\n\\n\")\n",
        "        for i, (score, idx) in enumerate(ranked_sentences[:summary_size], 1):\n",
        "            f.write(f\"{i}. {original_sentences[idx]}\\n\\n\")\n",
        "\n",
        "    print(\"âœ… RÃ©sumÃ© sauvegardÃ© dans 'tennis_articles_summary.txt'\")\n",
        "\n",
        "    # Analyse de la couverture thÃ©matique\n",
        "    print(\"\\nğŸ“ˆ Analyse de la distribution des phrases:\")\n",
        "    print(f\"- Nombre total de phrases: {len(original_sentences)}\")\n",
        "    print(f\"- Taux de compression: {(summary_size/len(original_sentences))*100:.1f}%\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Erreur: {str(e)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkzpTuwluMjg",
        "outputId": "db878d85-79ad-46ae-a07e-8fddab249f22"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â³ Chargement des donnÃ©es...\n",
            "âœ… DonnÃ©es chargÃ©es\n",
            "\n",
            "â³ Application de PageRank...\n",
            "\n",
            "ğŸ“ RÃ©sumÃ© gÃ©nÃ©rÃ© (5 phrases les plus importantes):\n",
            "\n",
            "-------------------\n",
            "\n",
            "1. Score PageRank: 0.011\n",
            "Phrase: Â“It's a very pleasant atmosphere, I'd have to say, around the locker rooms.\n",
            "\n",
            "2. Score PageRank: 0.011\n",
            "Phrase: So I'm not the one to strike up a conversation about the weather and know that in the next few minutes I have to go and try to win a tennis match.\n",
            "\n",
            "3. Score PageRank: 0.011\n",
            "Phrase: HeÂ’d backed up his last-32 showingat Melbourne Park with a string of wins over elites including French Open champion and then world No.9 Gaston Gaudio and Roland Garros runner-up Martin Verkerk in 2004 before illness struck.\n",
            "\n",
            "4. Score PageRank: 0.011\n",
            "Phrase: Roger Federer has revealed that organisers of the re-launched and condensed Davis Cup gave him three days to decide if he would commit to the controversial competition.\n",
            "\n",
            "5. Score PageRank: 0.011\n",
            "Phrase: Not because heÂ’d been out on a bender or anything Â— those days were in the past.\n",
            "\n",
            "ğŸ“Š Statistiques PageRank:\n",
            "- Score moyen: 0.008\n",
            "- Score maximal: 0.011\n",
            "- Score minimal: 0.001\n",
            "\n",
            "ğŸ’¾ Sauvegarde du rÃ©sumÃ©...\n",
            "âœ… RÃ©sumÃ© sauvegardÃ© dans 'tennis_articles_summary.txt'\n",
            "\n",
            "ğŸ“ˆ Analyse de la distribution des phrases:\n",
            "- Nombre total de phrases: 130\n",
            "- Taux de compression: 3.8%\n"
          ]
        }
      ]
    }
  ]
}