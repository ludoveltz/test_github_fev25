{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqtNigGmfsNyUqHadidBYN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ludoveltz/test_github_fev25/blob/main/Daily_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uoM-dGzolVA",
        "outputId": "5d2547c1-e898-44c0-b6d2-4f824442bec4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Aper√ßu du dataset:\n",
            "Dimensions du dataset: (8, 4)\n",
            "Nombre d'articles: 8\n",
            "\n",
            "üìë Colonnes pr√©sentes:\n",
            "- article_id: 8 valeurs non-nulles, type: int64\n",
            "- article_title: 8 valeurs non-nulles, type: object\n",
            "- article_text: 8 valeurs non-nulles, type: object\n",
            "- source: 8 valeurs non-nulles, type: object\n",
            "\n",
            "üìù Exemple d'article:\n",
            "\n",
            "Titre: I do not have friends in¬†tennis, says Maria Sharapova\n",
            "Source: https://www.tennisworldusa.org/tennis/news/Maria_Sharapova/62220/i-do-not-have-friends-in-tennis-says-maria-sharapova/\n",
            "Texte (premiers 300 caract√®res):\n",
            "Maria Sharapova has basically no friends as tennis players on the WTA Tour. The Russian player has no problems in openly speaking about it and in a recent interview she said: 'I don't really hide any feelings too much. I think everyone knows this is my job here. When I'm on the courts or when I'm on...\n",
            "\n",
            "üßπ Dataset nettoy√©:\n",
            "- Colonnes conserv√©es: ['article_text']\n",
            "- Nombre d'articles: 8\n",
            "\n",
            "üìà Statistiques sur la longueur des articles:\n",
            "- Longueur moyenne: 1967 caract√®res\n",
            "- Longueur minimale: 1080 caract√®res\n",
            "- Longueur maximale: 4674 caract√®res\n",
            "\n",
            "üíæ Dataset nettoy√© sauvegard√© dans 'tennis_articles_cleaned.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    # Lecture du fichier avec l'encodage correct\n",
        "    df = pd.read_csv('tennis_articles.csv', encoding='latin1')\n",
        "\n",
        "    # Exploration initiale\n",
        "    print(\"\\nüìä Aper√ßu du dataset:\")\n",
        "    print(f\"Dimensions du dataset: {df.shape}\")\n",
        "    print(f\"Nombre d'articles: {len(df)}\")\n",
        "\n",
        "    print(\"\\nüìë Colonnes pr√©sentes:\")\n",
        "    for col in df.columns:\n",
        "        non_null = df[col].count()\n",
        "        dtype = df[col].dtype\n",
        "        print(f\"- {col}: {non_null} valeurs non-nulles, type: {dtype}\")\n",
        "\n",
        "    print(\"\\nüìù Exemple d'article:\")\n",
        "    example = df.iloc[0]\n",
        "    print(f\"\\nTitre: {example['article_title']}\")\n",
        "    print(f\"Source: {example['source']}\")\n",
        "    print(f\"Texte (premiers 300 caract√®res):\\n{example['article_text'][:300]}...\")\n",
        "\n",
        "    # Nettoyage - garder uniquement article_text\n",
        "    df_cleaned = df[['article_text']]\n",
        "\n",
        "    print(\"\\nüßπ Dataset nettoy√©:\")\n",
        "    print(f\"- Colonnes conserv√©es: {df_cleaned.columns.tolist()}\")\n",
        "    print(f\"- Nombre d'articles: {len(df_cleaned)}\")\n",
        "\n",
        "    # Statistiques sur les articles\n",
        "    article_lengths = df_cleaned['article_text'].str.len()\n",
        "    print(\"\\nüìà Statistiques sur la longueur des articles:\")\n",
        "    print(f\"- Longueur moyenne: {article_lengths.mean():.0f} caract√®res\")\n",
        "    print(f\"- Longueur minimale: {article_lengths.min():.0f} caract√®res\")\n",
        "    print(f\"- Longueur maximale: {article_lengths.max():.0f} caract√®res\")\n",
        "\n",
        "    # Sauvegarde du dataset nettoy√©\n",
        "    df_cleaned.to_csv('tennis_articles_cleaned.csv', index=False)\n",
        "    print(\"\\nüíæ Dataset nettoy√© sauvegard√© dans 'tennis_articles_cleaned.csv'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur: {str(e)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "try:\n",
        "    # T√©l√©chargement complet des ressources NLTK n√©cessaires\n",
        "    print(\"‚è≥ T√©l√©chargement des ressources NLTK...\")\n",
        "    nltk.download('punkt_tab')\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "    print(\"‚úÖ Ressources NLTK t√©l√©charg√©es\")\n",
        "\n",
        "    # Chargement du dataset nettoy√©\n",
        "    df = pd.read_csv('tennis_articles_cleaned.csv')\n",
        "\n",
        "    # Fonction de nettoyage du texte\n",
        "    def clean_text(text):\n",
        "        # Conversion en minuscules\n",
        "        text = text.lower()\n",
        "        # Suppression de la ponctuation\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        # Suppression des chiffres\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        # Suppression des espaces multiples\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    # Chargement des embeddings GloVe (utilisation du mod√®le 100d pour la rapidit√©)\n",
        "    print(\"\\n‚è≥ Chargement des embeddings GloVe...\")\n",
        "    embeddings_dict = {}\n",
        "    with open('glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_dict[word] = vector\n",
        "    print(\"‚úÖ Embeddings charg√©s\")\n",
        "\n",
        "    # Pr√©traitement des articles\n",
        "    all_sentences = []\n",
        "    original_sentences = []\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    print(\"\\nüîÑ Traitement des articles...\")\n",
        "    for idx, article in enumerate(df['article_text'], 1):\n",
        "        print(f\"Traitement de l'article {idx}/{len(df)}...\")\n",
        "\n",
        "        try:\n",
        "            # Tokenization en phrases\n",
        "            sentences = sent_tokenize(article)\n",
        "            original_sentences.extend(sentences)\n",
        "\n",
        "            # Nettoyage et pr√©traitement de chaque phrase\n",
        "            for sentence in sentences:\n",
        "                # Nettoyage\n",
        "                clean_sent = clean_text(sentence)\n",
        "                # Tokenization en mots\n",
        "                words = word_tokenize(clean_sent)\n",
        "                # Suppression des stop words\n",
        "                words = [word for word in words if word not in stop_words]\n",
        "                # Ajout √† la liste\n",
        "                all_sentences.append(words)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur lors du traitement de l'article {idx}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"‚úÖ Nombre total de phrases trait√©es: {len(all_sentences)}\")\n",
        "\n",
        "    # Vectorisation des phrases\n",
        "    print(\"\\n‚è≥ Vectorisation des phrases...\")\n",
        "    sentence_vectors = []\n",
        "    for sentence in all_sentences:\n",
        "        if not sentence:\n",
        "            continue\n",
        "\n",
        "        word_vectors = []\n",
        "        for word in sentence:\n",
        "            if word in embeddings_dict:\n",
        "                word_vectors.append(embeddings_dict[word])\n",
        "\n",
        "        if word_vectors:\n",
        "            sentence_vector = np.mean(word_vectors, axis=0)\n",
        "            sentence_vectors.append(sentence_vector)\n",
        "        else:\n",
        "            sentence_vectors.append(np.zeros(100))\n",
        "\n",
        "    print(f\"‚úÖ Vectorisation termin√©e: {len(sentence_vectors)} vecteurs cr√©√©s\")\n",
        "\n",
        "    # Sauvegarde des r√©sultats pour l'√©tape suivante\n",
        "    np.save('sentence_vectors.npy', np.array(sentence_vectors))\n",
        "    with open('original_sentences.txt', 'w', encoding='utf-8') as f:\n",
        "        for sent in original_sentences:\n",
        "            f.write(sent + '\\n')\n",
        "\n",
        "    print(\"\\nüíæ R√©sultats sauvegard√©s pour l'√©tape suivante\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur g√©n√©rale: {str(e)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWfg3V0_qaBD",
        "outputId": "065654c6-c0fa-4c15-c668-019274f0f6d6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ T√©l√©chargement des ressources NLTK...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Ressources NLTK t√©l√©charg√©es\n",
            "\n",
            "‚è≥ Chargement des embeddings GloVe...\n",
            "‚úÖ Embeddings charg√©s\n",
            "\n",
            "üîÑ Traitement des articles...\n",
            "Traitement de l'article 1/8...\n",
            "Traitement de l'article 2/8...\n",
            "Traitement de l'article 3/8...\n",
            "Traitement de l'article 4/8...\n",
            "Traitement de l'article 5/8...\n",
            "Traitement de l'article 6/8...\n",
            "Traitement de l'article 7/8...\n",
            "Traitement de l'article 8/8...\n",
            "‚úÖ Nombre total de phrases trait√©es: 130\n",
            "\n",
            "‚è≥ Vectorisation des phrases...\n",
            "‚úÖ Vectorisation termin√©e: 129 vecteurs cr√©√©s\n",
            "\n",
            "üíæ R√©sultats sauvegard√©s pour l'√©tape suivante\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "from scipy.spatial.distance import cosine\n",
        "import pickle\n",
        "\n",
        "try:\n",
        "    # Chargement des vecteurs\n",
        "    print(\"‚è≥ Chargement des vecteurs...\")\n",
        "    sentence_vectors = np.load('sentence_vectors.npy')\n",
        "    print(f\"‚úÖ {len(sentence_vectors)} vecteurs charg√©s\")\n",
        "\n",
        "    # Calcul de la matrice de similarit√©\n",
        "    print(\"\\n‚è≥ Calcul de la matrice de similarit√©...\")\n",
        "    similarity_matrix = np.zeros((len(sentence_vectors), len(sentence_vectors)))\n",
        "\n",
        "    for i in range(len(sentence_vectors)):\n",
        "        for j in range(len(sentence_vectors)):\n",
        "            if i != j:\n",
        "                similarity = 1 - cosine(sentence_vectors[i], sentence_vectors[j])\n",
        "                similarity = max(0, similarity)\n",
        "                similarity_matrix[i][j] = similarity\n",
        "\n",
        "    print(\"‚úÖ Matrice de similarit√© calcul√©e\")\n",
        "\n",
        "    # Statistiques de la matrice\n",
        "    print(\"\\nüìä Statistiques de similarit√©:\")\n",
        "    print(f\"- Similarit√© moyenne: {np.mean(similarity_matrix):.3f}\")\n",
        "    print(f\"- Similarit√© maximale: {np.max(similarity_matrix):.3f}\")\n",
        "    print(f\"- Similarit√© minimale: {np.min(similarity_matrix):.3f}\")\n",
        "\n",
        "    # Construction du graphe avec seuil de similarit√©\n",
        "    print(\"\\n‚è≥ Construction du graphe avec seuil...\")\n",
        "    threshold = 0.7  # Seuil de similarit√© pour cr√©er une ar√™te\n",
        "    graph = nx.Graph()\n",
        "\n",
        "    # Ajout des n≈ìuds\n",
        "    for i in range(len(sentence_vectors)):\n",
        "        graph.add_node(i)\n",
        "\n",
        "    # Ajout des ar√™tes avec poids au-dessus du seuil\n",
        "    for i in range(len(sentence_vectors)):\n",
        "        for j in range(i + 1, len(sentence_vectors)):\n",
        "            if similarity_matrix[i][j] > threshold:\n",
        "                graph.add_edge(i, j, weight=similarity_matrix[i][j])\n",
        "\n",
        "    # Statistiques du graphe\n",
        "    print(\"üìä Statistiques du graphe:\")\n",
        "    print(f\"- Nombre de n≈ìuds: {graph.number_of_nodes()}\")\n",
        "    print(f\"- Nombre d'ar√™tes: {graph.number_of_edges()}\")\n",
        "    print(f\"- Densit√© du graphe: {nx.density(graph):.3f}\")\n",
        "\n",
        "    # Analyse des n≈ìuds centraux\n",
        "    print(\"\\nüîç Analyse des n≈ìuds centraux...\")\n",
        "    centrality = nx.degree_centrality(graph)\n",
        "    top_nodes = sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "\n",
        "    # Chargement des phrases originales\n",
        "    with open('original_sentences.txt', 'r', encoding='utf-8') as f:\n",
        "        original_sentences = f.readlines()\n",
        "\n",
        "    print(\"\\nüìà Top 5 des phrases les plus connect√©es:\")\n",
        "    for node_id, score in top_nodes:\n",
        "        print(f\"\\nScore de centralit√©: {score:.3f}\")\n",
        "        print(f\"Phrase: {original_sentences[node_id].strip()[:100]}...\")\n",
        "\n",
        "    # Sauvegarde des r√©sultats\n",
        "    print(\"\\nüíæ Sauvegarde des r√©sultats...\")\n",
        "    np.save('similarity_matrix.npy', similarity_matrix)\n",
        "    with open('sentence_graph.pkl', 'wb') as f:\n",
        "        pickle.dump(graph, f)\n",
        "    print(\"‚úÖ R√©sultats sauvegard√©s\")\n",
        "\n",
        "    # Pr√©paration pour l'√©tape suivante (PageRank)\n",
        "    print(\"\\nüîÑ Le graphe est pr√™t pour l'application de PageRank\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur: {str(e)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fuE2APEtLKB",
        "outputId": "65bf2e66-e2ec-4d3c-b04c-2fa26c116954"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Chargement des vecteurs...\n",
            "‚úÖ 129 vecteurs charg√©s\n",
            "\n",
            "‚è≥ Calcul de la matrice de similarit√©...\n",
            "‚úÖ Matrice de similarit√© calcul√©e\n",
            "\n",
            "üìä Statistiques de similarit√©:\n",
            "- Similarit√© moyenne: 0.724\n",
            "- Similarit√© maximale: 0.957\n",
            "- Similarit√© minimale: 0.000\n",
            "\n",
            "‚è≥ Construction du graphe avec seuil...\n",
            "üìä Statistiques du graphe:\n",
            "- Nombre de n≈ìuds: 129\n",
            "- Nombre d'ar√™tes: 5504\n",
            "- Densit√© du graphe: 0.667\n",
            "\n",
            "üîç Analyse des n≈ìuds centraux...\n",
            "\n",
            "üìà Top 5 des phrases les plus connect√©es:\n",
            "\n",
            "Score de centralit√©: 0.938\n",
            "Phrase: So I'm not the one to strike up a conversation about the weather and know that in the next few minut...\n",
            "\n",
            "Score de centralit√©: 0.938\n",
            "Phrase: ¬ìIt's a very pleasant atmosphere, I'd have to say, around the locker rooms....\n",
            "\n",
            "Score de centralit√©: 0.922\n",
            "Phrase: Roger Federer has revealed that organisers of the re-launched and condensed Davis Cup gave him three...\n",
            "\n",
            "Score de centralit√©: 0.922\n",
            "Phrase: Not because he¬íd been out on a bender or anything ¬ó those days were in the past....\n",
            "\n",
            "Score de centralit√©: 0.922\n",
            "Phrase: He¬íd backed up his last-32 showingat Melbourne Park with a string of wins over elites including Fren...\n",
            "\n",
            "üíæ Sauvegarde des r√©sultats...\n",
            "‚úÖ R√©sultats sauvegard√©s\n",
            "\n",
            "üîÑ Le graphe est pr√™t pour l'application de PageRank\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "import pickle\n",
        "\n",
        "try:\n",
        "    # Chargement du graphe et des phrases originales\n",
        "    print(\"‚è≥ Chargement des donn√©es...\")\n",
        "    with open('sentence_graph.pkl', 'rb') as f:\n",
        "        graph = pickle.load(f)\n",
        "\n",
        "    with open('original_sentences.txt', 'r', encoding='utf-8') as f:\n",
        "        original_sentences = f.readlines()\n",
        "    original_sentences = [s.strip() for s in original_sentences]\n",
        "\n",
        "    print(\"‚úÖ Donn√©es charg√©es\")\n",
        "\n",
        "    # Application de l'algorithme PageRank\n",
        "    print(\"\\n‚è≥ Application de PageRank...\")\n",
        "    pagerank_scores = nx.pagerank(graph, alpha=0.85)  # alpha est le facteur d'amortissement\n",
        "\n",
        "    # Tri des phrases selon leur score PageRank\n",
        "    ranked_sentences = [(score, idx) for idx, score in pagerank_scores.items()]\n",
        "    ranked_sentences.sort(reverse=True)\n",
        "\n",
        "    # Param√®tres du r√©sum√©\n",
        "    summary_size = 5  # Nombre de phrases pour le r√©sum√©\n",
        "\n",
        "    # Extraction des phrases les plus importantes\n",
        "    print(f\"\\nüìù R√©sum√© g√©n√©r√© ({summary_size} phrases les plus importantes):\")\n",
        "    print(\"\\n-------------------\")\n",
        "\n",
        "    for i, (score, idx) in enumerate(ranked_sentences[:summary_size], 1):\n",
        "        print(f\"\\n{i}. Score PageRank: {score:.3f}\")\n",
        "        print(f\"Phrase: {original_sentences[idx]}\")\n",
        "\n",
        "    # Statistiques sur les scores PageRank\n",
        "    scores = [score for score, _ in ranked_sentences]\n",
        "    print(\"\\nüìä Statistiques PageRank:\")\n",
        "    print(f\"- Score moyen: {np.mean(scores):.3f}\")\n",
        "    print(f\"- Score maximal: {np.max(scores):.3f}\")\n",
        "    print(f\"- Score minimal: {np.min(scores):.3f}\")\n",
        "\n",
        "    # Sauvegarde du r√©sum√©\n",
        "    print(\"\\nüíæ Sauvegarde du r√©sum√©...\")\n",
        "    with open('tennis_articles_summary.txt', 'w', encoding='utf-8') as f:\n",
        "        f.write(\"R√©sum√© des articles de tennis\\n\")\n",
        "        f.write(\"===========================\\n\\n\")\n",
        "        for i, (score, idx) in enumerate(ranked_sentences[:summary_size], 1):\n",
        "            f.write(f\"{i}. {original_sentences[idx]}\\n\\n\")\n",
        "\n",
        "    print(\"‚úÖ R√©sum√© sauvegard√© dans 'tennis_articles_summary.txt'\")\n",
        "\n",
        "    # Analyse de la couverture th√©matique\n",
        "    print(\"\\nüìà Analyse de la distribution des phrases:\")\n",
        "    print(f\"- Nombre total de phrases: {len(original_sentences)}\")\n",
        "    print(f\"- Taux de compression: {(summary_size/len(original_sentences))*100:.1f}%\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur: {str(e)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkzpTuwluMjg",
        "outputId": "db878d85-79ad-46ae-a07e-8fddab249f22"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Chargement des donn√©es...\n",
            "‚úÖ Donn√©es charg√©es\n",
            "\n",
            "‚è≥ Application de PageRank...\n",
            "\n",
            "üìù R√©sum√© g√©n√©r√© (5 phrases les plus importantes):\n",
            "\n",
            "-------------------\n",
            "\n",
            "1. Score PageRank: 0.011\n",
            "Phrase: ¬ìIt's a very pleasant atmosphere, I'd have to say, around the locker rooms.\n",
            "\n",
            "2. Score PageRank: 0.011\n",
            "Phrase: So I'm not the one to strike up a conversation about the weather and know that in the next few minutes I have to go and try to win a tennis match.\n",
            "\n",
            "3. Score PageRank: 0.011\n",
            "Phrase: He¬íd backed up his last-32 showingat Melbourne Park with a string of wins over elites including French Open champion and then world No.9 Gaston Gaudio and Roland Garros runner-up Martin Verkerk in 2004 before illness struck.\n",
            "\n",
            "4. Score PageRank: 0.011\n",
            "Phrase: Roger Federer has revealed that organisers of the re-launched and condensed Davis Cup gave him three days to decide if he would commit to the controversial competition.\n",
            "\n",
            "5. Score PageRank: 0.011\n",
            "Phrase: Not because he¬íd been out on a bender or anything ¬ó those days were in the past.\n",
            "\n",
            "üìä Statistiques PageRank:\n",
            "- Score moyen: 0.008\n",
            "- Score maximal: 0.011\n",
            "- Score minimal: 0.001\n",
            "\n",
            "üíæ Sauvegarde du r√©sum√©...\n",
            "‚úÖ R√©sum√© sauvegard√© dans 'tennis_articles_summary.txt'\n",
            "\n",
            "üìà Analyse de la distribution des phrases:\n",
            "- Nombre total de phrases: 130\n",
            "- Taux de compression: 3.8%\n"
          ]
        }
      ]
    }
  ]
}