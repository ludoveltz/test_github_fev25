{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTUplGLgZ4xgJMLMFijfq6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ludoveltz/test_github_fev25/blob/main/Daily_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports nécessaires\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from bs4 import SoupStrainer\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langgraph.graph import StateGraph\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from typing import TypedDict, List\n",
        "from langchain.prompts import PromptTemplate\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "\n",
        "# Activation de nest_asyncio pour Jupyter\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Chargement du document\n",
        "loader = WebBaseLoader(\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    bs_kwargs={\"parse_only\": SoupStrainer([\"title\", \"h1\", \"h2\", \"h3\", \"p\"])}\n",
        ")\n",
        "docs = loader.load()\n",
        "print(\"Document chargé avec succès!\")\n",
        "\n",
        "# Découpage du document\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=800,\n",
        "    chunk_overlap=150,\n",
        "    add_start_index=True\n",
        ")\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "print(f\"Nombre de chunks créés: {len(chunks)}\")\n",
        "\n",
        "# Configuration des embeddings\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': 'cpu'}\n",
        ")\n",
        "\n",
        "# Création du vectorstore\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embeddings\n",
        ")\n",
        "\n",
        "# Configuration du modèle de langage\n",
        "model_name = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "# Configuration du pipeline\n",
        "llm = HuggingFacePipeline(\n",
        "    pipeline=pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=128,\n",
        "        temperature=0.3,\n",
        "        do_sample=True,\n",
        "        no_repeat_ngram_size=3,\n",
        "        device=-1\n",
        "    )\n",
        ")\n",
        "\n",
        "# Définition de l'état RAG\n",
        "class RAGState(TypedDict):\n",
        "    question: str\n",
        "    context: List[str]\n",
        "    answer: str\n",
        "\n",
        "# Template du prompt\n",
        "rag_prompt = PromptTemplate.from_template(\"\"\"\n",
        "Based on the following context, provide a clear and concise answer.\n",
        "If you cannot answer based on the context, say so.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\")\n",
        "\n",
        "# Fonctions du pipeline\n",
        "def retrieve(state: RAGState) -> RAGState:\n",
        "    results = vectorstore.similarity_search(\n",
        "        state[\"question\"],\n",
        "        k=3\n",
        "    )\n",
        "    state[\"context\"] = [doc.page_content for doc in results]\n",
        "    return state\n",
        "\n",
        "# Le début du code reste identique jusqu'à la fonction generate\n",
        "\n",
        "def generate(state: RAGState) -> RAGState:\n",
        "    prompt = rag_prompt.format(\n",
        "        context=\"\\n\\n\".join(state[\"context\"]),\n",
        "        question=state[\"question\"]\n",
        "    )\n",
        "    # La sortie du modèle est une chaîne de caractères simple\n",
        "    response = llm(prompt)\n",
        "    # Nettoyage de la réponse\n",
        "    if isinstance(response, str):\n",
        "        state[\"answer\"] = response\n",
        "    elif isinstance(response, list):\n",
        "        state[\"answer\"] = response[0]['generated_text']\n",
        "    else:\n",
        "        state[\"answer\"] = str(response)\n",
        "    return state\n",
        "\n",
        "# Le reste du code reste identique jusqu'aux fonctions de test\n",
        "\n",
        "async def execute_pipeline():\n",
        "    print(\"=== Pipeline Execution ===\")\n",
        "\n",
        "    # Test basique\n",
        "    question = \"D'après l'article, comment les agents IA peuvent-ils être utilisés dans un contexte créatif?\"\n",
        "    try:\n",
        "        result = app.invoke({\n",
        "            \"question\": question,\n",
        "            \"context\": [],\n",
        "            \"answer\": \"\"\n",
        "        })\n",
        "        print(f\"\\nQuestion: {question}\")\n",
        "        print(f\"Réponse: {result['answer']}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors de l'exécution: {str(e)}\")\n",
        "\n",
        "    # Test avec streaming\n",
        "    print(\"\\nTest de streaming:\")\n",
        "    try:\n",
        "        async for step in app.astream(\n",
        "            {\n",
        "                \"question\": question,\n",
        "                \"context\": [],\n",
        "                \"answer\": \"\"\n",
        "            },\n",
        "            stream_mode=\"updates\"\n",
        "        ):\n",
        "            print(f\"Étape: {list(step.keys())}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors du streaming: {str(e)}\")\n",
        "\n",
        "def run_experiments():\n",
        "    print(\"\\n=== Évaluation et Expérimentation ===\")\n",
        "\n",
        "    # Questions adaptées au contexte\n",
        "    questions = [\n",
        "        \"Comment les agents IA peuvent-ils améliorer la production créative?\",\n",
        "        \"Quels sont les mécanismes de prise de décision des agents IA?\",\n",
        "        \"Quels sont les défis dans le développement d'agents IA pour la création de contenu?\"\n",
        "    ]\n",
        "\n",
        "    for question in questions:\n",
        "        try:\n",
        "            result = app.invoke({\n",
        "                \"question\": question,\n",
        "                \"context\": [],\n",
        "                \"answer\": \"\"\n",
        "            })\n",
        "            print(f\"\\nQuestion: {question}\")\n",
        "            print(f\"Réponse: {result['answer']}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur pour la question '{question}': {str(e)}\")\n",
        "\n",
        "    # Test des paramètres de chunking\n",
        "    print(\"\\nTest des paramètres de découpage:\")\n",
        "    for size in [500, 1000, 1500]:\n",
        "        overlap = size // 5\n",
        "        print(f\"\\nTaille de chunk: {size}, chevauchement: {overlap}\")\n",
        "        try:\n",
        "            splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=size,\n",
        "                chunk_overlap=overlap\n",
        "            )\n",
        "            chunks = splitter.split_documents(docs)\n",
        "            print(f\"Nombre de chunks: {len(chunks)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur avec la taille {size}: {str(e)}\")\n",
        "\n",
        "# Fonction principale avec gestion d'erreurs\n",
        "async def main():\n",
        "    try:\n",
        "        await execute_pipeline()\n",
        "        run_experiments()\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur dans l'exécution principale: {str(e)}\")\n",
        "\n",
        "# Pour exécuter dans Jupyter:\n",
        "await main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TxlCpPg7Y4-",
        "outputId": "c69c70c1-dea1-44ee-e2fb-fa2a41023b58"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document chargé avec succès!\n",
            "Nombre de chunks créés: 37\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Pipeline Execution ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Erreur lors de l'exécution: string indices must be integers, not 'str'\n",
            "\n",
            "Test de streaming:\n",
            "Étape: ['retrieve']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Erreur lors du streaming: string indices must be integers, not 'str'\n",
            "\n",
            "=== Évaluation et Expérimentation ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Erreur pour la question 'Comment les agents IA peuvent-ils améliorer la production créative?': string indices must be integers, not 'str'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Erreur pour la question 'Quels sont les mécanismes de prise de décision des agents IA?': string indices must be integers, not 'str'\n",
            "Erreur pour la question 'Quels sont les défis dans le développement d'agents IA pour la création de contenu?': string indices must be integers, not 'str'\n",
            "\n",
            "Test des paramètres de découpage:\n",
            "\n",
            "Taille de chunk: 500, chevauchement: 100\n",
            "Nombre de chunks: 58\n",
            "\n",
            "Taille de chunk: 1000, chevauchement: 200\n",
            "Nombre de chunks: 31\n",
            "\n",
            "Taille de chunk: 1500, chevauchement: 300\n",
            "Nombre de chunks: 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Imports nécessaires\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from bs4 import SoupStrainer\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langgraph.graph import StateGraph\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from typing import TypedDict, List\n",
        "from langchain.prompts import PromptTemplate\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "\n",
        "# 2. Document Loading\n",
        "print(\"=== Document Loading ===\")\n",
        "loader = WebBaseLoader(\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    bs_kwargs={\n",
        "        \"parse_only\": SoupStrainer([\"title\", \"h1\", \"h2\", \"h3\", \"p\"])\n",
        "    }\n",
        ")\n",
        "docs = loader.load()\n",
        "print(f\"Document loaded successfully!\")\n",
        "print(f\"Number of documents: {len(docs)}\")\n",
        "print(f\"First document preview: {docs[0].page_content[:200]}\")\n",
        "\n",
        "# 3. Document Splitting\n",
        "print(\"\\n=== Document Splitting ===\")\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=800,\n",
        "    chunk_overlap=150,\n",
        "    add_start_index=True\n",
        ")\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "print(f\"Number of chunks created: {len(chunks)}\")\n",
        "print(f\"First chunk preview: {chunks[0].page_content[:200]}\")\n",
        "\n",
        "# 4. Vector Store Indexing\n",
        "print(\"\\n=== Vector Store Indexing ===\")\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': 'cpu'}\n",
        ")\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embeddings\n",
        ")\n",
        "print(\"Vector store created successfully!\")\n",
        "\n",
        "# 5. LangGraph Pipeline Setup\n",
        "print(\"\\n=== LangGraph Pipeline Setup ===\")\n",
        "\n",
        "class RAGState(TypedDict):\n",
        "    question: str\n",
        "    context: List[str]\n",
        "    answer: str\n",
        "\n",
        "# Configuration du modèle\n",
        "model_name = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True)\n",
        "\n",
        "llm = HuggingFacePipeline(\n",
        "    pipeline=pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=128,\n",
        "        temperature=0.3,\n",
        "        do_sample=True,\n",
        "        no_repeat_ngram_size=3,\n",
        "        device=-1\n",
        "    )\n",
        ")\n",
        "\n",
        "# Prompt template comme spécifié dans l'exercice\n",
        "rag_prompt = PromptTemplate.from_template(\"\"\"\n",
        "Based on the following context, please provide a clear and concise answer.\n",
        "If the context doesn't contain relevant information, please say so.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\")\n",
        "\n",
        "def retrieve(state: RAGState) -> RAGState:\n",
        "    results = vectorstore.similarity_search(\n",
        "        state[\"question\"],\n",
        "        k=3\n",
        "    )\n",
        "    state[\"context\"] = [doc.page_content for doc in results]\n",
        "    return state\n",
        "\n",
        "def generate(state: RAGState) -> RAGState:\n",
        "    prompt = rag_prompt.format(\n",
        "        context=\"\\n\\n\".join(state[\"context\"]),\n",
        "        question=state[\"question\"]\n",
        "    )\n",
        "    response = llm(prompt)\n",
        "    state[\"answer\"] = response if isinstance(response, str) else response[0]['generated_text']\n",
        "    return state\n",
        "\n",
        "# Construction du graph\n",
        "workflow = StateGraph(RAGState)\n",
        "workflow.add_node(\"retrieve\", retrieve)\n",
        "workflow.add_node(\"generate\", generate)\n",
        "workflow.set_entry_point(\"retrieve\")\n",
        "workflow.add_edge(\"retrieve\", \"generate\")\n",
        "workflow.set_finish_point(\"generate\")\n",
        "\n",
        "# Compilation\n",
        "graph = workflow.compile()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2qgryb_8IME",
        "outputId": "09ff8197-030f-4261-aea4-60ea06c1b2d6"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Document Loading ===\n",
            "Document loaded successfully!\n",
            "Number of documents: 1\n",
            "First document preview: LLM Powered Autonomous Agents | Lil'Log\n",
            "      LLM Powered Autonomous Agents\n",
            "    Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demo\n",
            "\n",
            "=== Document Splitting ===\n",
            "Number of chunks created: 37\n",
            "First chunk preview: LLM Powered Autonomous Agents | Lil'Log\n",
            "      LLM Powered Autonomous Agents\n",
            "\n",
            "=== Vector Store Indexing ===\n",
            "Vector store created successfully!\n",
            "\n",
            "=== LangGraph Pipeline Setup ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Pipeline Execution\n",
        "async def test_pipeline():\n",
        "    print(\"\\n=== Pipeline Execution ===\")\n",
        "\n",
        "    question = \"What are the main components of an AI agent system?\"\n",
        "    print(f\"\\nTesting basic invoke with question: {question}\")\n",
        "\n",
        "    # Basic invoke\n",
        "    result = graph.invoke({\n",
        "        \"question\": question,\n",
        "        \"context\": [],\n",
        "        \"answer\": \"\"\n",
        "    })\n",
        "    print(f\"Answer: {result['answer']}\")\n",
        "\n",
        "    # Stream updates\n",
        "    print(\"\\nTesting stream updates:\")\n",
        "    async for step in graph.astream(\n",
        "        {\n",
        "            \"question\": question,\n",
        "            \"context\": [],\n",
        "            \"answer\": \"\"\n",
        "        },\n",
        "        stream_mode=\"updates\"\n",
        "    ):\n",
        "        print(f\"Step: {list(step.keys())}\")\n",
        "\n",
        "# 7. Evaluation and Experimentation\n",
        "def run_experiments():\n",
        "    print(\"\\n=== Evaluation and Experimentation ===\")\n",
        "\n",
        "    # Test different questions\n",
        "    test_questions = [\n",
        "        \"What is an AI agent?\",\n",
        "        \"How do AI agents make decisions?\",\n",
        "        \"What are the key challenges in AI agent development?\"\n",
        "    ]\n",
        "\n",
        "    for question in test_questions:\n",
        "        result = graph.invoke({\n",
        "            \"question\": question,\n",
        "            \"context\": [],\n",
        "            \"answer\": \"\"\n",
        "        })\n",
        "        print(f\"\\nQ: {question}\")\n",
        "        print(f\"A: {result['answer']}\")\n",
        "\n",
        "    # Test different chunk parameters\n",
        "    print(\"\\nTesting chunk parameters:\")\n",
        "    for size in [500, 1000, 1500]:\n",
        "        overlap = size // 5\n",
        "        print(f\"\\nChunk size: {size}, overlap: {overlap}\")\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=size,\n",
        "            chunk_overlap=overlap\n",
        "        )\n",
        "        chunks = splitter.split_documents(docs)\n",
        "        print(f\"Number of chunks: {len(chunks)}\")\n",
        "\n",
        "# 8. Main execution\n",
        "async def main():\n",
        "    await test_pipeline()\n",
        "    run_experiments()\n",
        "\n",
        "# Pour exécuter dans Jupyter:\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNJ3Do_c9Hrr",
        "outputId": "8cefb468-476b-46af-e84a-4851da1635f0"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Pipeline Execution ===\n",
            "\n",
            "Testing basic invoke with question: What are the main components of an AI agent system?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: \n",
            "Based on the following context, please provide a clear and concise answer.\n",
            "If the context doesn't contain relevant information, please say so.\n",
            "\n",
            "Context: Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.Agent System Overview#In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to\n",
            "\n",
            "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.Agent System Overview#In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to\n",
            "\n",
            "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.Agent System Overview#In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:Component One: Planning#A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.Task Decomposition#Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to\n",
            "\n",
            "Question: What are the main components of an AI agent system?\n",
            "\n",
            "Answer: What is the main component of an agent system?\n",
            "Answer : What is a main component in an AI system? Answer : What are some components of a AI agent systems? Answer : How do you define the model? Answer: How do we define the models? Answer\n",
            "Question : How does you define a model?\n",
            "Answer (): How do I define the system? Answer): How does I define a system? Question : How can you define an AI model? Question: How can we define an agent? Answer (): What can you specify the model?\" Answer (: How does the model?) Answer ( : How is the model\n",
            "\n",
            "Testing stream updates:\n",
            "Step: ['retrieve']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: ['generate']\n",
            "\n",
            "=== Evaluation and Experimentation ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: What is an AI agent?\n",
            "A: \n",
            "Based on the following context, please provide a clear and concise answer.\n",
            "If the context doesn't contain relevant information, please say so.\n",
            "\n",
            "Context: LLM Powered Autonomous Agents | Lil'Log\n",
            "      LLM Powered Autonomous Agents\n",
            "\n",
            "LLM Powered Autonomous Agents | Lil'Log\n",
            "      LLM Powered Autonomous Agents\n",
            "\n",
            "LLM Powered Autonomous Agents | Lil'Log\n",
            "      LLM Powered Autonomous Agents\n",
            "\n",
            "Question: What is an AI agent?\n",
            "\n",
            "Answer: An AI agent is a robot that is a human.\n",
            "Answer.\n",
            "Question.\n",
            "What is an autonomous agent? What is a machine? What are the robots?\n",
            "Answer,\n",
            "Question,\n",
            "Answer\n",
            "Question\n",
            "Question,\n",
            "Question (Answer)\n",
            "Question(Answer)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: How do AI agents make decisions?\n",
            "A: \n",
            "Based on the following context, please provide a clear and concise answer.\n",
            "If the context doesn't contain relevant information, please say so.\n",
            "\n",
            "Context: refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.Citation#Cited as:Weng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.OrReferences#[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\n",
            "\n",
            "refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.Citation#Cited as:Weng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.OrReferences#[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\n",
            "\n",
            "refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.Citation#Cited as:Weng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.OrReferences#[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\n",
            "\n",
            "Question: How do AI agents make decisions?\n",
            "\n",
            "Answer: How can AI agents do this?\n",
            "Answer, How can I do this:\n",
            "Answer and Answer: How could I do it:\n",
            "Question, How could my AI agent do this?:\n",
            "Answer or Answer:\n",
            "Q: How does my AI agents have a sense of what is going on?\n",
            "Question or Answer, How do my AI Agents have a feeling of what's going on?:\n",
            "Question. How do I do that?\n",
            "Q. How can my AI Agent do this:[4]\n",
            "Q, How does the AI agents know what is happening?\n",
            "Quoting:\n",
            "“I think that the AI agent is\n",
            "\n",
            "Q: What are the key challenges in AI agent development?\n",
            "A: \n",
            "Based on the following context, please provide a clear and concise answer.\n",
            "If the context doesn't contain relevant information, please say so.\n",
            "\n",
            "Context: and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.Reliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.Citation#Cited as:Weng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.OrReferences#[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601\n",
            "\n",
            "and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.Reliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.Citation#Cited as:Weng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.OrReferences#[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601\n",
            "\n",
            "power is not as powerful as full attention.Challenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.Reliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.Citation#Cited as:Weng, Lilian. (Jun\n",
            "\n",
            "Question: What are the key challenges in AI agent development?\n",
            "\n",
            "Answer: The answer is that AI agents are not as strong as humans. They are more likely to be able to understand the problem and solve it in a way that is consistent with the human experience.\n",
            "In the recent paper, we examined the impact of AI agents on the human brain and the brain. We found that AI agent performance was positively correlated with the ability to understand and solve the problem.\n",
            "The main challenge in AI agents is to understand, in the recent study, we analyzed the impact on AI agents. We discovered that AI agency performance was negatively correlated with a ability to comprehend and solve a problem. We also found that, in a\n",
            "\n",
            "Testing chunk parameters:\n",
            "\n",
            "Chunk size: 500, overlap: 100\n",
            "Number of chunks: 58\n",
            "\n",
            "Chunk size: 1000, overlap: 200\n",
            "Number of chunks: 31\n",
            "\n",
            "Chunk size: 1500, overlap: 300\n",
            "Number of chunks: 22\n"
          ]
        }
      ]
    }
  ]
}