{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premiers 200 caractères du corpus :\n",
      "start of the project gutenberg ebook alices adventures in wonderland illustration alices adventures in wonderland by lewis carroll the millennium fulcrum editioncontents chapter idown the rabbithole c\n"
     ]
    }
   ],
   "source": [
    "# 1. Import des bibliothèques nécessaires\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 2. Chargement et prétraitement des données\n",
    "def load_and_preprocess_text(url):\n",
    "    # Télécharger le texte\n",
    "    response = requests.get(url)\n",
    "    text = response.text\n",
    "    \n",
    "    # Trouver le début et la fin du texte principal\n",
    "    start_index = text.find(\"*** START\")\n",
    "    end_index = text.find(\"*** END\")\n",
    "    main_text = text[start_index:end_index]\n",
    "    \n",
    "    # Nettoyage du texte\n",
    "    def preprocess_text(text):\n",
    "        # Supprimer les caractères spéciaux et les nombres\n",
    "        text = re.sub(r'[^a-zA-Z\\s.]', '', text)\n",
    "        # Convertir en minuscules\n",
    "        text = text.lower()\n",
    "        # Supprimer les espaces multiples\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "    \n",
    "    cleaned_text = preprocess_text(main_text)\n",
    "    \n",
    "    # Diviser en phrases\n",
    "    corpus = cleaned_text.split('.')\n",
    "    # Nettoyer les phrases vides\n",
    "    corpus = [sentence.strip() for sentence in corpus if len(sentence.strip()) > 0]\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "# Charger et prétraiter le texte\n",
    "url = \"https://www.gutenberg.org/cache/epub/11/pg11.txt\"\n",
    "corpus = load_and_preprocess_text(url)\n",
    "\n",
    "# Afficher les 200 premiers caractères\n",
    "print(\"Premiers 200 caractères du corpus :\")\n",
    "print(''.join(corpus)[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Création du vocabulaire avec Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Préparation des séquences d'entrée\n",
    "def create_sequences(corpus, sequence_length=10):\n",
    "    input_sequences = []\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
    "        \n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    \n",
    "    return input_sequences\n",
    "\n",
    "# Créer les séquences\n",
    "input_sequences = create_sequences(corpus)\n",
    "\n",
    "# Padding des séquences (pre-padding car c'est plus approprié pour le RNN)\n",
    "max_sequence_length = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, \n",
    "                                       maxlen=max_sequence_length, \n",
    "                                       padding='pre'))\n",
    "\n",
    "# Séparer les entrées et les sorties\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Construction du modèle (version LSTM)\n",
    "def create_model(total_words, max_sequence_length, embedding_dim=100):\n",
    "    model = Sequential([\n",
    "        Embedding(total_words, embedding_dim, input_length=max_sequence_length-1),\n",
    "        LSTM(150, return_sequences=True),\n",
    "        LSTM(100),\n",
    "        Dense(100, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(total_words, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Version alternative avec GRU\n",
    "def create_model_gru(total_words, max_sequence_length, embedding_dim=100):\n",
    "    model = Sequential([\n",
    "        Embedding(total_words, embedding_dim, input_length=max_sequence_length-1),\n",
    "        GRU(150, return_sequences=True),\n",
    "        GRU(100),\n",
    "        Dense(100, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(total_words, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2025-03-10 12:25:33.252720: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Max\n",
      "2025-03-10 12:25:33.252771: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 64.00 GB\n",
      "2025-03-10 12:25:33.252778: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 24.00 GB\n",
      "2025-03-10 12:25:33.252820: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-03-10 12:25:33.252833: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2025-03-10 12:25:33.858741: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 338ms/step - accuracy: 0.0420 - loss: 6.7990 - val_accuracy: 0.0852 - val_loss: 6.2385\n",
      "Epoch 2/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 290ms/step - accuracy: 0.0545 - loss: 6.0008 - val_accuracy: 0.0871 - val_loss: 6.1682\n",
      "Epoch 3/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 305ms/step - accuracy: 0.0602 - loss: 5.8242 - val_accuracy: 0.0926 - val_loss: 6.1294\n",
      "Epoch 4/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 357ms/step - accuracy: 0.0694 - loss: 5.6647 - val_accuracy: 0.0922 - val_loss: 6.1180\n",
      "Epoch 5/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 358ms/step - accuracy: 0.0670 - loss: 5.5859 - val_accuracy: 0.0928 - val_loss: 6.1027\n",
      "Epoch 6/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 318ms/step - accuracy: 0.0755 - loss: 5.4843 - val_accuracy: 0.0973 - val_loss: 6.0667\n",
      "Epoch 7/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 340ms/step - accuracy: 0.0880 - loss: 5.3628 - val_accuracy: 0.1069 - val_loss: 6.0583\n",
      "Epoch 8/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 361ms/step - accuracy: 0.0954 - loss: 5.2779 - val_accuracy: 0.0930 - val_loss: 6.0953\n",
      "Epoch 9/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 375ms/step - accuracy: 0.0938 - loss: 5.1818 - val_accuracy: 0.0924 - val_loss: 6.0987\n",
      "Epoch 10/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 367ms/step - accuracy: 0.0998 - loss: 5.0984 - val_accuracy: 0.0895 - val_loss: 6.1268\n",
      "Epoch 11/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 372ms/step - accuracy: 0.1091 - loss: 5.0153 - val_accuracy: 0.0975 - val_loss: 6.1264\n",
      "Epoch 12/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 407ms/step - accuracy: 0.1181 - loss: 4.9164 - val_accuracy: 0.0967 - val_loss: 6.1410\n"
     ]
    }
   ],
   "source": [
    "# 6. Compilation et entraînement\n",
    "model = create_model(total_words, max_sequence_length)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping pour éviter le surapprentissage\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Entraînement\n",
    "history = model.fit(X, y, \n",
    "                   epochs=50, \n",
    "                   batch_size=128, \n",
    "                   validation_split=0.2,\n",
    "                   callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: alice wondered\n",
      "Generated: alice wondered fifteenth fair family wife grow\n",
      "\n",
      "Seed: the white rabbit\n",
      "Generated: the white rabbit snappishly left through bound confusion\n",
      "\n",
      "Seed: the queen of\n",
      "Generated: the queen of pounds tumbling favoured picked hoarse\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. Fonction de génération de texte\n",
    "def generate_text(seed_text, next_words, model, max_sequence_length):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
    "        \n",
    "        # Prédiction\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        \n",
    "        # Appliquer la température et normaliser\n",
    "        temperature = 0.5\n",
    "        predicted = predicted[0] / temperature\n",
    "        # Convertir en probabilités avec softmax\n",
    "        exp_preds = np.exp(predicted)\n",
    "        predicted = exp_preds / np.sum(exp_preds)\n",
    "        \n",
    "        # Maintenant on peut utiliser multinomial en toute sécurité\n",
    "        predicted = np.random.multinomial(1, predicted)\n",
    "        predicted_index = np.argmax(predicted)\n",
    "        \n",
    "        # Convertir l'index en mot\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_index:\n",
    "                output_word = word\n",
    "                break\n",
    "        \n",
    "        # Ajouter le mot prédit au texte\n",
    "        seed_text += \" \" + output_word\n",
    "    \n",
    "    return seed_text\n",
    "\n",
    "# Test\n",
    "seed_texts = [\n",
    "    \"alice wondered\",\n",
    "    \"the white rabbit\",\n",
    "    \"the queen of\"\n",
    "]\n",
    "\n",
    "for seed in seed_texts:\n",
    "    print(f\"Seed: {seed}\")\n",
    "    print(f\"Generated: {generate_text(seed, 5, model, max_sequence_length)}\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
