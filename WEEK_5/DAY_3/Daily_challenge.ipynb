{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ludoveltz/test_github_fev25/blob/main/Exercices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpMfd6Bkw_sT",
        "outputId": "56481e29-5ea7-48e9-c088-aea3cae4a63a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# 1. Installation des bibliothèques nécessaires\n",
        "!pip install -q langchain\n",
        "!pip install -q torch\n",
        "!pip install -q transformers\n",
        "!pip install -q sentence-transformers\n",
        "!pip install -q datasets\n",
        "!pip install -q faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4XO40HHxfF1",
        "outputId": "9050e4ec-7719-48bb-ef44-cd2f284178cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chargement du dataset Dolly...\n",
            "\n",
            "Vérification des premiers documents :\n",
            "[Document(metadata={'source': 'dolly'}, page_content=\"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\"), Document(metadata={'source': 'dolly'}, page_content='')]\n"
          ]
        }
      ],
      "source": [
        "# 2. Chargement du dataset\n",
        "from datasets import load_dataset\n",
        "from langchain.schema import Document\n",
        "\n",
        "print(\"Chargement du dataset Dolly...\")\n",
        "dataset = load_dataset(\n",
        "    \"databricks/databricks-dolly-15k\",\n",
        "    split=\"train\",\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "# Conversion en Documents Langchain\n",
        "data = [\n",
        "    Document(\n",
        "        page_content=item[\"context\"],\n",
        "        metadata={\"source\": \"dolly\"}\n",
        "    )\n",
        "    for item in list(dataset.take(1000))  # On prend les 1000 premiers exemples\n",
        "]\n",
        "\n",
        "print(\"\\nVérification des premiers documents :\")\n",
        "print(data[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03bGp6Zax1dH",
        "outputId": "a054457e-e8c3-4e59-8605-43bfb5e96ec4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Premier document découpé :\n",
            "page_content='Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.' metadata={'source': 'dolly'}\n"
          ]
        }
      ],
      "source": [
        "# 3. Découpage des documents\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=150\n",
        ")\n",
        "docs = text_splitter.split_documents(data)\n",
        "print(\"\\nPremier document découpé :\")\n",
        "print(docs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmeSYb7-x-Tg",
        "outputId": "fd780204-02de-4dab-9718-05cf30a89255"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVRiZowTx3jP",
        "outputId": "f987bee3-b6cf-4d13-87ef-5da29cd9214a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test d'embedding (3 premières valeurs) :\n",
            "[-0.038338541984558105, 0.12346471846103668, -0.02864297851920128]\n"
          ]
        }
      ],
      "source": [
        "# 4. Création des embeddings\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=modelPath,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "# Test des embeddings\n",
        "text = \"This is a test document.\"\n",
        "query_result = embeddings.embed_query(text)\n",
        "print(\"\\nTest d'embedding (3 premières valeurs) :\")\n",
        "print(query_result[:3])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2frFqveyOka",
        "outputId": "f3c66e97-bcf2-4a03-c86d-e60e4ec81d12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Vector store créé avec succès\n"
          ]
        }
      ],
      "source": [
        "# 5. Création du vector store\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "db = FAISS.from_documents(docs, embeddings)\n",
        "print(\"\\nVector store créé avec succès\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zTRMK7AQzXox"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain-huggingface\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8Dl4eN41Vmn",
        "outputId": "d2166c75-03e6-4fdd-cc7b-e88f8e6a2e95"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Invalid model-index. Not loading eval results into CardData.\n",
            "WARNING:huggingface_hub.repocard_data:Invalid model-index. Not loading eval results into CardData.\n",
            "Invalid model-index. Not loading eval results into CardData.\n",
            "WARNING:huggingface_hub.repocard_data:Invalid model-index. Not loading eval results into CardData.\n",
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "# 6. Préparation du modèle LLM\n",
        "# Import des classes nécessaires\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline  # Note: using langchain_huggingface instead of deprecated langchain\n",
        "\n",
        "# Chargement du tokenizer et du modèle question-answering\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Intel/dynamic_tinybert\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"Intel/dynamic_tinybert\")\n",
        "\n",
        "# Création du pipeline question-answering\n",
        "model_name = \"Intel/dynamic_tinybert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True, max_length=512)\n",
        "Youtubeer = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=model_name,\n",
        "    tokenizer=tokenizer,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "# Création du wrapper Langchain pipeline\n",
        "llm = HuggingFacePipeline(\n",
        "    pipeline=Youtubeer,\n",
        "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "ObeQ7ngK1ZZj"
      },
      "outputs": [],
      "source": [
        "# 7. Construction de la chaîne de Retrieval QA avec une approche plus directe\n",
        "def get_answer_with_youtubeer(question):\n",
        "    # Utilisation du retriever existant\n",
        "    docs = retriever.get_relevant_documents(question)\n",
        "\n",
        "    # Préparation du contexte\n",
        "    context = docs[0].page_content if docs else \"\"\n",
        "\n",
        "    # Utilisation directe du pipeline Youtubeer configuré au point 6\n",
        "    # avec le format correct question/contexte\n",
        "    response = Youtubeer(\n",
        "        {\n",
        "            \"question\": question,\n",
        "            \"context\": context\n",
        "        }\n",
        "    )\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPgcI7_d1bgO",
        "outputId": "3b03f5ce-9f73-4286-e8aa-601ab4ed2cfb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Question : What is cheesemaking?\n",
            "Réponse : A grilled cheese sandwich is made by placing a cheese filling\n",
            "\n",
            "Note : La réponse a un faible niveau de confiance. Dans le contexte de votre analyse des plats traditionnels, il serait pertinent d'ajouter des documents sur les processus de fabrication des fromages.\n"
          ]
        }
      ],
      "source": [
        "# 8. Test du système\n",
        "question = \"What is cheesemaking?\"\n",
        "try:\n",
        "    resultat = get_answer_with_youtubeer(question)\n",
        "    print(\"\\nQuestion :\", question)\n",
        "    print(\"Réponse :\", resultat['answer'])\n",
        "    if resultat['score'] < 0.5:  # Si le score est faible\n",
        "        print(\"\\nNote : La réponse a un faible niveau de confiance. Dans le contexte de votre analyse des plats traditionnels, il serait pertinent d'ajouter des documents sur les processus de fabrication des fromages.\")\n",
        "except Exception as e:\n",
        "    print(f\"Erreur : {str(e)}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMd6SqmNwuJXXwJt1/N095O",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf_m1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
