{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMS4Ita91UL4FNV61aoaWcD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ludoveltz/test_github_fev25/blob/main/Excs_1_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercice 1\n",
        "\n",
        "**Analyse Comparative (3-5 phrases)**\n",
        "Les trois niveaux d'ouverture des LLMs repr√©sentent diff√©rents degr√©s de transparence et de flexibilit√©. Les mod√®les \"Fully Open\" offrent un acc√®s complet au code source, aux poids et √† l'architecture, permettant une personnalisation totale et une compr√©hension approfondie du mod√®le. Les mod√®les \"Weights Released\" fournissent les poids pr√©-entra√Æn√©s et l'architecture mais peuvent avoir des restrictions sur la modification du code source, tandis que les mod√®les \"Architecture Only\" ne partagent que la structure du mod√®le, limitant significativement les possibilit√©s de personnalisation et de r√©entra√Ænement.\n",
        "\n",
        "\n",
        "**R√©ponse au Prompt (Assistant M√©dical)**\n",
        "Pour construire un assistant m√©dical qui n√©cessite un r√©entra√Ænement, le niveau \"Fully Open\" est essentiel pour plusieurs raisons :\n",
        "\n",
        "- S√©curit√© et Conformit√© : L'acc√®s complet au code source permet de v√©rifier et garantir la conformit√© avec les r√©glementations m√©dicales (HIPAA, RGPD).\n",
        "\n",
        "- Personnalisation Sp√©cifique : Le r√©entra√Ænement sur des donn√©es m√©dicales sp√©cialis√©es n√©cessite une compr√©hension et un contr√¥le total du mod√®le.\n",
        "\n",
        "- Tra√ßabilit√© : La transparence totale permet de documenter et auditer le processus d'apprentissage, crucial dans le domaine m√©dical.\n",
        "\n",
        "- Adaptation Continue : La possibilit√© de modifier et optimiser tous les aspects du mod√®le est essentielle pour maintenir la pr√©cision et la pertinence des r√©ponses m√©dicales."
      ],
      "metadata": {
        "id": "NAExuoInleDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercice 2\n",
        "\n",
        "# Comparaison des Licences LLM\n",
        "\n",
        "## Mod√®le 1: mistralai/Mistral-7B-Instruct\n",
        "\n",
        "‚úÖ Type de Licence\n",
        "- Apache 2.0\n",
        "\n",
        "‚úÖ Usage Commercial\n",
        "- Autoris√©\n",
        "\n",
        "‚úÖ Restrictions\n",
        "- Attribution requise\n",
        "- Inclusion d'une copie de la licence\n",
        "- Conservation des notices de copyright\n",
        "- Documentation des modifications\n",
        "\n",
        "## Mod√®le 2: meta-llama/Llama-2-7b-chat-hf\n",
        "\n",
        "‚úÖ Type de Licence\n",
        "- Llama 2 Community License\n",
        "\n",
        "‚úÖ Usage Commercial\n",
        "- Autoris√© sous conditions\n",
        "\n",
        "‚úÖ Restrictions\n",
        "- Demande d'acc√®s obligatoire\n",
        "- Limite de 700M utilisateurs mensuels\n",
        "- Interdiction d'usage militaire\n",
        "- Obligation de signalement des failles\n",
        "- Attribution requise\n"
      ],
      "metadata": {
        "id": "clkYrlwumIRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercice 3\n",
        "\n",
        "# LLM Matchmaker Challenge\n",
        "\n",
        "| Team | Needs | Your Pick | Justification |\n",
        "|------|--------|------------|---------------|\n",
        "| LegalTech | Fast model for logic-heavy chatbot on CPU | Mistral-7B-Instruct | ‚Ä¢ Optimis√© pour CPU<br>‚Ä¢ Excellent raisonnement logique<br>‚Ä¢ Versions quantifi√©es disponibles (GGUF)<br>‚Ä¢ Performance rapide m√™me sur CPU |\n",
        "| EdTech | Logic/math-focused LLM on low-end laptops | TinyLlama-1.1B | ‚Ä¢ Tr√®s l√©ger (1.1B param√®tres)<br>‚Ä¢ Optimis√© pour machines modestes<br>‚Ä¢ Bon en math√©matiques basiques<br>‚Ä¢ Facilement d√©ployable localement |\n",
        "| Global NGO | Model that speaks 5+ languages well | BLOOM-7B1 | ‚Ä¢ Support natif de 46+ langues<br>‚Ä¢ Performance multilingue consistante<br>‚Ä¢ Open source sans restriction<br>‚Ä¢ Bonne qualit√© de traduction |\n"
      ],
      "metadata": {
        "id": "G_R6sEHnnXCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîç Audit de Pr√©paration Local\n",
        "\n",
        "## Liste de V√©rification Mat√©rielle pour Mod√®le 7B Quantifi√©\n",
        "\n",
        "| Exigence | Sp√©cifications Syst√®me | Compatible ? |\n",
        "|----------|------------------------|--------------|\n",
        "| RAM (‚â• 16 GB) | 32 GB DDR4 | ‚úÖ |\n",
        "| Espace Disque (‚â• 40 GB) | 500 GB SSD | ‚úÖ |\n",
        "| OS (Linux/WSL2) | Ubuntu 22.04 LTS | ‚úÖ |\n",
        "\n",
        "\n",
        "## Analyse de Compatibilit√© pour llama.cpp\n",
        "\n",
        "### Configuration Actuelle\n",
        "- ‚úÖ RAM suffisante pour le traitement d'images haute r√©solution\n",
        "- ‚úÖ Espace disque adapt√© aux mod√®les et datasets\n",
        "- ‚úÖ Syst√®me d'exploitation compatible avec les outils de ML\n",
        "\n",
        "### Besoins d'Am√©lioration\n",
        "Pour ex√©cuter LLaMA 7B quantifi√© localement :\n",
        "1. **GPU** : Recommand√© mais non obligatoire avec llama.cpp\n",
        "2. **M√©moire VRAM** : 8GB minimum pour performance optimale\n",
        "3. **CPU** : Multi-core pour traitement efficace\n",
        "\n",
        "### Conclusion\n",
        "La configuration actuelle permet d'ex√©cuter un mod√®le 7B quantifi√© via llama.cpp sans mise √† niveau imm√©diate n√©cessaire."
      ],
      "metadata": {
        "id": "QrKNfKtwnvmX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# üåü Explorateur de Mod√®les Bas√© sur les Benchmarks\n",
        "\n",
        "| Model Name | HellaSwag Score | MMLU Score | License Type | Ideal Use Case |\n",
        "|------------|----------------|------------|--------------|----------------|\n",
        "| Mixtral-8x7B | 87.2 | 70.3 | Apache 2.0 | T√¢ches g√©n√©rales complexes et raisonnement multidomaine |\n",
        "| Llama-2-70B | 85.9 | 69.8 | Llama 2 License | Applications commerciales n√©cessitant des performances √©lev√©es |\n",
        "| Mistral-7B | 83.1 | 64.7 | Apache 2.0 | D√©ploiements efficaces avec ressources limit√©es |\n",
        "\n",
        "Notes sur les benchmarks :\n",
        "- HellaSwag : √âvalue la compr√©hension du sens commun\n",
        "- MMLU : Mesure les connaissances multidisciplinaires\n",
        "\n",
        "Choix pr√©f√©r√© bas√© sur les benchmarks : Mixtral-8x7B\n",
        "Raisons :\n",
        "- Meilleurs scores sur les deux benchmarks\n",
        "- Licence permissive\n",
        "- Bon √©quilibre performance/accessibilit√©\n"
      ],
      "metadata": {
        "id": "D895HConoeYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üåü Analyse Comparative : D√©ploiement Local vs Cloud\n",
        "\n",
        "## D√©ploiement Local\n",
        "\n",
        "### ‚úÖ Avantages\n",
        "- Confidentialit√© totale des donn√©es\n",
        "- Pas de co√ªts r√©currents\n",
        "- Latence stable et pr√©visible\n",
        "- Contr√¥le total de l'infrastructure\n",
        "- Pas de limites de temps d'ex√©cution\n",
        "\n",
        "### ‚ùå Inconv√©nients\n",
        "- Investissement mat√©riel initial √©lev√©\n",
        "- Maintenance technique requise\n",
        "- Ressources limit√©es au hardware disponible\n",
        "- Mises √† jour manuelles n√©cessaires\n",
        "- Consommation d'√©nergie locale\n",
        "\n",
        "## D√©ploiement Cloud (Colab/RunPod)\n",
        "\n",
        "### ‚úÖ Avantages\n",
        "- Acc√®s √† des GPUs puissants\n",
        "- Pas d'investissement mat√©riel\n",
        "- Scalabilit√© instantan√©e\n",
        "- Mises √† jour automatiques\n",
        "- Environnement pr√©-configur√©\n",
        "\n",
        "### ‚ùå Inconv√©nients\n",
        "- Co√ªts r√©currents\n",
        "- D√©pendance √† la connexion internet\n",
        "- Temps de session limit√©s (Colab)\n",
        "- Confidentialit√© des donn√©es limit√©e\n",
        "- Latence variable\n"
      ],
      "metadata": {
        "id": "Guvh1JJ_oj-P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7UUg-VekF2G"
      },
      "outputs": [],
      "source": []
    }
  ]
}