{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLzauVCut7f7JZBFO0e8j2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ludoveltz/test_github_fev25/blob/main/Mini_projet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "K7BjvMvv271K",
        "outputId": "93b69ed1-87bb-4f23-eb8c-ffbde1dc8821"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scholarly\n",
            "  Using cached scholarly-1.7.11-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting arxiv\n",
            "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Collecting arrow (from scholarly)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting bibtexparser (from scholarly)\n",
            "  Downloading bibtexparser-1.4.3.tar.gz (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.11/dist-packages (from scholarly) (1.2.18)\n",
            "Collecting fake-useragent (from scholarly)\n",
            "  Downloading fake_useragent-2.1.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting free-proxy (from scholarly)\n",
            "  Downloading free_proxy-1.1.3.tar.gz (5.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from scholarly) (0.28.1)\n",
            "Collecting python-dotenv (from scholarly)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting selenium (from scholarly)\n",
            "  Downloading selenium-4.30.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting sphinx-rtd-theme (from scholarly)\n",
            "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from scholarly) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow->scholarly)\n",
            "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated->scholarly) (1.17.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from free-proxy->scholarly) (5.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->scholarly) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->scholarly) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->scholarly) (0.14.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->scholarly) (1.7.1)\n",
            "Collecting trio~=0.17 (from selenium->scholarly)\n",
            "  Downloading trio-0.29.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium->scholarly)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium->scholarly) (1.8.0)\n",
            "Requirement already satisfied: sphinx<9,>=6 in /usr/local/lib/python3.11/dist-packages (from sphinx-rtd-theme->scholarly) (8.2.3)\n",
            "Requirement already satisfied: docutils<0.22,>0.18 in /usr/local/lib/python3.11/dist-packages (from sphinx-rtd-theme->scholarly) (0.21.2)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->scholarly)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.1.6)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.18.0)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.4.1)\n",
            "Requirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.1.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->scholarly) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->scholarly) (2.4.0)\n",
            "Collecting outcome (from trio~=0.17->selenium->scholarly)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->scholarly) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium->scholarly)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1->sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.0.2)\n",
            "Downloading scholarly-1.7.11-py3-none-any.whl (39 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fake_useragent-2.1.0-py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.8/125.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading selenium-4.30.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.29.0-py3-none-any.whl (492 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: bibtexparser, free-proxy, sgmllib3k\n",
            "  Building wheel for bibtexparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bibtexparser: filename=bibtexparser-1.4.3-py3-none-any.whl size=43550 sha256=ba3889297e616ffccd4e29dfae777c05a249c96df93dc70ab4b0fbb4d690aed7\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/fb/76/306387739cf9d53b1c39b0c8aadbbb17dc05f256756d8fd915\n",
            "  Building wheel for free-proxy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for free-proxy: filename=free_proxy-1.1.3-py3-none-any.whl size=6096 sha256=a02ca3f848fed47efd519e7b0fffcbfd7d1593f14598f3e0e437b09e292514d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/f8/90/1e74c4166b7fbb213260a35e83fac3119f5e390c8bddda8a37\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=f675969a4ce9b69bcb355e9e3bd8fac443ee431f223db7132a523c1ce353ed6d\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built bibtexparser free-proxy sgmllib3k\n",
            "Installing collected packages: sgmllib3k, wsproto, types-python-dateutil, python-dotenv, PyPDF2, outcome, feedparser, fake-useragent, bibtexparser, trio, free-proxy, arxiv, arrow, trio-websocket, sphinxcontrib-jquery, sphinx-rtd-theme, selenium, scholarly\n",
            "Successfully installed PyPDF2-3.0.1 arrow-1.3.0 arxiv-2.1.3 bibtexparser-1.4.3 fake-useragent-2.1.0 feedparser-6.0.11 free-proxy-1.1.3 outcome-1.3.0.post0 python-dotenv-1.1.0 scholarly-1.7.11 selenium-4.30.0 sgmllib3k-1.0.0 sphinx-rtd-theme-3.0.2 sphinxcontrib-jquery-4.1 trio-0.29.0 trio-websocket-0.12.2 types-python-dateutil-2.9.0.20241206 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "# 1. Installation des bibliothèques nécessaires\n",
        "!pip install scholarly pandas numpy transformers beautifulsoup4 requests PyPDF2 arxiv seaborn matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tabulate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYSHrl906Gqw",
        "outputId": "83b18c82-623c-4968-8cb5-08fcd631acd2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (0.9.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import reportlab\n",
        "except ImportError:\n",
        "    !pip install reportlab\n",
        "    import reportlab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GVwa3bK4_iu8",
        "outputId": "219342ca-0e95-4ab3-9514-a2a24bdcc58d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting reportlab\n",
            "  Downloading reportlab-4.3.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from reportlab) (11.1.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from reportlab) (5.2.0)\n",
            "Downloading reportlab-4.3.1-py3-none-any.whl (1.9 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: reportlab\n",
            "Successfully installed reportlab-4.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "\n",
        "def fetch_relevant_papers():\n",
        "    client = arxiv.Client()\n",
        "    search = arxiv.Search(\n",
        "        query = \"Large Language Models transformers GPT-4 instruction tuning evaluation\",\n",
        "        max_results = 5\n",
        "    )\n",
        "\n",
        "    papers = []\n",
        "    try:\n",
        "        for result in client.results(search):\n",
        "            papers.append({\n",
        "                'title': result.title,\n",
        "                'authors': ', '.join([str(author) for author in result.authors]),\n",
        "                'published': result.published,\n",
        "                'summary': result.summary,\n",
        "                'venue': result.entry_id.split('/')[-1],  # Extraction du venue depuis l'ID\n",
        "                'categories': ', '.join(result.categories),\n",
        "                'problem': extract_problem(result.summary),\n",
        "                'solution': extract_solution(result.summary),\n",
        "                'datasets': extract_datasets(result.summary),\n",
        "                'metrics': extract_metrics(result.summary),\n",
        "                'architecture': extract_architecture(result.summary)\n",
        "            })\n",
        "        print(f\"Nombre de papers récupérés : {len(papers)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors de la recherche : {e}\")\n",
        "\n",
        "    return pd.DataFrame(papers)\n",
        "\n",
        "def extract_problem(text):\n",
        "    \"\"\"Extrait le problème de recherche du résumé\"\"\"\n",
        "    # Simplification : premiers 100 caractères\n",
        "    return text[:100] + \"...\"\n",
        "\n",
        "def extract_solution(text):\n",
        "    \"\"\"Extrait la solution proposée du résumé\"\"\"\n",
        "    # Simplification : caractères du milieu\n",
        "    middle = len(text)//2\n",
        "    return text[middle:middle+100] + \"...\"\n",
        "\n",
        "def extract_metrics(text):\n",
        "    \"\"\"Identifie les métriques d'évaluation de manière plus précise\"\"\"\n",
        "    metrics_patterns = {\n",
        "        'BLEU': r'BLEU[-\\s]?(?:\\d+)?',\n",
        "        'ROUGE': r'ROUGE[-\\s]?(?:[12L])?',\n",
        "        'Accuracy': r'accuracy|précision|precision',\n",
        "        'F1': r'f1[-\\s]?(?:score)?',\n",
        "        'Human Evaluation': r'human\\s+(?:evaluation|assessment|rating)',\n",
        "        'Zero-shot': r'zero[-\\s]?shot',\n",
        "        'Few-shot': r'few[-\\s]?shot',\n",
        "        'Perplexity': r'perplexity|ppl',\n",
        "        'Cross-validation': r'cross[-\\s]?validation',\n",
        "        'Training Loss': r'training\\s+loss|loss\\s+function',\n",
        "        'Inference Time': r'inference\\s+(?:time|speed|performance)',\n",
        "        'Memory Usage': r'memory\\s+(?:usage|consumption)',\n",
        "        'Benchmark Scores': r'benchmark\\s+(?:scores|results)',\n",
        "    }\n",
        "\n",
        "    found_metrics = []\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    for metric, pattern in metrics_patterns.items():\n",
        "        if re.search(pattern, text_lower):\n",
        "            found_metrics.append(metric)\n",
        "\n",
        "    # Recherche de pourcentages spécifiques\n",
        "    if re.search(r'(\\d+(?:\\.\\d+)?%)|(\\d+(?:\\.\\d+)?\\s*percent)', text_lower):\n",
        "        found_metrics.append('Percentage Metrics')\n",
        "\n",
        "    return found_metrics if found_metrics else ['Metrics Not Specified']\n",
        "\n",
        "def extract_datasets(text):\n",
        "    \"\"\"Identifie les datasets de manière plus complète\"\"\"\n",
        "    datasets_patterns = {\n",
        "        'GPT-4': r'GPT-4',\n",
        "        'LLaVA': r'LLaVA',\n",
        "        'Vision-Flan': r'Vision-Flan',\n",
        "        'LAION': r'LAION',\n",
        "        'WebText': r'WebText',\n",
        "        'CommonCrawl': r'CommonCrawl',\n",
        "        'C4': r'C4\\b',\n",
        "        'The Pile': r'The\\s+Pile',\n",
        "        'MMLU': r'MMLU',\n",
        "        'HumanEval': r'HumanEval',\n",
        "        'Custom Dataset': r'custom\\s+dataset|proprietary\\s+data'\n",
        "    }\n",
        "\n",
        "    found_datasets = []\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    for dataset, pattern in datasets_patterns.items():\n",
        "        if re.search(pattern, text, re.IGNORECASE):\n",
        "            found_datasets.append(dataset)\n",
        "\n",
        "    # Détection de mentions de données spécifiques\n",
        "    if re.search(r'training\\s+data|fine-tuning\\s+dataset', text_lower):\n",
        "        found_datasets.append('Training Dataset')\n",
        "\n",
        "    return found_datasets if found_datasets else ['Dataset Not Specified']\n",
        "\n",
        "def extract_architecture(text):\n",
        "    \"\"\"Identifie l'architecture du modèle de manière plus détaillée\"\"\"\n",
        "    architecture_patterns = {\n",
        "        'Transformer Base': r'(?i)transformer(?!\\s+based)',\n",
        "        'GPT Family': r'(?i)(gpt-[234]|gpt\\b)',\n",
        "        'LLaMA Variant': r'(?i)(llama[v2]*|alpaca)',\n",
        "        'Vision-Language Model': r'(?i)(vision.*language|multimodal|visual.*transformer)',\n",
        "        'BERT Variant': r'(?i)(bert|roberta|deberta)',\n",
        "        'Custom Architecture': r'(?i)(custom|novel|proposed)\\s+(architecture|model)',\n",
        "        'Ensemble Model': r'(?i)(ensemble|combined|hybrid)',\n",
        "        'Fine-tuned LLM': r'(?i)(fine.*tuned|adapted)\\s+(model|llm)',\n",
        "        'Specialized Model': r'(?i)(specialized|domain.*specific|task.*specific)',\n",
        "        'LLaVA': r'(?i)llava',\n",
        "        'Vision Transformer': r'(?i)(vit|vision\\s+transformer)',\n",
        "    }\n",
        "\n",
        "    found_architectures = []\n",
        "\n",
        "    for arch, pattern in architecture_patterns.items():\n",
        "        if re.search(pattern, text):\n",
        "            found_architectures.append(arch)\n",
        "\n",
        "    # Analyse contextuelle supplémentaire\n",
        "    if re.search(r'(?i)instruction.*tuning', text):\n",
        "        found_architectures.append('Instruction-tuned Model')\n",
        "    if re.search(r'(?i)zero.*shot|few.*shot', text):\n",
        "        found_architectures.append('Transfer Learning Model')\n",
        "\n",
        "    return found_architectures if found_architectures else ['Architecture Not Specified']\n",
        "\n",
        "def extract_strengths(summary):\n",
        "    \"\"\"Extrait les points forts du résumé de manière structurée\"\"\"\n",
        "    strengths = []\n",
        "\n",
        "    # Analyse des indicateurs positifs\n",
        "    if 'improve' in summary.lower() or 'better' in summary.lower():\n",
        "        strengths.append('Performance Improvement')\n",
        "    if 'novel' in summary.lower() or 'new' in summary.lower():\n",
        "        strengths.append('Novel Approach')\n",
        "    if 'efficient' in summary.lower():\n",
        "        strengths.append('Efficiency')\n",
        "    if 'state-of-the-art' in summary.lower() or 'sota' in summary.lower():\n",
        "        strengths.append('SOTA Results')\n",
        "    if 'scalable' in summary.lower():\n",
        "        strengths.append('Scalability')\n",
        "\n",
        "    # Ajout d'éléments spécifiques selon le contexte\n",
        "    if 'GPT-4' in summary:\n",
        "        strengths.append('Advanced Model Integration')\n",
        "    if 'human' in summary.lower() and 'evaluation' in summary.lower():\n",
        "        strengths.append('Human Validation')\n",
        "\n",
        "    return ', '.join(strengths) if strengths else 'Not Explicitly Stated'\n",
        "\n",
        "def extract_limitations(summary):\n",
        "    \"\"\"Extrait les limitations du résumé de manière structurée\"\"\"\n",
        "    limitations = []\n",
        "\n",
        "    # Analyse des indicateurs de limitation\n",
        "    if 'limitation' in summary.lower() or 'limited' in summary.lower():\n",
        "        limitations.append('Explicit Limitations')\n",
        "    if 'future work' in summary.lower():\n",
        "        limitations.append('Future Work Needed')\n",
        "    if 'computational' in summary.lower() and ('cost' in summary.lower() or 'resource' in summary.lower()):\n",
        "        limitations.append('Computational Costs')\n",
        "    if 'data' in summary.lower() and ('scarce' in summary.lower() or 'limited' in summary.lower()):\n",
        "        limitations.append('Data Limitations')\n",
        "    if 'challenge' in summary.lower():\n",
        "        limitations.append('Technical Challenges')\n",
        "\n",
        "    return ', '.join(limitations) if limitations else 'Not Explicitly Stated'\n",
        "\n",
        "def identify_training_strategy(text):\n",
        "    \"\"\"Identifie la stratégie d'entraînement de manière plus détaillée\"\"\"\n",
        "    strategies = []\n",
        "\n",
        "    # Patterns plus spécifiques\n",
        "    if re.search(r'(?i)instruction.*tun(ing|ed)', text):\n",
        "        strategies.append('Instruction Tuning')\n",
        "    if re.search(r'(?i)fine.*tun(ing|ed)', text):\n",
        "        strategies.append('Fine-tuning')\n",
        "    if re.search(r'(?i)pre.*train(ing|ed)', text):\n",
        "        strategies.append('Pre-training')\n",
        "    if re.search(r'(?i)(multi.*task|mtl)', text):\n",
        "        strategies.append('Multi-task Learning')\n",
        "    if re.search(r'(?i)(transfer.*learning|knowledge.*transfer)', text):\n",
        "        strategies.append('Transfer Learning')\n",
        "    if re.search(r'(?i)(zero.*shot|few.*shot)', text):\n",
        "        strategies.append('Few/Zero-shot Learning')\n",
        "    if re.search(r'(?i)(curriculum|progressive)', text):\n",
        "        strategies.append('Curriculum Learning')\n",
        "    if re.search(r'(?i)(adversarial|gan)', text):\n",
        "        strategies.append('Adversarial Training')\n",
        "    if re.search(r'(?i)(self.*supervised|unsupervised)', text):\n",
        "        strategies.append('Self-supervised Learning')\n",
        "\n",
        "    return ', '.join(strategies) if strategies else 'Training Strategy Not Specified'\n",
        "\n",
        "def create_comparative_analysis(df):\n",
        "    \"\"\"Crée une analyse comparative plus détaillée des papers\"\"\"\n",
        "    comparison = {\n",
        "        'Objectives': df['problem'].tolist(),\n",
        "        'Architecture': [extract_architecture(text) for text in df['summary']],\n",
        "        'Datasets': df['datasets'].tolist(),\n",
        "        'Metrics': df['metrics'].tolist(),\n",
        "        'Training Strategy': [identify_training_strategy(text) for text in df['summary']],\n",
        "        'Reproducibility': [\n",
        "            'Code & Data Available' if all(term in str(text).lower() for term in ['github', 'data'])\n",
        "            else 'Code Available' if 'github' in str(text).lower()\n",
        "            else 'Data Available' if 'dataset' in str(text).lower()\n",
        "            else 'Not Specified'\n",
        "            for text in df['summary']\n",
        "        ]\n",
        "    }\n",
        "    return pd.DataFrame(comparison)\n",
        "\n",
        "def generate_report(df):\n",
        "    if len(df) > 0:\n",
        "        try:\n",
        "            comparison_df = create_comparative_analysis(df)\n",
        "\n",
        "            # Création de la section des forces et limitations séparément\n",
        "            strengths_limitations = []\n",
        "            for title, summary in zip(df['title'], df['summary']):\n",
        "                row = f\"| {title[:50]}... | {extract_strengths(summary)} | {extract_limitations(summary)} |\"\n",
        "                strengths_limitations.append(row)\n",
        "            strengths_limitations_text = '\\n'.join(strengths_limitations)\n",
        "\n",
        "            report = f\"\"\"\n",
        "# Analysis of Recent Advances in Large Language Models\n",
        "\n",
        "## 1. Introduction\n",
        "This report analyzes recent research papers focusing on Large Language Models (LLMs), examining their architectures, training methodologies, and key contributions to the field.\n",
        "\n",
        "Papers analyzed: {len(df)} recent publications\n",
        "\n",
        "## 2. Research Papers Overview\n",
        "{df[['title', 'authors', 'published', 'venue']].to_markdown()}\n",
        "\n",
        "## 3. Key Contributions and Methodologies\n",
        "\n",
        "### Research Problems and Solutions\n",
        "{df[['title', 'problem', 'solution']].to_markdown()}\n",
        "\n",
        "### Technical Approaches\n",
        "{comparison_df.to_markdown()}\n",
        "\n",
        "## 4. Comparative Analysis\n",
        "\n",
        "### Model Architectures and Training Strategies\n",
        "{pd.DataFrame({\n",
        "    'Paper': df['title'],\n",
        "    'Architecture': df['architecture'],\n",
        "    'Training Strategy': comparison_df['Training Strategy'],\n",
        "    'Reproducibility': comparison_df['Reproducibility']\n",
        "}).to_markdown()}\n",
        "\n",
        "### Datasets and Evaluation\n",
        "{pd.DataFrame({\n",
        "    'Paper': df['title'],\n",
        "    'Datasets Used': df['datasets'],\n",
        "    'Evaluation Metrics': df['metrics']\n",
        "}).to_markdown()}\n",
        "\n",
        "### Key Strengths and Limitations\n",
        "| Paper | Strengths | Limitations |\n",
        "|-------|-----------|-------------|\n",
        "{strengths_limitations_text}\n",
        "\n",
        "## 5. Common Challenges and Future Directions\n",
        "\n",
        "### Identified Challenges\n",
        "- Training efficiency and computational requirements\n",
        "- Model interpretability and transparency\n",
        "- Ethical considerations and bias mitigation\n",
        "- Scalability and deployment challenges\n",
        "\n",
        "### Future Research Directions\n",
        "- Improved efficiency in pre-training and fine-tuning\n",
        "- Enhanced interpretability methods\n",
        "- Novel architectural innovations\n",
        "- Robust evaluation frameworks\n",
        "\n",
        "## 6. Conclusion\n",
        "Analysis period: {df['published'].min().date()} to {df['published'].max().date()}\n",
        "\n",
        "This analysis reveals several key trends in LLM research:\n",
        "- Current focus areas in the field\n",
        "- Emerging methodologies\n",
        "- Critical challenges to address\n",
        "- Promising directions for future research\n",
        "\"\"\"\n",
        "\n",
        "            # Création du PDF avec meilleure gestion des erreurs\n",
        "            try:\n",
        "                from reportlab.pdfgen import canvas\n",
        "                from reportlab.lib.pagesizes import letter\n",
        "                from reportlab.lib.styles import getSampleStyleSheet\n",
        "                from reportlab.platypus import SimpleDocTemplate, Paragraph\n",
        "\n",
        "                pdf_filename = \"llm_research_analysis.pdf\"\n",
        "                doc = SimpleDocTemplate(pdf_filename, pagesize=letter)\n",
        "\n",
        "                # Conversion du markdown en éléments PDF\n",
        "                styles = getSampleStyleSheet()\n",
        "                elements = []\n",
        "\n",
        "                # Ajout du titre\n",
        "                elements.append(Paragraph(\"Analysis of Recent Advances in Large Language Models\", styles['Title']))\n",
        "                elements.append(Paragraph(report, styles['Normal']))\n",
        "\n",
        "                # Génération du PDF\n",
        "                doc.build(elements)\n",
        "\n",
        "                # Sauvegarde sur Google Drive\n",
        "                try:\n",
        "                    from google.colab import drive\n",
        "                    from datetime import datetime\n",
        "\n",
        "                    # Monter Google Drive\n",
        "                    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "                    # Créer nom de fichier avec date\n",
        "                    date_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "                    drive_filename = f\"/content/drive/MyDrive/LLM_Reports/llm_analysis_{date_str}.pdf\"\n",
        "\n",
        "                    # Créer dossier si nécessaire\n",
        "                    import os\n",
        "                    os.makedirs(os.path.dirname(drive_filename), exist_ok=True)\n",
        "\n",
        "                    # Copier vers Drive\n",
        "                    import shutil\n",
        "                    shutil.copy2(pdf_filename, drive_filename)\n",
        "\n",
        "                    print(f\"Rapport PDF sauvegardé sur Google Drive : {drive_filename}\")\n",
        "\n",
        "                except Exception as drive_error:\n",
        "                    print(f\"Erreur lors de la sauvegarde sur Google Drive : {drive_error}\")\n",
        "\n",
        "            except Exception as pdf_error:\n",
        "                print(f\"Erreur lors de la création du PDF : {pdf_error}\")\n",
        "                print(\"Génération du rapport en format Markdown uniquement...\")\n",
        "\n",
        "            # Affichage Markdown dans Colab\n",
        "            from IPython.display import display, Markdown\n",
        "            display(Markdown(report))\n",
        "\n",
        "            # Sauvegarde en Markdown comme backup\n",
        "            with open('meta_analysis_report.md', 'w', encoding='utf-8') as f:\n",
        "                f.write(report)\n",
        "\n",
        "            print(\"Rapport généré avec succès\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur rapport : {e}\")\n",
        "    else:\n",
        "        print(\"Pas de données à inclure dans le rapport\")\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"Démarrage de l'analyse...\")\n",
        "\n",
        "    df = fetch_relevant_papers()\n",
        "\n",
        "    if len(df) > 0:\n",
        "        print(f\"\\nAnalyse de {len(df)} papers...\")\n",
        "        generate_report(df)\n",
        "\n",
        "        # Afficher le contenu du fichier sauvegardé\n",
        "        print(\"\\nContenu du rapport sauvegardé :\")\n",
        "        with open('meta_analysis_report.md', 'r', encoding='utf-8') as f:\n",
        "            print(f.read())\n",
        "\n",
        "        return df\n",
        "    else:\n",
        "        print(\"Aucun paper trouvé\")\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BJmxqOf03doN",
        "outputId": "f50a8d15-eac6-49a0-c022-b8b0aaf52954"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Démarrage de l'analyse...\n",
            "Nombre de papers récupérés : 5\n",
            "\n",
            "Analyse de 5 papers...\n",
            "Mounted at /content/drive\n",
            "Rapport PDF sauvegardé sur Google Drive : /content/drive/MyDrive/LLM_Reports/llm_analysis_20250328_093031.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n# Analysis of Recent Advances in Large Language Models\n\n## 1. Introduction\nThis report analyzes recent research papers focusing on Large Language Models (LLMs), examining their architectures, training methodologies, and key contributions to the field.\n\nPapers analyzed: 5 recent publications\n\n## 2. Research Papers Overview\n|    | title                                                                                                                                                   | authors                                                                                              | published                 | venue        |\n|---:|:--------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------|:--------------------------|:-------------|\n|  0 | Instruction Tuning with GPT-4                                                                                                                           | Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, Jianfeng Gao                                  | 2023-04-06 17:58:09+00:00 | 2304.03277v1 |\n|  1 | Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning                                                                                   | Zhiyang Xu, Chao Feng, Rulin Shao, Trevor Ashby, Ying Shen, Di Jin, Yu Cheng, Qifan Wang, Lifu Huang | 2024-02-18 19:38:44+00:00 | 2402.11690v1 |\n|  2 | Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on Japanese | Yikun Sun, Zhen Wan, Nobuhiro Ueda, Sakiko Yahata, Fei Cheng, Chenhui Chu, Sadao Kurohashi           | 2024-03-06 13:17:07+00:00 | 2403.03690v1 |\n|  3 | Visual Instruction Tuning                                                                                                                               | Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee                                                  | 2023-04-17 17:59:25+00:00 | 2304.08485v2 |\n|  4 | Detect Llama -- Finding Vulnerabilities in Smart Contracts using Large Language Models                                                                  | Peter Ince, Xiapu Luo, Jiangshan Yu, Joseph K. Liu, Xiaoning Du                                      | 2024-07-12 03:33:13+00:00 | 2407.08969v1 |\n\n## 3. Key Contributions and Methodologies\n\n### Research Problems and Solutions\n|    | title                                                                                                                                                   | problem                                                                      | solution                                                                       |\n|---:|:--------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------|:-------------------------------------------------------------------------------|\n|  0 | Instruction Tuning with GPT-4                                                                                                                           | Prior work has shown that finetuning large language models (LLMs) using      | w that the 52K English and                                                     |\n|    |                                                                                                                                                         | machine-generated instructio...                                              | Chinese instruction-following data generated by GPT-4 leads to superior        |\n|    |                                                                                                                                                         |                                                                              | z...                                                                           |\n|  1 | Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning                                                                                   | Despite vision-language models' (VLMs) remarkable capabilities as versatile  | irstly finetuned on Vision-Flan and                                            |\n|    |                                                                                                                                                         | visual assistants, two s...                                                  | further tuned on GPT-4 synthesized data. We find this two-stage ...            |\n|  2 | Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on Japanese | The creation of instruction data and evaluation benchmarks for serving Large | tion data. We also                                                             |\n|    |                                                                                                                                                         | language models often i...                                                   | construct an evaluation benchmark containing 80 questions across 8 categories, |\n|    |                                                                                                                                                         |                                                                              | us...                                                                          |\n|  3 | Visual Instruction Tuning                                                                                                                               | Instruction tuning large language models (LLMs) using machine-generated      | purpose visual and language understanding.Our early experiments                |\n|    |                                                                                                                                                         | instruction-following data h...                                              | show that LLaVA demonstrates impress...                                        |\n|  4 | Detect Llama -- Finding Vulnerabilities in Smart Contracts using Large Language Models                                                                  | In this paper, we test the hypothesis that although OpenAI's GPT-4 performs  | art contract                                                                   |\n|    |                                                                                                                                                         | well generally, we can f...                                                  | vulnerable?), our two best-performing models, GPT-3.5FT and Detect Llama -     |\n|    |                                                                                                                                                         |                                                                              | Foundation, ...                                                                |\n\n### Technical Approaches\n|    | Objectives                                                                   | Architecture                                                                                           | Datasets                 | Metrics                                         | Training Strategy                                                     | Reproducibility   |\n|---:|:-----------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------|:-------------------------|:------------------------------------------------|:----------------------------------------------------------------------|:------------------|\n|  0 | Prior work has shown that finetuning large language models (LLMs) using      | ['GPT Family', 'LLaMA Variant', 'Instruction-tuned Model', 'Transfer Learning Model']                  | ['GPT-4']                | ['Zero-shot']                                   | Instruction Tuning, Fine-tuning, Pre-training, Few/Zero-shot Learning | Not Specified     |\n|    | machine-generated instructio...                                              |                                                                                                        |                          |                                                 |                                                                       |                   |\n|  1 | Despite vision-language models' (VLMs) remarkable capabilities as versatile  | ['GPT Family', 'Vision-Language Model', 'Instruction-tuned Model']                                     | ['GPT-4', 'Vision-Flan'] | ['Metrics Not Specified']                       | Instruction Tuning, Fine-tuning, Pre-training                         | Data Available    |\n|    | visual assistants, two s...                                                  |                                                                                                        |                          |                                                 |                                                                       |                   |\n|  2 | The creation of instruction data and evaluation benchmarks for serving Large | ['GPT Family', 'LLaMA Variant']                                                                        | ['GPT-4']                | ['Human Evaluation']                            | Fine-tuning, Pre-training                                             | Not Specified     |\n|    | language models often i...                                                   |                                                                                                        |                          |                                                 |                                                                       |                   |\n|  3 | Instruction tuning large language models (LLMs) using machine-generated      | ['GPT Family', 'Vision-Language Model', 'LLaVA', 'Instruction-tuned Model', 'Transfer Learning Model'] | ['GPT-4', 'LLaVA']       | ['Accuracy', 'Zero-shot', 'Percentage Metrics'] | Instruction Tuning, Fine-tuning, Few/Zero-shot Learning               | Data Available    |\n|    | instruction-following data h...                                              |                                                                                                        |                          |                                                 |                                                                       |                   |\n|  4 | In this paper, we test the hypothesis that although OpenAI's GPT-4 performs  | ['GPT Family', 'LLaMA Variant']                                                                        | ['GPT-4']                | ['F1']                                          | Training Strategy Not Specified                                       | Data Available    |\n|    | well generally, we can f...                                                  |                                                                                                        |                          |                                                 |                                                                       |                   |\n\n## 4. Comparative Analysis\n\n### Model Architectures and Training Strategies\n|    | Paper                                                                                                                                                   | Architecture                                                                                           | Training Strategy                                                     | Reproducibility   |\n|---:|:--------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:------------------|\n|  0 | Instruction Tuning with GPT-4                                                                                                                           | ['GPT Family', 'LLaMA Variant', 'Instruction-tuned Model', 'Transfer Learning Model']                  | Instruction Tuning, Fine-tuning, Pre-training, Few/Zero-shot Learning | Not Specified     |\n|  1 | Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning                                                                                   | ['GPT Family', 'Vision-Language Model', 'Instruction-tuned Model']                                     | Instruction Tuning, Fine-tuning, Pre-training                         | Data Available    |\n|  2 | Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on Japanese | ['GPT Family', 'LLaMA Variant']                                                                        | Fine-tuning, Pre-training                                             | Not Specified     |\n|  3 | Visual Instruction Tuning                                                                                                                               | ['GPT Family', 'Vision-Language Model', 'LLaVA', 'Instruction-tuned Model', 'Transfer Learning Model'] | Instruction Tuning, Fine-tuning, Few/Zero-shot Learning               | Data Available    |\n|  4 | Detect Llama -- Finding Vulnerabilities in Smart Contracts using Large Language Models                                                                  | ['GPT Family', 'LLaMA Variant']                                                                        | Training Strategy Not Specified                                       | Data Available    |\n\n### Datasets and Evaluation\n|    | Paper                                                                                                                                                   | Datasets Used            | Evaluation Metrics                              |\n|---:|:--------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------|:------------------------------------------------|\n|  0 | Instruction Tuning with GPT-4                                                                                                                           | ['GPT-4']                | ['Zero-shot']                                   |\n|  1 | Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning                                                                                   | ['GPT-4', 'Vision-Flan'] | ['Metrics Not Specified']                       |\n|  2 | Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on Japanese | ['GPT-4']                | ['Human Evaluation']                            |\n|  3 | Visual Instruction Tuning                                                                                                                               | ['GPT-4', 'LLaVA']       | ['Accuracy', 'Zero-shot', 'Percentage Metrics'] |\n|  4 | Detect Llama -- Finding Vulnerabilities in Smart Contracts using Large Language Models                                                                  | ['GPT-4']                | ['F1']                                          |\n\n### Key Strengths and Limitations\n| Paper | Strengths | Limitations |\n|-------|-----------|-------------|\n| Instruction Tuning with GPT-4... | Novel Approach, SOTA Results, Advanced Model Integration, Human Validation | Not Explicitly Stated |\n| Vision-Flan: Scaling Human-Labeled Tasks in Visual... | SOTA Results, Advanced Model Integration, Human Validation | Technical Challenges |\n| Rapidly Developing High-quality Instruction Data a... | Efficiency, Advanced Model Integration, Human Validation | Not Explicitly Stated |\n| Visual Instruction Tuning... | Performance Improvement, Novel Approach, SOTA Results, Advanced Model Integration | Not Explicitly Stated |\n| Detect Llama -- Finding Vulnerabilities in Smart C... | Advanced Model Integration | Not Explicitly Stated |\n\n## 5. Common Challenges and Future Directions\n\n### Identified Challenges\n- Training efficiency and computational requirements\n- Model interpretability and transparency\n- Ethical considerations and bias mitigation\n- Scalability and deployment challenges\n\n### Future Research Directions\n- Improved efficiency in pre-training and fine-tuning\n- Enhanced interpretability methods\n- Novel architectural innovations\n- Robust evaluation frameworks\n\n## 6. Conclusion\nAnalysis period: 2023-04-06 to 2024-07-12\n\nThis analysis reveals several key trends in LLM research:\n- Current focus areas in the field\n- Emerging methodologies\n- Critical challenges to address\n- Promising directions for future research\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rapport généré avec succès\n",
            "\n",
            "Contenu du rapport sauvegardé :\n",
            "\n",
            "# Analysis of Recent Advances in Large Language Models\n",
            "\n",
            "## 1. Introduction\n",
            "This report analyzes recent research papers focusing on Large Language Models (LLMs), examining their architectures, training methodologies, and key contributions to the field.\n",
            "\n",
            "Papers analyzed: 5 recent publications\n",
            "\n",
            "## 2. Research Papers Overview\n",
            "|    | title                                                                                                                                                   | authors                                                                                              | published                 | venue        |\n",
            "|---:|:--------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------|:--------------------------|:-------------|\n",
            "|  0 | Instruction Tuning with GPT-4                                                                                                                           | Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, Jianfeng Gao                                  | 2023-04-06 17:58:09+00:00 | 2304.03277v1 |\n",
            "|  1 | Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning                                                                                   | Zhiyang Xu, Chao Feng, Rulin Shao, Trevor Ashby, Ying Shen, Di Jin, Yu Cheng, Qifan Wang, Lifu Huang | 2024-02-18 19:38:44+00:00 | 2402.11690v1 |\n",
            "|  2 | Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on Japanese | Yikun Sun, Zhen Wan, Nobuhiro Ueda, Sakiko Yahata, Fei Cheng, Chenhui Chu, Sadao Kurohashi           | 2024-03-06 13:17:07+00:00 | 2403.03690v1 |\n",
            "|  3 | Visual Instruction Tuning                                                                                                                               | Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee                                                  | 2023-04-17 17:59:25+00:00 | 2304.08485v2 |\n",
            "|  4 | Detect Llama -- Finding Vulnerabilities in Smart Contracts using Large Language Models                                                                  | Peter Ince, Xiapu Luo, Jiangshan Yu, Joseph K. Liu, Xiaoning Du                                      | 2024-07-12 03:33:13+00:00 | 2407.08969v1 |\n",
            "\n",
            "## 3. Key Contributions and Methodologies\n",
            "\n",
            "### Research Problems and Solutions\n",
            "|    | title                                                                                                                                                   | problem                                                                      | solution                                                                       |\n",
            "|---:|:--------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------|:-------------------------------------------------------------------------------|\n",
            "|  0 | Instruction Tuning with GPT-4                                                                                                                           | Prior work has shown that finetuning large language models (LLMs) using      | w that the 52K English and                                                     |\n",
            "|    |                                                                                                                                                         | machine-generated instructio...                                              | Chinese instruction-following data generated by GPT-4 leads to superior        |\n",
            "|    |                                                                                                                                                         |                                                                              | z...                                                                           |\n",
            "|  1 | Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning                                                                                   | Despite vision-language models' (VLMs) remarkable capabilities as versatile  | irstly finetuned on Vision-Flan and                                            |\n",
            "|    |                                                                                                                                                         | visual assistants, two s...                                                  | further tuned on GPT-4 synthesized data. We find this two-stage ...            |\n",
            "|  2 | Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on Japanese | The creation of instruction data and evaluation benchmarks for serving Large | tion data. We also                                                             |\n",
            "|    |                                                                                                                                                         | language models often i...                                                   | construct an evaluation benchmark containing 80 questions across 8 categories, |\n",
            "|    |                                                                                                                                                         |                                                                              | us...                                                                          |\n",
            "|  3 | Visual Instruction Tuning                                                                                                                               | Instruction tuning large language models (LLMs) using machine-generated      | purpose visual and language understanding.Our early experiments                |\n",
            "|    |                                                                                                                                                         | instruction-following data h...                                              | show that LLaVA demonstrates impress...                                        |\n",
            "|  4 | Detect Llama -- Finding Vulnerabilities in Smart Contracts using Large Language Models                                                                  | In this paper, we test the hypothesis that although OpenAI's GPT-4 performs  | art contract                                                                   |\n",
            "|    |                                                                                                                                                         | well generally, we can f...                                                  | vulnerable?), our two best-performing models, GPT-3.5FT and Detect Llama -     |\n",
            "|    |                                                                                                                                                         |                                                                              | Foundation, ...                                                                |\n",
            "\n",
            "### Technical Approaches\n",
            "|    | Objectives                                                                   | Architecture                                                                                           | Datasets                 | Metrics                                         | Training Strategy                                                     | Reproducibility   |\n",
            "|---:|:-----------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------|:-------------------------|:------------------------------------------------|:----------------------------------------------------------------------|:------------------|\n",
            "|  0 | Prior work has shown that finetuning large language models (LLMs) using      | ['GPT Family', 'LLaMA Variant', 'Instruction-tuned Model', 'Transfer Learning Model']                  | ['GPT-4']                | ['Zero-shot']                                   | Instruction Tuning, Fine-tuning, Pre-training, Few/Zero-shot Learning | Not Specified     |\n",
            "|    | machine-generated instructio...                                              |                                                                                                        |                          |                                                 |                                                                       |                   |\n",
            "|  1 | Despite vision-language models' (VLMs) remarkable capabilities as versatile  | ['GPT Family', 'Vision-Language Model', 'Instruction-tuned Model']                                     | ['GPT-4', 'Vision-Flan'] | ['Metrics Not Specified']                       | Instruction Tuning, Fine-tuning, Pre-training                         | Data Available    |\n",
            "|    | visual assistants, two s...                                                  |                                                                                                        |                          |                                                 |                                                                       |                   |\n",
            "|  2 | The creation of instruction data and evaluation benchmarks for serving Large | ['GPT Family', 'LLaMA Variant']                                                                        | ['GPT-4']                | ['Human Evaluation']                            | Fine-tuning, Pre-training                                             | Not Specified     |\n",
            "|    | language models often i...                                                   |                                                                                                        |                          |                                                 |                                                                       |                   |\n",
            "|  3 | Instruction tuning large language models (LLMs) using machine-generated      | ['GPT Family', 'Vision-Language Model', 'LLaVA', 'Instruction-tuned Model', 'Transfer Learning Model'] | ['GPT-4', 'LLaVA']       | ['Accuracy', 'Zero-shot', 'Percentage Metrics'] | Instruction Tuning, Fine-tuning, Few/Zero-shot Learning               | Data Available    |\n",
            "|    | instruction-following data h...                                              |                                                                                                        |                          |                                                 |                                                                       |                   |\n",
            "|  4 | In this paper, we test the hypothesis that although OpenAI's GPT-4 performs  | ['GPT Family', 'LLaMA Variant']                                                                        | ['GPT-4']                | ['F1']                                          | Training Strategy Not Specified                                       | Data Available    |\n",
            "|    | well generally, we can f...                                                  |                                                                                                        |                          |                                                 |                                                                       |                   |\n",
            "\n",
            "## 4. Comparative Analysis\n",
            "\n",
            "### Model Architectures and Training Strategies\n",
            "|    | Paper                                                                                                                                                   | Architecture                                                                                           | Training Strategy                                                     | Reproducibility   |\n",
            "|---:|:--------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:------------------|\n",
            "|  0 | Instruction Tuning with GPT-4                                                                                                                           | ['GPT Family', 'LLaMA Variant', 'Instruction-tuned Model', 'Transfer Learning Model']                  | Instruction Tuning, Fine-tuning, Pre-training, Few/Zero-shot Learning | Not Specified     |\n",
            "|  1 | Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning                                                                                   | ['GPT Family', 'Vision-Language Model', 'Instruction-tuned Model']                                     | Instruction Tuning, Fine-tuning, Pre-training                         | Data Available    |\n",
            "|  2 | Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on Japanese | ['GPT Family', 'LLaMA Variant']                                                                        | Fine-tuning, Pre-training                                             | Not Specified     |\n",
            "|  3 | Visual Instruction Tuning                                                                                                                               | ['GPT Family', 'Vision-Language Model', 'LLaVA', 'Instruction-tuned Model', 'Transfer Learning Model'] | Instruction Tuning, Fine-tuning, Few/Zero-shot Learning               | Data Available    |\n",
            "|  4 | Detect Llama -- Finding Vulnerabilities in Smart Contracts using Large Language Models                                                                  | ['GPT Family', 'LLaMA Variant']                                                                        | Training Strategy Not Specified                                       | Data Available    |\n",
            "\n",
            "### Datasets and Evaluation\n",
            "|    | Paper                                                                                                                                                   | Datasets Used            | Evaluation Metrics                              |\n",
            "|---:|:--------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------|:------------------------------------------------|\n",
            "|  0 | Instruction Tuning with GPT-4                                                                                                                           | ['GPT-4']                | ['Zero-shot']                                   |\n",
            "|  1 | Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning                                                                                   | ['GPT-4', 'Vision-Flan'] | ['Metrics Not Specified']                       |\n",
            "|  2 | Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on Japanese | ['GPT-4']                | ['Human Evaluation']                            |\n",
            "|  3 | Visual Instruction Tuning                                                                                                                               | ['GPT-4', 'LLaVA']       | ['Accuracy', 'Zero-shot', 'Percentage Metrics'] |\n",
            "|  4 | Detect Llama -- Finding Vulnerabilities in Smart Contracts using Large Language Models                                                                  | ['GPT-4']                | ['F1']                                          |\n",
            "\n",
            "### Key Strengths and Limitations\n",
            "| Paper | Strengths | Limitations |\n",
            "|-------|-----------|-------------|\n",
            "| Instruction Tuning with GPT-4... | Novel Approach, SOTA Results, Advanced Model Integration, Human Validation | Not Explicitly Stated |\n",
            "| Vision-Flan: Scaling Human-Labeled Tasks in Visual... | SOTA Results, Advanced Model Integration, Human Validation | Technical Challenges |\n",
            "| Rapidly Developing High-quality Instruction Data a... | Efficiency, Advanced Model Integration, Human Validation | Not Explicitly Stated |\n",
            "| Visual Instruction Tuning... | Performance Improvement, Novel Approach, SOTA Results, Advanced Model Integration | Not Explicitly Stated |\n",
            "| Detect Llama -- Finding Vulnerabilities in Smart C... | Advanced Model Integration | Not Explicitly Stated |\n",
            "\n",
            "## 5. Common Challenges and Future Directions\n",
            "\n",
            "### Identified Challenges\n",
            "- Training efficiency and computational requirements\n",
            "- Model interpretability and transparency\n",
            "- Ethical considerations and bias mitigation\n",
            "- Scalability and deployment challenges\n",
            "\n",
            "### Future Research Directions\n",
            "- Improved efficiency in pre-training and fine-tuning\n",
            "- Enhanced interpretability methods\n",
            "- Novel architectural innovations\n",
            "- Robust evaluation frameworks\n",
            "\n",
            "## 6. Conclusion\n",
            "Analysis period: 2023-04-06 to 2024-07-12\n",
            "\n",
            "This analysis reveals several key trends in LLM research:\n",
            "- Current focus areas in the field\n",
            "- Emerging methodologies\n",
            "- Critical challenges to address\n",
            "- Promising directions for future research\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔍 4. Insights et Réflexion\n",
        "\n",
        "## 1. Tendances et patterns émergents :\n",
        "- Dominance de l'instruction tuning : 3 sur 5 papers se concentrent sur cette approche\n",
        "- Forte utilisation de GPT-4 comme base de référence ou dataset\n",
        "- Émergence des modèles multimodaux (vision-langage)\n",
        "- Focus sur la réduction de l'effort humain dans la création de données d'entraînement\n",
        "\n",
        "## 2. Méthodes les plus prometteuses :\n",
        "- Approches hybrides combinant instruction tuning et fine-tuning\n",
        "- Modèles vision-langage (LLaVA, Vision-Flan) pour des applications multimodales\n",
        "- Méthodes d'évaluation automatisées avec validation humaine minimale\n",
        "- Techniques de transfer learning et few/zero-shot learning\n",
        "\n",
        "## 3. Limitations et défis communs :\n",
        "- Besoins importants en ressources computationnelles\n",
        "- Manque de reproductibilité (2 papers sur 5 \"Not Specified\")\n",
        "- Dépendance aux modèles propriétaires (GPT-4)\n",
        "- Métriques d'évaluation parfois insuffisantes ou non standardisées\n",
        "\n",
        "## 4. Directions futures potentielles :\n",
        "- Développement de benchmarks plus robustes\n",
        "- Amélioration de l'efficacité des processus d'instruction tuning\n",
        "- Extension des capacités multimodales\n",
        "- Réduction de la dépendance aux données annotées manuellement\n",
        "\n",
        "# ✅ 5. Conclusion\n",
        "\n",
        "L'évolution du domaine montre plusieurs tendances majeures :\n",
        "\n",
        "## 1. Maturité technique :\n",
        "- Passage des approches génériques vers des solutions spécialisées\n",
        "- Développement d'architectures hybrides combinant plusieurs techniques\n",
        "- Importance croissante de la validation empirique\n",
        "\n",
        "## 2. Démocratisation :\n",
        "- Efforts pour réduire les barrières d'entrée (coûts, données, expertise)\n",
        "- Focus sur la reproductibilité et l'accessibilité\n",
        "- Développement de solutions pour différentes langues (ex: japonais)\n",
        "\n",
        "## 3. Innovation méthodologique :\n",
        "- Nouvelles approches d'instruction tuning\n",
        "- Integration vision-langage\n",
        "- Automatisation de la création de données d'entraînement\n",
        "\n",
        "## 4. Défis persistants :\n",
        "- Équilibre entre performance et efficacité\n",
        "- Standardisation des métriques d'évaluation\n",
        "- Réduction de la dépendance aux modèles propriétaires\n"
      ],
      "metadata": {
        "id": "wCqXilnJGNER"
      }
    }
  ]
}