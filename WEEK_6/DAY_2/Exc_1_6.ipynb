{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension d'entrée: torch.Size([32, 768])\n",
      "Dimension de sortie: torch.Size([32, 512])\n",
      "Nombre de paramètres entraînables: 5120\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_features,    # Dimension d'entrée\n",
    "                 out_features,   # Dimension de sortie\n",
    "                 rank=4,         # Rang de la décomposition\n",
    "                 alpha=1.0):     # Facteur d'échelle\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialisation des dimensions\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # Initialisation des matrices A et B\n",
    "        # A : matrice de dimension (rank x in_features)\n",
    "        # B : matrice de dimension (out_features x rank)\n",
    "        self.A = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        \n",
    "        # Initialisation des poids avec une distribution normale\n",
    "        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.B)  # Initialisation à zéro pour B\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x : tensor d'entrée de dimension (batch_size x in_features)\n",
    "        # Calcul de la transformation LoRA : x → (B·A)x\n",
    "        # 1. Multiplication avec A\n",
    "        first_proj = torch.matmul(x, self.A.T)  # (batch_size x rank)\n",
    "        # 2. Multiplication avec B et application du scaling\n",
    "        lora_output = torch.matmul(first_proj, self.B.T) * self.scaling  # (batch_size x out_features)\n",
    "        return lora_output\n",
    "\n",
    "# Test de la classe\n",
    "def test_lora_layer():\n",
    "    # Paramètres de test\n",
    "    batch_size = 32\n",
    "    in_features = 768   # Dimension typique d'un modèle transformeur\n",
    "    out_features = 512\n",
    "    rank = 4\n",
    "    \n",
    "    # Création de la couche LoRA\n",
    "    lora = LoRALayer(in_features, out_features, rank)\n",
    "    \n",
    "    # Création d'un tensor d'entrée aléatoire\n",
    "    x = torch.randn(batch_size, in_features)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = lora(x)\n",
    "    \n",
    "    # Vérifications\n",
    "    print(f\"Dimension d'entrée: {x.shape}\")\n",
    "    print(f\"Dimension de sortie: {output.shape}\")\n",
    "    print(f\"Nombre de paramètres entraînables: {sum(p.numel() for p in lora.parameters())}\")\n",
    "\n",
    "# Exécution du test\n",
    "test_lora_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test de LinearWithLoRA :\n",
      "Dimension d'entrée: torch.Size([32, 768])\n",
      "Dimension de sortie: torch.Size([32, 512])\n",
      "\n",
      "Paramètres entraînables:\n",
      "Nombre total de paramètres entraînables: 5120\n",
      "Dont LoRA: 5120\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2\n",
    "\n",
    "class LinearWithLoRA(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 out_features,\n",
    "                 rank=4,\n",
    "                 alpha=1.0,\n",
    "                 bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "        \n",
    "        self.lora = LoRALayer(\n",
    "            in_features=in_features,\n",
    "            out_features=out_features,\n",
    "            rank=rank,\n",
    "            alpha=alpha\n",
    "        )\n",
    "        \n",
    "        # Freeze les paramètres de la couche linéaire\n",
    "        for param in self.linear.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)\n",
    "\n",
    "# Fonction de test\n",
    "def test_linear_with_lora():\n",
    "    batch_size = 32\n",
    "    in_features = 768  # Comme dans votre projet de classification de mobiles\n",
    "    out_features = 512\n",
    "    rank = 4\n",
    "    \n",
    "    layer = LinearWithLoRA(\n",
    "        in_features=in_features,\n",
    "        out_features=out_features,\n",
    "        rank=rank\n",
    "    )\n",
    "    \n",
    "    x = torch.randn(batch_size, in_features)\n",
    "    output = layer(x)\n",
    "    \n",
    "    print(\"Test de LinearWithLoRA :\")\n",
    "    print(f\"Dimension d'entrée: {x.shape}\")\n",
    "    print(f\"Dimension de sortie: {output.shape}\")\n",
    "    print(\"\\nParamètres entraînables:\")\n",
    "    trainable_params = sum(p.numel() for p in layer.parameters() if p.requires_grad)\n",
    "    print(f\"Nombre total de paramètres entraînables: {trainable_params}\")\n",
    "    print(f\"Dont LoRA: {sum(p.numel() for p in layer.lora.parameters())}\")\n",
    "\n",
    "# Exécution du test\n",
    "if __name__ == \"__main__\":\n",
    "    test_linear_with_lora()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test des réseaux neuronaux :\n",
      "Dimension d'entrée: torch.Size([32, 768])\n",
      "Dimension de sortie (standard): torch.Size([32, 256])\n",
      "Dimension de sortie (LoRA): torch.Size([32, 256])\n",
      "\n",
      "Comparaison des paramètres :\n",
      "Paramètres entraînables (standard): 525056\n",
      "Paramètres entraînables (LoRA): 136448\n",
      "Réduction des paramètres: 74.01%\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNetworkWithLoRA(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim=768,    # Dimension similaire à vos exercices RAG\n",
    "                 hidden_dim=512,\n",
    "                 output_dim=256,\n",
    "                 rank=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Première couche avec LoRA\n",
    "        self.layer1 = LinearWithLoRA(\n",
    "            in_features=input_dim,\n",
    "            out_features=hidden_dim,\n",
    "            rank=rank\n",
    "        )\n",
    "        \n",
    "        # Activation ReLU\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Couche de sortie standard\n",
    "        self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "def test_network():\n",
    "    # Paramètres de test\n",
    "    batch_size = 32\n",
    "    input_dim = 768  # Dimension similaire à vos embeddings RAG\n",
    "    hidden_dim = 512\n",
    "    output_dim = 256\n",
    "    \n",
    "    # Création des réseaux (standard et avec LoRA)\n",
    "    network_standard = nn.Sequential(\n",
    "        nn.Linear(input_dim, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, output_dim)\n",
    "    )\n",
    "    \n",
    "    network_lora = SimpleNetworkWithLoRA(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        output_dim=output_dim\n",
    "    )\n",
    "    \n",
    "    # Données de test\n",
    "    x = torch.randn(batch_size, input_dim)\n",
    "    \n",
    "    # Test des deux réseaux\n",
    "    with torch.no_grad():\n",
    "        output_standard = network_standard(x)\n",
    "        output_lora = network_lora(x)\n",
    "    \n",
    "    # Affichage des résultats\n",
    "    print(\"Test des réseaux neuronaux :\")\n",
    "    print(f\"Dimension d'entrée: {x.shape}\")\n",
    "    print(f\"Dimension de sortie (standard): {output_standard.shape}\")\n",
    "    print(f\"Dimension de sortie (LoRA): {output_lora.shape}\")\n",
    "    \n",
    "    # Analyse des paramètres entraînables\n",
    "    params_standard = sum(p.numel() for p in network_standard.parameters() if p.requires_grad)\n",
    "    params_lora = sum(p.numel() for p in network_lora.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(\"\\nComparaison des paramètres :\")\n",
    "    print(f\"Paramètres entraînables (standard): {params_standard}\")\n",
    "    print(f\"Paramètres entraînables (LoRA): {params_lora}\")\n",
    "    print(f\"Réduction des paramètres: {(1 - params_lora/params_standard)*100:.2f}%\")\n",
    "\n",
    "# Exécution du test\n",
    "if __name__ == \"__main__\":\n",
    "    test_network()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test d'équivalence des sorties :\n",
      "Différence max avant fusion: 0.0\n",
      "Différence max après fusion: 0.0\n",
      "\n",
      "Comparaison des performances :\n",
      "Temps moyen (standard): 1.926 ms\n",
      "Temps moyen (fusionné): 0.873 ms\n",
      "Accélération: 2.21x\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class LinearWithLoRAMerged(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 out_features,\n",
    "                 rank=4,\n",
    "                 alpha=1.0,\n",
    "                 bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Couche linéaire standard\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "        \n",
    "        # Matrices LoRA\n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # Initialisation des poids LoRA\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "        \n",
    "        # Matrice des poids fusionnés (initialement None)\n",
    "        self.merged_weights = None\n",
    "        \n",
    "    def merge_weights(self):\n",
    "        \"\"\"Fusionne les poids LoRA avec la matrice de poids principale\"\"\"\n",
    "        if self.merged_weights is None:\n",
    "            # Calcul de la matrice LoRA\n",
    "            lora_matrix = torch.matmul(self.lora_B, self.lora_A) * self.scaling\n",
    "            \n",
    "            # Fusion avec les poids originaux\n",
    "            self.merged_weights = self.linear.weight.data.clone()\n",
    "            self.merged_weights += lora_matrix\n",
    "            \n",
    "    def unmerge_weights(self):\n",
    "        \"\"\"Restaure les poids originaux\"\"\"\n",
    "        self.merged_weights = None\n",
    "            \n",
    "    def forward(self, x):\n",
    "        if self.merged_weights is not None:\n",
    "            # Utilisation des poids fusionnés\n",
    "            return torch.nn.functional.linear(x, self.merged_weights, self.linear.bias)\n",
    "        else:\n",
    "            # Calcul séparé (comme dans LinearWithLoRA)\n",
    "            linear_output = self.linear(x)\n",
    "            lora_output = torch.matmul(x, self.lora_A.T)\n",
    "            lora_output = torch.matmul(lora_output, self.lora_B.T) * self.scaling\n",
    "            return linear_output + lora_output\n",
    "\n",
    "def test_lora_merged():\n",
    "    # Paramètres de test\n",
    "    batch_size = 32\n",
    "    in_features = 768  # Comme dans vos exercices RAG\n",
    "    out_features = 512\n",
    "    rank = 4\n",
    "    \n",
    "    # Création des modèles pour comparaison\n",
    "    model_standard = LinearWithLoRA(\n",
    "        in_features=in_features,\n",
    "        out_features=out_features,\n",
    "        rank=rank\n",
    "    )\n",
    "    \n",
    "    model_merged = LinearWithLoRAMerged(\n",
    "        in_features=in_features,\n",
    "        out_features=out_features,\n",
    "        rank=rank\n",
    "    )\n",
    "    \n",
    "    # Copie des poids pour assurer l'équivalence\n",
    "    model_merged.linear.weight.data = model_standard.linear.weight.data.clone()\n",
    "    model_merged.linear.bias.data = model_standard.linear.bias.data.clone()\n",
    "    model_merged.lora_A.data = model_standard.lora.A.data.clone()\n",
    "    model_merged.lora_B.data = model_standard.lora.B.data.clone()\n",
    "    \n",
    "    # Données de test\n",
    "    x = torch.randn(batch_size, in_features)\n",
    "    \n",
    "    # Test des sorties\n",
    "    with torch.no_grad():\n",
    "        output_standard = model_standard(x)\n",
    "        \n",
    "        # Test sans fusion\n",
    "        output_merged_before = model_merged(x)\n",
    "        \n",
    "        # Test avec fusion\n",
    "        model_merged.merge_weights()\n",
    "        output_merged_after = model_merged(x)\n",
    "    \n",
    "    # Vérification de l'équivalence\n",
    "    print(\"Test d'équivalence des sorties :\")\n",
    "    print(f\"Différence max avant fusion: {(output_standard - output_merged_before).abs().max().item()}\")\n",
    "    print(f\"Différence max après fusion: {(output_standard - output_merged_after).abs().max().item()}\")\n",
    "    \n",
    "    # Comparaison des performances\n",
    "    import time\n",
    "    \n",
    "    def measure_time(model, x, n_runs=1000):\n",
    "        start = time.time()\n",
    "        for _ in range(n_runs):\n",
    "            with torch.no_grad():\n",
    "                _ = model(x)\n",
    "        return (time.time() - start) / n_runs\n",
    "    \n",
    "    time_standard = measure_time(model_standard, x)\n",
    "    time_merged = measure_time(model_merged, x)\n",
    "    \n",
    "    print(\"\\nComparaison des performances :\")\n",
    "    print(f\"Temps moyen (standard): {time_standard*1000:.3f} ms\")\n",
    "    print(f\"Temps moyen (fusionné): {time_merged*1000:.3f} ms\")\n",
    "    print(f\"Accélération: {time_standard/time_merged:.2f}x\")\n",
    "\n",
    "# Exécution du test\n",
    "if __name__ == \"__main__\":\n",
    "    test_lora_merged()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture du MLP avec LoRA :\n",
      "Couche 0: LinearWithLoRAMerged\n",
      "Couche 1: ReLU\n",
      "Couche 2: Dropout\n",
      "Couche 3: LinearWithLoRAMerged\n",
      "Couche 4: ReLU\n",
      "Couche 5: Dropout\n",
      "Couche 6: LinearWithLoRAMerged\n",
      "\n",
      "Test de forward pass :\n",
      "Dimension d'entrée: torch.Size([32, 768])\n",
      "Dimension de sortie: torch.Size([32, 128])\n",
      "\n",
      "Comparaison des performances :\n",
      "Temps moyen (sans fusion): 3.095 ms\n",
      "Temps moyen (avec fusion): 1.990 ms\n",
      "Accélération: 1.55x\n",
      "\n",
      "Nombre total de paramètres entraînables: 567680\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLPWithLoRA(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim=768,    # Dimension similaire à vos embeddings RAG\n",
    "                 hidden_dims=[512, 256],\n",
    "                 output_dim=128,\n",
    "                 rank=4,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Construction des couches\n",
    "        dims = [input_dim] + hidden_dims + [output_dim]\n",
    "        for i in range(len(dims)-1):\n",
    "            # Ajout de la couche LinearWithLoRAMerged\n",
    "            self.layers.append(LinearWithLoRAMerged(\n",
    "                in_features=dims[i],\n",
    "                out_features=dims[i+1],\n",
    "                rank=rank\n",
    "            ))\n",
    "            \n",
    "            # Ajout de l'activation et dropout (sauf pour la dernière couche)\n",
    "            if i < len(dims)-2:\n",
    "                self.layers.append(nn.ReLU())\n",
    "                self.layers.append(nn.Dropout(dropout))\n",
    "    \n",
    "    def merge_lora_weights(self):\n",
    "        \"\"\"Fusionne les poids LoRA dans toutes les couches\"\"\"\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, LinearWithLoRAMerged):\n",
    "                layer.merge_weights()\n",
    "    \n",
    "    def unmerge_lora_weights(self):\n",
    "        \"\"\"Défusionne les poids LoRA dans toutes les couches\"\"\"\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, LinearWithLoRAMerged):\n",
    "                layer.unmerge_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "def test_mlp_with_lora():\n",
    "    # Paramètres de test\n",
    "    batch_size = 32\n",
    "    input_dim = 768  # Dimension type pour vos embeddings RAG\n",
    "    hidden_dims = [512, 256]\n",
    "    output_dim = 128\n",
    "    rank = 4\n",
    "    \n",
    "    # Création du modèle\n",
    "    model = MLPWithLoRA(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dims=hidden_dims,\n",
    "        output_dim=output_dim,\n",
    "        rank=rank\n",
    "    )\n",
    "    \n",
    "    # Affichage de l'architecture\n",
    "    print(\"Architecture du MLP avec LoRA :\")\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        print(f\"Couche {i}: {layer.__class__.__name__}\")\n",
    "    \n",
    "    # Test avec données\n",
    "    x = torch.randn(batch_size, input_dim)\n",
    "    \n",
    "    # Test avant fusion\n",
    "    print(\"\\nTest de forward pass :\")\n",
    "    output_before = model(x)\n",
    "    print(f\"Dimension d'entrée: {x.shape}\")\n",
    "    print(f\"Dimension de sortie: {output_before.shape}\")\n",
    "    \n",
    "    # Test des performances avec et sans fusion\n",
    "    import time\n",
    "    \n",
    "    def measure_time(model, x, n_runs=1000):\n",
    "        start = time.time()\n",
    "        for _ in range(n_runs):\n",
    "            with torch.no_grad():\n",
    "                _ = model(x)\n",
    "        return (time.time() - start) / n_runs\n",
    "    \n",
    "    # Test sans fusion\n",
    "    time_before = measure_time(model, x)\n",
    "    \n",
    "    # Test avec fusion\n",
    "    model.merge_lora_weights()\n",
    "    time_after = measure_time(model, x)\n",
    "    \n",
    "    print(\"\\nComparaison des performances :\")\n",
    "    print(f\"Temps moyen (sans fusion): {time_before*1000:.3f} ms\")\n",
    "    print(f\"Temps moyen (avec fusion): {time_after*1000:.3f} ms\")\n",
    "    print(f\"Accélération: {time_before/time_after:.2f}x\")\n",
    "    \n",
    "    # Calcul des paramètres entraînables\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nNombre total de paramètres entraînables: {trainable_params}\")\n",
    "\n",
    "# Exécution du test\n",
    "if __name__ == \"__main__\":\n",
    "    test_mlp_with_lora()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyse des paramètres après gel:\n",
      "Paramètre entraînable: layers.0.lora_A, Shape: torch.Size([4, 768])\n",
      "Paramètre entraînable: layers.0.lora_B, Shape: torch.Size([512, 4])\n",
      "Paramètre entraînable: layers.3.lora_A, Shape: torch.Size([4, 512])\n",
      "Paramètre entraînable: layers.3.lora_B, Shape: torch.Size([256, 4])\n",
      "Paramètre entraînable: layers.6.lora_A, Shape: torch.Size([4, 256])\n",
      "Paramètre entraînable: layers.6.lora_B, Shape: torch.Size([128, 4])\n",
      "\n",
      "Ratio de paramètres entraînables: 1.71%\n",
      "Total paramètres: 567680\n",
      "Paramètres entraînables: 9728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début de l'entraînement...\n",
      "Epoch 1/5\n",
      "Train Loss: 1.0128\n",
      "Val Loss: 0.9922\n",
      "\n",
      "Epoch 2/5\n",
      "Train Loss: 1.0086\n",
      "Val Loss: 0.9879\n",
      "\n",
      "Epoch 3/5\n",
      "Train Loss: 1.0045\n",
      "Val Loss: 0.9864\n",
      "\n",
      "Epoch 4/5\n",
      "Train Loss: 1.0027\n",
      "Val Loss: 0.9862\n",
      "\n",
      "Epoch 5/5\n",
      "Train Loss: 1.0021\n",
      "Val Loss: 0.9860\n",
      "\n",
      "\n",
      "Test final avec fusion LoRA:\n",
      "Dimension de sortie: torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def freeze_linear_layers(model):\n",
    "    \"\"\"Gèle les couches Linear standard et garde les paramètres LoRA entraînables\"\"\"\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, LinearWithLoRAMerged):\n",
    "            # Geler les poids de la couche linéaire standard\n",
    "            module.linear.weight.requires_grad = False\n",
    "            if module.linear.bias is not None:\n",
    "                module.linear.bias.requires_grad = False\n",
    "            \n",
    "            # Garder les paramètres LoRA entraînables\n",
    "            module.lora_A.requires_grad = True\n",
    "            module.lora_B.requires_grad = True\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"Affiche les détails des paramètres entraînables\"\"\"\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        all_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "            print(f\"Paramètre entraînable: {name}, Shape: {param.shape}\")\n",
    "    \n",
    "    print(f\"\\nRatio de paramètres entraînables: {trainable_params/all_params*100:.2f}%\")\n",
    "    print(f\"Total paramètres: {all_params}\")\n",
    "    print(f\"Paramètres entraînables: {trainable_params}\")\n",
    "\n",
    "def train_and_evaluate(model, train_loader, val_loader, epochs=10):\n",
    "    \"\"\"Entraîne et évalue le modèle\"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "    \n",
    "    print(\"Début de l'entraînement...\")\n",
    "    for epoch in range(epochs):\n",
    "        # Mode entraînement\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_x)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Mode évaluation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                output = model(batch_x)\n",
    "                val_loss += criterion(output, batch_y).item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "        print(f\"Val Loss: {val_loss/len(val_loader):.4f}\\n\")\n",
    "\n",
    "def test_frozen_mlp():\n",
    "    # Paramètres adaptés à votre contexte Solweig & Izar\n",
    "    input_dim = 768  # Dimension type pour vos embeddings\n",
    "    hidden_dims = [512, 256]\n",
    "    output_dim = 128\n",
    "    batch_size = 32\n",
    "    n_samples = 1000\n",
    "    \n",
    "    # Création du modèle\n",
    "    model = MLPWithLoRA(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dims=hidden_dims,\n",
    "        output_dim=output_dim\n",
    "    )\n",
    "    \n",
    "    # Gel des couches linéaires\n",
    "    freeze_linear_layers(model)\n",
    "    \n",
    "    # Affichage des paramètres entraînables\n",
    "    print(\"Analyse des paramètres après gel:\")\n",
    "    print_trainable_parameters(model)\n",
    "    \n",
    "    # Création d'un dataset synthétique (simulant vos données de design)\n",
    "    X = torch.randn(n_samples, input_dim)\n",
    "    y = torch.randn(n_samples, output_dim)  # Cible synthétique\n",
    "    \n",
    "    # Split train/val\n",
    "    train_size = int(0.8 * n_samples)\n",
    "    X_train, X_val = X[:train_size], X[train_size:]\n",
    "    y_train, y_val = y[:train_size], y[train_size:]\n",
    "    \n",
    "    # Création des dataloaders\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Entraînement et évaluation\n",
    "    train_and_evaluate(model, train_loader, val_loader, epochs=5)\n",
    "    \n",
    "    # Test final avec fusion LoRA\n",
    "    print(\"\\nTest final avec fusion LoRA:\")\n",
    "    model.merge_lora_weights()\n",
    "    with torch.no_grad():\n",
    "        test_input = torch.randn(1, input_dim)\n",
    "        output = model(test_input)\n",
    "        print(f\"Dimension de sortie: {output.shape}\")\n",
    "\n",
    "# Exécution du test\n",
    "if __name__ == \"__main__\":\n",
    "    test_frozen_mlp()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse de la convergence** :\n",
    "Epoch 1 → 5 :\n",
    "Train Loss: 1.0128 → 1.0021 (↓ 0.0107)\n",
    "Val Loss: 0.9922 → 0.9860 (↓ 0.0062)\n",
    "\n",
    "\n",
    "Cette évolution montre :\n",
    "1. **Une convergence stable** :\n",
    "   - Diminution régulière de la loss\n",
    "   - Pas de surapprentissage (validation loss suit la même tendance)\n",
    "\n",
    "2. **Performance optimisée** :\n",
    "   - Réduction de ~1% de l'erreur\n",
    "   - Stabilisation progressive\n",
    "\n",
    "3. **Dimension de sortie adaptée** :\n",
    "Dimension de sortie: torch.Size([1, 128])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
