{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN544jhFIPPqa4n9vlYMN5h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ludoveltz/test_github_fev25/blob/main/Exc_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q rouge-score transformers torch tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6y_eK22c7bb",
        "outputId": "fc9cf0ab-ff25-44dd-a408-cc97974d0e64"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Affichons le contenu pour trouver le bon chemin\n",
        "!ls \"/content/drive/MyDrive\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9v6jhqTd7DQ",
        "outputId": "64eeb641-ed97-47b3-a809-e3ab85b1756e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "'Colab Notebooks'  'Dogs vs Cats'\t\t      metadata.csv\n",
            " Data\t\t    household_power_consumption.txt   reponses_emotionnelles.json\n",
            " DATASET\t   'IMDB Dataset.csv'\t\t      structure_dossiers.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaIaYdfNYd9L",
        "outputId": "b136aff3-40b1-4556-9d12-fbe9edcc769b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "📊 Données chargées: 12120 exemples train, 5195 exemples test\n",
            "🖥️ Utilisation de: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-dcbf92cbe81a>:122: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  sample_df = self.train_df.groupby('language', group_keys=False).apply(\n",
            "Comparaison des modèles: 45it [04:28,  5.97s/it]\n",
            "<ipython-input-10-dcbf92cbe81a>:148: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  sample_df = self.train_df.groupby('language').apply(\n",
            "Génération des résumés: 75it [07:09,  5.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Scores ROUGE agrégés par modèle:\n",
            "\n",
            "Moyenne globale:\n",
            "          rouge1  rouge2  rougeL\n",
            "model                           \n",
            "gpt2       0.016  0.0062  0.0124\n",
            "t5-base    0.088  0.0370  0.0785\n",
            "t5-small   0.090  0.0387  0.0815\n",
            "\n",
            "Moyenne par langue et modèle:\n",
            "                     rouge1  rouge2  rougeL\n",
            "language   model                           \n",
            "Arabic     gpt2      0.0000  0.0000  0.0000\n",
            "           t5-base   0.0000  0.0000  0.0000\n",
            "           t5-small  0.0000  0.0000  0.0000\n",
            "Bulgarian  gpt2      0.0000  0.0000  0.0000\n",
            "           t5-base   0.0000  0.0000  0.0000\n",
            "           t5-small  0.0000  0.0000  0.0000\n",
            "Chinese    gpt2      0.0000  0.0000  0.0000\n",
            "           t5-base   0.0000  0.0000  0.0000\n",
            "           t5-small  0.0000  0.0000  0.0000\n",
            "English    gpt2      0.1598  0.0670  0.1122\n",
            "           t5-base   0.2476  0.1510  0.2476\n",
            "           t5-small  0.2106  0.1402  0.2106\n",
            "French     gpt2      0.0143  0.0050  0.0143\n",
            "           t5-base   0.2491  0.1043  0.1693\n",
            "           t5-small  0.2472  0.1047  0.1894\n",
            "German     gpt2      0.0000  0.0000  0.0000\n",
            "           t5-base   0.1374  0.0682  0.1265\n",
            "           t5-small  0.1699  0.0849  0.1699\n",
            "Greek      gpt2      0.0000  0.0000  0.0000\n",
            "           t5-base   0.0000  0.0000  0.0000\n",
            "           t5-small  0.0000  0.0000  0.0000\n",
            "Hindi      gpt2      0.0000  0.0000  0.0000\n",
            "           t5-base   0.0000  0.0000  0.0000\n",
            "           t5-small  0.0000  0.0000  0.0000\n",
            "Russian    gpt2      0.0000  0.0000  0.0000\n",
            "           t5-base   0.0000  0.0000  0.0000\n",
            "           t5-small  0.0000  0.0000  0.0000\n",
            "Spanish    gpt2      0.0000  0.0000  0.0000\n",
            "           t5-base   0.2229  0.0785  0.2229\n",
            "           t5-small  0.2309  0.1017  0.2309\n",
            "Swahili    gpt2      0.0000  0.0000  0.0000\n",
            "           t5-base   0.0678  0.0171  0.0678\n",
            "           t5-small  0.0684  0.0171  0.0684\n",
            "Thai       gpt2      0.0000  0.0000  0.0000\n",
            "           t5-base   0.0000  0.0000  0.0000\n",
            "           t5-small  0.0000  0.0000  0.0000\n",
            "Turkish    gpt2      0.0047  0.0000  0.0047\n",
            "           t5-base   0.1807  0.0846  0.1807\n",
            "           t5-small  0.1728  0.0800  0.1728\n",
            "Urdu       gpt2      0.0000  0.0000  0.0000\n",
            "           t5-base   0.0000  0.0000  0.0000\n",
            "           t5-small  0.0000  0.0000  0.0000\n",
            "Vietnamese gpt2      0.0613  0.0204  0.0546\n",
            "           t5-base   0.2144  0.0516  0.1626\n",
            "           t5-small  0.2502  0.0512  0.1801\n",
            "\n",
            "Écart-type par modèle:\n",
            "          rouge1  rouge2  rougeL\n",
            "model                           \n",
            "gpt2      0.0670  0.0266  0.0464\n",
            "t5-base   0.1261  0.0670  0.1125\n",
            "t5-small  0.1291  0.0695  0.1150\n",
            "✅ Résultats sauvegardés avec timestamp: 20250319_2244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer\n",
        ")\n",
        "from rouge_score import rouge_scorer\n",
        "from typing import Dict, List\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from google.colab import drive\n",
        "\n",
        "class ModelComparator:\n",
        "    def __init__(self, train_df: pd.DataFrame, test_df: pd.DataFrame):\n",
        "            \"\"\"\n",
        "            Initialisation des modèles et des tokenizers avec optimisation GPU\n",
        "            \"\"\"\n",
        "            self.train_df = train_df\n",
        "            self.test_df = test_df\n",
        "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "            if self.device.type == \"cuda\":\n",
        "                torch.cuda.empty_cache()  # Nettoyage de la mémoire GPU\n",
        "\n",
        "            print(f\"🖥️ Utilisation de: {self.device}\")\n",
        "\n",
        "            # Initialisation des modèles\n",
        "            self.models = {\n",
        "                't5-small': (\n",
        "                    T5ForConditionalGeneration.from_pretrained('t5-small').to(self.device),\n",
        "                    T5Tokenizer.from_pretrained('t5-small')\n",
        "                ),\n",
        "                't5-base': (\n",
        "                    T5ForConditionalGeneration.from_pretrained('t5-base').to(self.device),\n",
        "                    T5Tokenizer.from_pretrained('t5-base')\n",
        "                ),\n",
        "                'gpt2': (\n",
        "                    GPT2LMHeadModel.from_pretrained('gpt2').to(self.device),\n",
        "                    GPT2Tokenizer.from_pretrained('gpt2')\n",
        "                )\n",
        "            }\n",
        "\n",
        "            self.rouge_scorer = rouge_scorer.RougeScorer(\n",
        "                ['rouge1', 'rouge2', 'rougeL'],\n",
        "                use_stemmer=True\n",
        "            )\n",
        "\n",
        "\n",
        "    def generate_summary(self, text: str, model_name: str) -> str:\n",
        "        \"\"\"\n",
        "        Génération de résumé unifiée pour tous les modèles\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if model_name.startswith('t5'):\n",
        "                return self._generate_t5_summary(text, model_name)\n",
        "            else:\n",
        "                return self._generate_gpt2_summary(text)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Erreur génération résumé ({model_name}): {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def _generate_t5_summary(self, text: str, model_name: str) -> str:\n",
        "        model, tokenizer = self.models[model_name]\n",
        "        input_text = f\"summarize: {text}\"\n",
        "        inputs = tokenizer(input_text, max_length=512, truncation=True, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                inputs.input_ids,\n",
        "                max_length=150,\n",
        "                min_length=40,\n",
        "                num_beams=4,\n",
        "                length_penalty=2.0\n",
        "            )\n",
        "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    def _generate_gpt2_summary(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Génération de résumé avec GPT-2 avec gestion correcte du padding\n",
        "        \"\"\"\n",
        "        model, tokenizer = self.models['gpt2']\n",
        "\n",
        "        # Configuration du pad_token\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "            model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "        input_text = f\"{text}\\nTL;DR:\"\n",
        "        inputs = tokenizer(\n",
        "            input_text,\n",
        "            max_length=1024,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(self.device)\n",
        "\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    inputs.input_ids,\n",
        "                    attention_mask=inputs.attention_mask,\n",
        "                    max_new_tokens=150,\n",
        "                    min_length=30,\n",
        "                    num_beams=4,\n",
        "                    length_penalty=2.0,\n",
        "                    pad_token_id=tokenizer.pad_token_id\n",
        "                )\n",
        "\n",
        "            summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            return summary.split(\"TL;DR:\")[-1].strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Erreur génération GPT-2: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "\n",
        "\n",
        "    def compare_models(self, sample_size: int = 50) -> pd.DataFrame:\n",
        "        # Modification de l'échantillonnage stratifié\n",
        "        sample_df = self.train_df.groupby('language', group_keys=False).apply(\n",
        "            lambda x: x.sample(min(len(x), sample_size//len(self.train_df['language'].unique())))\n",
        "        ).reset_index(drop=True)\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for idx, row in tqdm(sample_df.iterrows(), desc=\"Comparaison des modèles\"):\n",
        "            for model_name in self.models.keys():\n",
        "                prediction = self.generate_summary(row['premise'], model_name)\n",
        "                scores = self.rouge_scorer.score(row['hypothesis'], prediction)\n",
        "\n",
        "                results.append({\n",
        "                    'model': model_name,\n",
        "                    'language': row['language'],\n",
        "                    'rouge1': scores['rouge1'].fmeasure,\n",
        "                    'rouge2': scores['rouge2'].fmeasure,\n",
        "                    'rougeL': scores['rougeL'].fmeasure\n",
        "                })\n",
        "\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "    def compare_models_summaries(self, sample_size: int = 5) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Comparaison côte à côte des résumés générés\n",
        "        \"\"\"\n",
        "        # Échantillonnage stratifié plus petit pour la lisibilité\n",
        "        sample_df = self.train_df.groupby('language').apply(\n",
        "            lambda x: x.sample(min(len(x), sample_size))\n",
        "        ).reset_index(drop=True)\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for idx, row in tqdm(sample_df.iterrows(), desc=\"Génération des résumés\"):\n",
        "            summary_row = {\n",
        "                'original_text': row['premise'],\n",
        "                'reference': row['hypothesis'],\n",
        "                'language': row['language']\n",
        "            }\n",
        "\n",
        "            # Génération des résumés pour chaque modèle\n",
        "            for model_name in self.models.keys():\n",
        "                summary = self.generate_summary(row['premise'], model_name)\n",
        "                summary_row[f'{model_name}_summary'] = summary\n",
        "\n",
        "                # Calcul des scores ROUGE\n",
        "                scores = self.rouge_scorer.score(row['hypothesis'], summary)\n",
        "                for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
        "                    summary_row[f'{model_name}_{metric}'] = scores[metric].fmeasure\n",
        "\n",
        "            results.append(summary_row)\n",
        "\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "def display_comparison_results(aggregated_scores: pd.DataFrame, summary_comparison: pd.DataFrame):\n",
        "    print(\"\\n📊 Scores ROUGE agrégés par modèle:\")\n",
        "\n",
        "    # Formatage des scores moyens\n",
        "    mean_scores = aggregated_scores.groupby('model')[['rouge1', 'rouge2', 'rougeL']].mean()\n",
        "    print(\"\\nMoyenne globale:\")\n",
        "    print(mean_scores.round(4))\n",
        "\n",
        "    # Formatage des scores par langue\n",
        "    lang_scores = aggregated_scores.groupby(['language', 'model'])[['rouge1', 'rouge2', 'rougeL']].mean()\n",
        "    print(\"\\nMoyenne par langue et modèle:\")\n",
        "    print(lang_scores.round(4))\n",
        "\n",
        "    # Ajout des écarts-types\n",
        "    print(\"\\nÉcart-type par modèle:\")\n",
        "    print(aggregated_scores.groupby('model')[['rouge1', 'rouge2', 'rougeL']].std().round(4))\n",
        "\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # Montage du Drive\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "        # Chemin vers les données\n",
        "        base_path = \"/content/drive/MyDrive/DATASET\"\n",
        "\n",
        "        # Chargement des données train uniquement\n",
        "        train_df = pd.read_csv(f\"{base_path}/train.csv\")\n",
        "        test_df = pd.read_csv(f\"{base_path}/test.csv\")  # Ajout du test_df\n",
        "        print(f\"📊 Données chargées: {len(train_df)} exemples train, {len(test_df)} exemples test\")\n",
        "\n",
        "        # Échantillonnage pour l'évaluation\n",
        "        sample_size = 50\n",
        "\n",
        "        # Initialisation et évaluation avec ModelComparator (et non ModelEvaluator)\n",
        "        comparator = ModelComparator(train_df, test_df)  # Utilisation du bon nom de classe\n",
        "\n",
        "        # Génération des résultats avec les deux méthodes\n",
        "        aggregated_scores = comparator.compare_models(sample_size)\n",
        "        summary_comparison = comparator.compare_models_summaries(5)\n",
        "\n",
        "        # Affichage des résultats\n",
        "        display_comparison_results(aggregated_scores, summary_comparison)\n",
        "\n",
        "        # Sauvegarde des résultats avec horodatage\n",
        "        from datetime import datetime\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "        # Sauvegarde des deux types de résultats\n",
        "        aggregated_scores.to_csv(f\"{base_path}/aggregated_scores_{timestamp}.csv\", index=False)\n",
        "        summary_comparison.to_csv(f\"{base_path}/summary_comparison_{timestamp}.csv\", index=False)\n",
        "        print(f\"✅ Résultats sauvegardés avec timestamp: {timestamp}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erreur: {e}\")\n",
        "        import traceback\n",
        "        print(traceback.format_exc())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance Globale des Modèles\n",
        "T5-Small (légèrement meilleur) :\n",
        "\n",
        "- ROUGE-1 : 0.090 (9.0%)\n",
        "- ROUGE-2 : 0.039 (3.9%)\n",
        "- ROUGE-L : 0.082 (8.2%)\n",
        "\n",
        "T5-Base (très proche) :\n",
        "- ROUGE-1 : 0.088 (8.8%)\n",
        "- ROUGE-2 : 0.037 (3.7%)\n",
        "- ROUGE-L : 0.079 (7.9%)\n",
        "\n",
        "GPT-2 (performances faibles) :\n",
        "- ROUGE-1 : 0.016 (1.6%)\n",
        "- ROUGE-2 : 0.006 (0.6%)\n",
        "- ROUGE-L : 0.012 (1.2%)\n",
        "\n",
        "\n",
        "Analyse par Langue\n",
        "Meilleures performances :\n",
        "\n",
        "Anglais :\n",
        "- T5-Base : ROUGE-1 = 0.248 (24.8%)\n",
        "- T5-Small : ROUGE-1 = 0.211 (21.1%)\n",
        "- GPT-2 : ROUGE-1 = 0.160 (16.0%)\n",
        "\n",
        "Français :\n",
        "- T5-Base : ROUGE-1 = 0.249 (24.9%)\n",
        "- T5-Small : ROUGE-1 = 0.247 (24.7%)\n",
        "\n",
        "Espagnol :\n",
        "- T5-Small : ROUGE-1 = 0.231 (23.1%)\n",
        "- T5-Base : ROUGE-1 = 0.223 (22.3%)\n",
        "\n",
        "Performances nulles (ROUGE = 0) :\n",
        "Arabic, Bulgarian, Chinese, Greek, Hindi, Russian, Thai, Urdu\n",
        "Points Clés 🔍\n",
        "\n",
        "a) Biais Linguistique :\n",
        "- Performances excellentes sur les langues européennes\n",
        "- Échec total sur les langues non-latines\n",
        "- Possible biais dans l'entraînement des modèles\n",
        "\n",
        "b) Écarts-types :\n",
        "- T5-Small : plus variable (σ = 0.129 pour ROUGE-1)\n",
        "- T5-Base : légèrement plus stable (σ = 0.126 pour ROUGE-1)\n",
        "- GPT-2 : moins variable mais performances faibles (σ = 0.067)\n",
        "\n",
        "Améliorations Multilingues :\n",
        "- Fine-tuning spécifique pour les langues non-latines\n",
        "- Utilisation de tokenizers adaptés aux différentes langues\n",
        "- Possible utilisation de modèles multilingues spécialisés (mT5, XLM-R)"
      ],
      "metadata": {
        "id": "dGBvYDdqn8FZ"
      }
    }
  ]
}