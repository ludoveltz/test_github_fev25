{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjHzyNaB84zQQi3e7aL2+F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ludoveltz/test_github_fev25/blob/main/Exc_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSdTWkIzZWzt",
        "outputId": "c7ebd0a7-76d1-4489-8321-de5e6ca29980"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "üìä Donn√©es charg√©es: 12120 exemples\n",
            "üñ•Ô∏è Utilisation de: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "√âvaluation de t5-small: 20it [00:54,  2.74s/it]\n",
            "√âvaluation de t5-base: 20it [02:47,  8.36s/it]\n",
            "√âvaluation de gpt2: 20it [05:55, 17.77s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä R√©sultats de l'√©valuation:\n",
            "\n",
            "ü§ñ Mod√®le: t5-small\n",
            "\n",
            "Scores moyens:\n",
            "rouge1    0.203724\n",
            "rouge2    0.055118\n",
            "rougeL    0.191737\n",
            "dtype: float64\n",
            "‚úÖ R√©sultats sauvegard√©s: /content/drive/MyDrive/DATASET/results_t5-small_20250319_2232.csv\n",
            "\n",
            "ü§ñ Mod√®le: t5-base\n",
            "\n",
            "Scores moyens:\n",
            "rouge1    0.194170\n",
            "rouge2    0.055759\n",
            "rougeL    0.181527\n",
            "dtype: float64\n",
            "‚úÖ R√©sultats sauvegard√©s: /content/drive/MyDrive/DATASET/results_t5-base_20250319_2232.csv\n",
            "\n",
            "ü§ñ Mod√®le: gpt2\n",
            "\n",
            "Scores moyens:\n",
            "rouge1    0.035225\n",
            "rouge2    0.005264\n",
            "rougeL    0.035225\n",
            "dtype: float64\n",
            "‚úÖ R√©sultats sauvegard√©s: /content/drive/MyDrive/DATASET/results_gpt2_20250319_2232.csv\n"
          ]
        }
      ],
      "source": [
        "# Installation des d√©pendances\n",
        "!pip install -q rouge-score transformers torch tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer\n",
        ")\n",
        "from rouge_score import rouge_scorer\n",
        "from typing import Dict, List\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from google.colab import drive\n",
        "\n",
        "class ModelEvaluator:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialisation des mod√®les et m√©triques\"\"\"\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"üñ•Ô∏è Utilisation de: {self.device}\")\n",
        "\n",
        "        # Initialisation des mod√®les\n",
        "        self.models = {\n",
        "            't5-small': (\n",
        "                T5ForConditionalGeneration.from_pretrained('t5-small').to(self.device),\n",
        "                T5Tokenizer.from_pretrained('t5-small')\n",
        "            ),\n",
        "            't5-base': (\n",
        "                T5ForConditionalGeneration.from_pretrained('t5-base').to(self.device),\n",
        "                T5Tokenizer.from_pretrained('t5-base')\n",
        "            ),\n",
        "            'gpt2': (\n",
        "                GPT2LMHeadModel.from_pretrained('gpt2').to(self.device),\n",
        "                GPT2Tokenizer.from_pretrained('gpt2', pad_token='<|endoftext|>')  # D√©finition explicite du pad_token\n",
        "            )\n",
        "        }\n",
        "\n",
        "        # Configuration sp√©ciale pour GPT2\n",
        "        self.models['gpt2'][1].padding_side = 'left'  # Important pour GPT2\n",
        "\n",
        "        # Initialisation du scorer ROUGE\n",
        "        self.rouge_scorer = rouge_scorer.RougeScorer(\n",
        "            ['rouge1', 'rouge2', 'rougeL'],\n",
        "            use_stemmer=True\n",
        "        )\n",
        "\n",
        "    def summarize_with_t5(self, text: str, model_name: str) -> str:\n",
        "        \"\"\"G√©n√©ration de r√©sum√© avec T5\"\"\"\n",
        "        try:\n",
        "            model, tokenizer = self.models[model_name]\n",
        "\n",
        "            inputs = tokenizer(\n",
        "                f\"summarize: {text}\",\n",
        "                max_length=512,\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    inputs.input_ids,\n",
        "                    max_new_tokens=150,  # Utilisation de max_new_tokens au lieu de max_length\n",
        "                    min_length=40,\n",
        "                    num_beams=4,\n",
        "                    length_penalty=2.0,\n",
        "                    early_stopping=True\n",
        "                )\n",
        "\n",
        "            return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur T5 ({model_name}): {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def summarize_with_gpt2(self, text: str) -> str:\n",
        "        \"\"\"G√©n√©ration de r√©sum√© avec GPT-2\"\"\"\n",
        "        try:\n",
        "            model, tokenizer = self.models['gpt2']\n",
        "\n",
        "            input_text = f\"{text}\\nTL;DR:\"\n",
        "            inputs = tokenizer(\n",
        "                input_text,\n",
        "                max_length=1024,\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                return_tensors=\"pt\",\n",
        "                return_attention_mask=True  # Explicitement demander le masque d'attention\n",
        "            ).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    input_ids=inputs.input_ids,\n",
        "                    attention_mask=inputs.attention_mask,\n",
        "                    max_new_tokens=150,  # Utilisation de max_new_tokens\n",
        "                    min_length=30,\n",
        "                    num_beams=4,\n",
        "                    length_penalty=2.0,\n",
        "                    pad_token_id=tokenizer.eos_token_id,\n",
        "                    early_stopping=True\n",
        "                )\n",
        "\n",
        "            summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            return summary.split(\"TL;DR:\")[-1].strip()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur GPT2: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def compute_rouge_score(self, reference: str, prediction: str) -> Dict[str, float]:\n",
        "        \"\"\"Calcul des scores ROUGE\"\"\"\n",
        "        scores = self.rouge_scorer.score(reference, prediction)\n",
        "        return {\n",
        "            'rouge1': scores['rouge1'].fmeasure,\n",
        "            'rouge2': scores['rouge2'].fmeasure,\n",
        "            'rougeL': scores['rougeL'].fmeasure\n",
        "        }\n",
        "\n",
        "    def compute_rouge_per_row(self, df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
        "        \"\"\"Calcul des scores ROUGE pour chaque ligne et chaque mod√®le\"\"\"\n",
        "        results = {}\n",
        "\n",
        "        for model_name in self.models.keys():\n",
        "            model_results = []\n",
        "\n",
        "            for _, row in tqdm(df.iterrows(), desc=f\"√âvaluation de {model_name}\"):\n",
        "                # G√©n√©ration du r√©sum√©\n",
        "                if model_name.startswith('t5'):\n",
        "                    prediction = self.summarize_with_t5(row['premise'], model_name)\n",
        "                else:\n",
        "                    prediction = self.summarize_with_gpt2(row['premise'])\n",
        "\n",
        "                # Calcul des scores ROUGE\n",
        "                scores = self.compute_rouge_score(row['hypothesis'], prediction)\n",
        "\n",
        "                model_results.append({\n",
        "                    'id': row['id'],\n",
        "                    'language': row['language'],\n",
        "                    'original': row['premise'],\n",
        "                    'reference': row['hypothesis'],\n",
        "                    'prediction': prediction,\n",
        "                    **scores\n",
        "                })\n",
        "\n",
        "            results[model_name] = pd.DataFrame(model_results)\n",
        "\n",
        "        return results\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # Montage du Drive et chargement des donn√©es\n",
        "        drive.mount('/content/drive')\n",
        "        base_path = \"/content/drive/MyDrive/DATASET\"\n",
        "\n",
        "        # Chargement des donn√©es\n",
        "        train_df = pd.read_csv(f\"{base_path}/train.csv\")\n",
        "        print(f\"üìä Donn√©es charg√©es: {len(train_df)} exemples\")\n",
        "\n",
        "        # √âchantillonnage pour l'√©valuation (r√©duit pour test)\n",
        "        sample_size = 20  # R√©duit √† 20 pour test\n",
        "        eval_df = train_df.sample(n=sample_size, random_state=42)\n",
        "\n",
        "        # Initialisation et √©valuation\n",
        "        evaluator = ModelEvaluator()\n",
        "        results = evaluator.compute_rouge_per_row(eval_df)\n",
        "\n",
        "        # Affichage et sauvegarde des r√©sultats\n",
        "        from datetime import datetime\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "        print(\"\\nüìä R√©sultats de l'√©valuation:\")\n",
        "        for model_name, df in results.items():\n",
        "            print(f\"\\nü§ñ Mod√®le: {model_name}\")\n",
        "            print(\"\\nScores moyens:\")\n",
        "            print(df[['rouge1', 'rouge2', 'rougeL']].mean())\n",
        "\n",
        "            # Sauvegarde avec timestamp\n",
        "            output_file = f\"{base_path}/results_{model_name}_{timestamp}.csv\"\n",
        "            df.to_csv(output_file, index=False)\n",
        "            print(f\"‚úÖ R√©sultats sauvegard√©s: {output_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur: {e}\")\n",
        "        import traceback\n",
        "        print(traceback.format_exc())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance Comparative des Mod√®les\n",
        "\n",
        "T5-Small (meilleure performance globale) :\n",
        "- ROUGE-1 : 0.204 (20.4%) - Meilleur score de correspondance de mots uniques\n",
        "- ROUGE-2 : 0.055 (5.5%) - Capture des bi-grammes\n",
        "- ROUGE-L : 0.192 (19.2%) - Meilleure s√©quence commune\n",
        "\n",
        "T5-Base (performance similaire) :\n",
        "- ROUGE-1 : 0.194 (19.4%)\n",
        "- ROUGE-2 : 0.056 (5.6%) - L√©g√®rement meilleur en bi-grammes\n",
        "- ROUGE-L : 0.182 (18.2%)\n",
        "\n",
        "GPT-2 (performance significativement inf√©rieure) :\n",
        "- ROUGE-1 : 0.035 (3.5%)\n",
        "- ROUGE-2 : 0.005 (0.5%)\n",
        "- ROUGE-L : 0.035 (3.5%)\n",
        "\n",
        "Analyse des Temps d'Ex√©cution\n",
        "- T5-Small : 2.74s/it\n",
        "- T5-Base : 8.36s/it\n",
        "- GPT-2 : 17.77s/it\n",
        "\n",
        "Points Cl√©s üîç\n",
        "a) Efficacit√© des Mod√®les T5 :\n",
        "- T5-Small surpasse T5-Base malgr√© sa taille r√©duite\n",
        "- Meilleur compromis performance/rapidit√©\n",
        "- Particuli√®rement efficace pour la correspondance lexicale (ROUGE-1)\n",
        "\n",
        "b) Limitations de GPT-2 :\n",
        "- Performances nettement inf√©rieures\n",
        "- Temps d'ex√©cution le plus long\n",
        "- Possible inad√©quation avec la t√¢che de r√©sum√©\n",
        "\n",
        "Choix de Mod√®le : T5-Small appara√Æt comme le meilleur choix pour :\n",
        "- Meilleure performance globale\n",
        "- Temps d'ex√©cution plus rapide\n",
        "- Utilisation efficace des ressources"
      ],
      "metadata": {
        "id": "X56j6OvFi3JK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iyd4PptNi07d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}