{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç V√©rification des fichiers...\n",
      "\n",
      "üìä Structure du Dataset:\n",
      "\n",
      "Colonnes du train.csv:\n",
      "['id', 'premise', 'hypothesis', 'lang_abv', 'language', 'label']\n",
      "\n",
      "Aper√ßu des premi√®res lignes train.csv:\n",
      "           id                                            premise  \\\n",
      "0  5130fd2cb5  and these comments were considered in formulat...   \n",
      "1  5b72532a0b  These are issues that we wrestle with in pract...   \n",
      "2  3931fbe82a  Des petites choses comme celles-l√† font une di...   \n",
      "3  5622f0c60b  you know they can't really defend themselves l...   \n",
      "4  86aaa48b45  ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡πà‡∏ô‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡∏™‡∏°‡∏°‡∏∏‡∏ï‡∏¥‡∏Å‡πá‡πÄ‡∏ä‡πà‡∏ô‡∏Å‡∏±‡∏ô ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÅ‡∏™‡∏î...   \n",
      "\n",
      "                                          hypothesis lang_abv language  label  \n",
      "0  The rules developed in the interim were put to...       en  English      0  \n",
      "1  Practice groups are not permitted to work on t...       en  English      2  \n",
      "2              J'essayais d'accomplir quelque chose.       fr   French      0  \n",
      "3  They can't defend themselves because of their ...       en  English      0  \n",
      "4    ‡πÄ‡∏î‡πá‡∏Å‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏´‡πá‡∏ô‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥‡∏û‡∏±‡∏ô‡∏ò‡∏∏‡πå‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£       th     Thai      1  \n",
      "\n",
      "Informations sur le dataset train:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12120 entries, 0 to 12119\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          12120 non-null  object\n",
      " 1   premise     12120 non-null  object\n",
      " 2   hypothesis  12120 non-null  object\n",
      " 3   lang_abv    12120 non-null  object\n",
      " 4   language    12120 non-null  object\n",
      " 5   label       12120 non-null  int64 \n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 568.2+ KB\n",
      "None\n",
      "\n",
      "Description statistique train:\n",
      "              label\n",
      "count  12120.000000\n",
      "mean       0.990759\n",
      "std        0.824523\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        1.000000\n",
      "75%        2.000000\n",
      "max        2.000000\n"
     ]
    }
   ],
   "source": [
    "# Exercice 1\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def verify_dataset_structure(base_path: str):\n",
    "    \"\"\"\n",
    "    V√©rifie la structure r√©elle du dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construction des chemins\n",
    "        train_path = os.path.join(base_path, \"train.csv\")\n",
    "        test_path = os.path.join(base_path, \"test.csv\")\n",
    "        \n",
    "        # V√©rification de l'existence des fichiers\n",
    "        print(\"üîç V√©rification des fichiers...\")\n",
    "        if not os.path.exists(train_path):\n",
    "            print(f\"‚ùå Le fichier train.csv n'existe pas dans {train_path}\")\n",
    "            return\n",
    "        if not os.path.exists(test_path):\n",
    "            print(f\"‚ùå Le fichier test.csv n'existe pas dans {test_path}\")\n",
    "            return\n",
    "            \n",
    "        # Chargement des donn√©es pour inspection\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "        \n",
    "        print(\"\\nüìä Structure du Dataset:\")\n",
    "        print(\"\\nColonnes du train.csv:\")\n",
    "        print(train_df.columns.tolist())\n",
    "        \n",
    "        print(\"\\nAper√ßu des premi√®res lignes train.csv:\")\n",
    "        print(train_df.head())\n",
    "        \n",
    "        print(\"\\nInformations sur le dataset train:\")\n",
    "        print(train_df.info())\n",
    "        \n",
    "        print(\"\\nDescription statistique train:\")\n",
    "        print(train_df.describe())\n",
    "        \n",
    "        return train_df, test_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors de la v√©rification: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def main():\n",
    "    base_path = \"/Users/ludovicveltz/Documents/Bootcamp_GENAI_2025/Crashcourse/WEEK_6/DAY_1/DATASET\"\n",
    "    train_df, test_df = verify_dataset_structure(base_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Analyse du Dataset NLI Multilingue:\n",
      "\n",
      "Distribution par langue:\n",
      "Arabic: 18 exemples\n",
      "Bulgarian: 18 exemples\n",
      "Chinese: 18 exemples\n",
      "English: 18 exemples\n",
      "French: 18 exemples\n",
      "German: 18 exemples\n",
      "Greek: 18 exemples\n",
      "Hindi: 18 exemples\n",
      "Russian: 18 exemples\n",
      "Spanish: 18 exemples\n",
      "Swahili: 18 exemples\n",
      "Thai: 18 exemples\n",
      "Turkish: 18 exemples\n",
      "Urdu: 18 exemples\n",
      "Vietnamese: 18 exemples\n",
      "\n",
      "Distribution des labels:\n",
      "Contradiction: 90 exemples\n",
      "Neutre: 90 exemples\n",
      "Implication: 90 exemples\n",
      "\n",
      "Longueur moyenne des textes:\n",
      "Premise: 101 caract√®res\n",
      "Hypothesis: 51 caract√®res\n",
      "\n",
      "Exemples par langue:\n",
      "\n",
      "Arabic:\n",
      "Premise: ŸÉÿßŸÜ ŸÑŸÑÿ™ŸÑÿßŸÖŸäÿ∞ ÿ®ŸÖÿ´ÿßÿ®ÿ© ŸÜÿßÿµÿ≠ÿå ŸàŸÖÿπŸÑŸÖÿå ŸàŸÇÿ≥Ÿäÿ≥ÿå ŸàÿπŸÖÿå ŸàÿµÿØŸäŸÇ ÿ≠ŸÇŸäŸÇŸä....\n",
      "Hypothesis: ŸÑŸÇÿØ ŸÉÿßŸÜ ŸÖŸÅŸäÿØŸãÿß ÿ¨ÿØŸãÿß ŸÑÿ∑ŸÑÿßÿ®Ÿá....\n",
      "Label: Contradiction\n",
      "\n",
      "Bulgarian:\n",
      "Premise: –ö–∞—Ç–æ —á–ª–µ–Ω –Ω–∞ –í—ä—Ç—Ä–µ—à–Ω–∏—è –∫—Ä—ä–≥ —â–µ –ø–æ–ª—É—á–∏—Ç–µ –∏–∑–±—Ä–∞–Ω–æ –º—è—Å—Ç–æ –ø–æ –≤—Ä–µ–º–µ –Ω–∞ –ö–æ–Ω–≤–µ–Ω—Ü–∏—è—Ç–∞ –∏ —Å–ø–µ—Ü–∏–∞–ª–Ω–∏ –ø–æ–∫–∞–Ω–∏ –∑–∞ ...\n",
      "Hypothesis: –ß–ª–µ–Ω–æ–≤–µ—Ç–µ –Ω–∞ –í—ä—Ç—Ä–µ—à–Ω–∏—è –∫—Ä—ä–≥ –ø–æ–ª—É—á–∞–≤–∞—Ç —Ä–∞–∑–ª–∏—á–Ω–∏ –±–æ–Ω—É—Å–∏....\n",
      "Label: Contradiction\n",
      "\n",
      "Chinese:\n",
      "Premise: Êàë‰ªäÂ§©Êó©‰∏äÂà∞ÈÇ£Èáå, ÂëÉÔºåÊàëÂøò‰∫ÜÊòØÊàëÈóÆ‰∫Ü‰∏Ä‰∏™ÈóÆÈ¢òËøòÊòØ‰ªñËøõÊù•‰∫Ü, Èöè‰æøÂêß„ÄÇ...\n",
      "Hypothesis: ‰ªäÂ§©Êó©‰∏äÊàëÊù•‰∫ÜÔºå‰ªñ‰πüÊù•‰∫Ü„ÄÇ...\n",
      "Label: Contradiction\n",
      "\n",
      "English:\n",
      "Premise: The Star reports that actress Jodie Foster is pregnant through artificial insemination....\n",
      "Hypothesis: It has been reported by The Star that actress Jodie Foster is pregnant through artificial inseminati...\n",
      "Label: Contradiction\n",
      "\n",
      "French:\n",
      "Premise: Avant que tu ne me donnes une fess√©e, pourquoi ne me laisserais-tu pas juste avoir un grand verre de...\n",
      "Hypothesis: Je voudrais un verre de lait au chocolat....\n",
      "Label: Contradiction\n",
      "\n",
      "German:\n",
      "Premise: Der Zugang zu unserem Gel√§nde wird f√ºr jeden mit einem Computer und einem Modem ge√∂ffnet....\n",
      "Hypothesis: Die Leute brauchen sowohl einen Computer als auch ein Modem, um das Gel√§nde zu betreten....\n",
      "Label: Contradiction\n",
      "\n",
      "Greek:\n",
      "Premise: ŒïŒΩ œÑœâ ŒºŒµœÑŒ±Œæœç, œÑŒø Caldas de Monchique ŒµŒØŒΩŒ±Œπ Œ≠ŒΩŒ± Œ∫Œ±Œªœå ŒºŒ≠œÅŒøœÇ Œ≥ŒπŒ± Œ≠ŒΩŒ± œÄŒπŒ∫ŒΩŒØŒ∫ Œ∫Œ±Œπ ŒºŒπŒ± Œ≤œåŒªœÑŒ± œÉœÑŒø Œ¥Œ¨œÉŒøœÇ....\n",
      "Hypothesis: Œ•œÄŒ¨œÅœáŒøœÖŒΩ Œ¥Œ¨œÉŒ∑ Œ≥œçœÅœâ Œ±œÄœå œÑŒø Caldas de Monchique....\n",
      "Label: Contradiction\n",
      "\n",
      "Hindi:\n",
      "Premise: ‡§Æ‡•à‡§Ç ‡§¶‡•á‡§ñ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å, ‡§Æ‡§π‡•ã‡§¶‡§Ø, ‡§ï‡§ø ‡§Ü‡§™ ‡§™‡§∞‡§ø‡§∏‡•ç‡§•‡§ø‡§§‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•ã ‡§Ö‡§≠‡•Ä ‡§§‡§ï ‡§®‡§π‡•Ä‡§Ç ‡§∏‡§Æ‡§ù‡•á ‡§π‡•à ‡•§...\n",
      "Hypothesis: ‡§Æ‡§à ‡§§‡•Å‡§ù‡§™‡•á ‡§µ‡§ø‡§∂‡•ç‡§µ‡§æ‡§∏ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞‡§§‡§æ ‡§∏‡§æ‡§°‡•Ä ‡§π‡§æ‡§≤‡§§ ‡§∏‡§Æ‡§ù ‡§≤‡•ã...\n",
      "Label: Contradiction\n",
      "\n",
      "Russian:\n",
      "Premise: –ü–µ—Ä–≤–∞—è –≥—Ä—É–ø–ø–∞ NYUD ESU –≤–æ—à–ª–∞ –≤ –≤–µ—Å—Ç–∏–±—é–ª—å –°–µ–≤–µ—Ä–Ω–æ–π –±–∞—à–Ω–∏ –Ω–∞ –ó–∞–ø–∞–¥–Ω–æ–π —É–ª–∏—Ü–µ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∏–ª–∞—Å—å –∫ –ø–æ–¥—ä–µ–º—É ...\n",
      "Hypothesis: –í 09:15 –±–∞—à–Ω—è –µ—â–µ —Å—Ç–æ—è–ª–∞....\n",
      "Label: Contradiction\n",
      "\n",
      "Spanish:\n",
      "Premise: Las met√°foras animales originales son pr√°cticamente destru√≠das con palabras que no hacen referencia ...\n",
      "Hypothesis: Las met√°foras de animales pr√°cticamente han desaparecido....\n",
      "Label: Contradiction\n",
      "\n",
      "Swahili:\n",
      "Premise: Mnamo mwaka wa 1868 na kupitishwa kwa Marekebisho ya Kumi na nne, tulikuwa tumezingatia kizingiti ch...\n",
      "Hypothesis: Marekebisho ya kumi na nne yalianzishwa mwaka wa 1868....\n",
      "Label: Contradiction\n",
      "\n",
      "Thai:\n",
      "Premise: ‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏â‡∏±‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏ñ‡∏∂‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏î‡∏°‡∏ó‡∏∏‡∏ô...\n",
      "Hypothesis: ‡∏â‡∏±‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏π‡∏î‡∏Ñ‡∏∏‡∏¢‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏î‡∏°‡∏ó‡∏∏‡∏ô...\n",
      "Label: Contradiction\n",
      "\n",
      "Turkish:\n",
      "Premise: ≈û√∂valye, atƒ±na, Latin caballus atƒ±yla baƒülƒ±dƒ±r....\n",
      "Hypothesis: ≈û√∂valyenin adƒ±, atƒ±yla olan baƒüƒ±na i≈üaret ediyor....\n",
      "Label: Contradiction\n",
      "\n",
      "Urdu:\n",
      "Premise: ŸÖ€ÅÿØÿß€Åÿ± ŸÜ€í ÿßŸæŸÜ€í ŸÖÿ∑ŸÑŸàÿ®€Å ÿÆÿ∑ÿßÿ® ⁄©Ÿà ŸÜ€åŸà€åÿßÿ±⁄© ÿ¥€Åÿ±ÿå ŸÖ€åÿ±ŸπŸàÿ™ €ÅŸàŸπŸÑ ⁄©€í ÿ∑Ÿàÿ± Ÿæÿ± ÿØ€åÿßÿå ŸÑ€å⁄©ŸÜ ÿßÿ≥ ⁄©€í ÿ®ÿ¨ÿßÿ¶€í ⁄©ÿ≥€å ÿßŸàÿ±ŸÜ€åŸà €åÿß...\n",
      "Hypothesis: ŸÖÿØ⁄æÿßÿ± ŸÜ€í ÿßÿ≥ €ÅŸàŸπŸÑ ŸÖ€å⁄∫ ŸÜ€Å€å⁄∫ ŸÇ€åÿßŸÖ ŸÜ€Å€å⁄∫ ⁄©€åÿß  ÿ¨ÿ≥ ⁄©€í ÿ®ÿßÿ±€í ŸÖ€å⁄∫ ÿßÿ≥ ŸÜ€í ⁄©€Åÿß ÿ™⁄æÿß€î...\n",
      "Label: Contradiction\n",
      "\n",
      "Vietnamese:\n",
      "Premise: T√†i s·∫£n t√≠ch l≈©y c√≥ th·ªÉ t·∫°o ra thu nh·∫≠p d∆∞·ªõi h√¨nh th·ª©c l√£i su·∫•t v√† c·ªï t·ª©c v√† nh·ªØng kho·∫£n n√†y c√≥ th·ªÉ ...\n",
      "Hypothesis: B·∫°n c√≥ th·ªÉ t·∫°o thu nh·∫≠p v·ªõi t√†i s·∫£n....\n",
      "Label: Contradiction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3h/ppgwfhhj1rz7_5v8s2zf1pwr0000gn/T/ipykernel_78859/3709443932.py:23: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train_sample = (train_df.groupby(['language', 'label'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "class NLIDatasetExplorer:\n",
    "    def __init__(self, base_path: str):\n",
    "        \"\"\"\n",
    "        Initialisation de l'explorateur pour le dataset NLI multilingue\n",
    "        \"\"\"\n",
    "        self.base_path = base_path\n",
    "        self.label_map = {\n",
    "            0: \"Contradiction\",\n",
    "            1: \"Neutre\",\n",
    "            2: \"Implication\"\n",
    "        }\n",
    "        \n",
    "    def load_datasets(self, train_sample_size: int = 100, test_sample_size: int = 50):\n",
    "        try:\n",
    "            train_df = pd.read_csv(os.path.join(self.base_path, \"train.csv\"))\n",
    "            test_df = pd.read_csv(os.path.join(self.base_path, \"test.csv\"))\n",
    "            \n",
    "            # Correction du groupby deprecated\n",
    "            train_sample = (train_df.groupby(['language', 'label'])\n",
    "                        .apply(lambda x: x.sample(min(len(x), train_sample_size//len(train_df['language'].unique()))))\n",
    "                        .reset_index(drop=True))\n",
    "            \n",
    "            return train_sample, test_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur: {e}\")\n",
    "            return None, None\n",
    "\n",
    "\n",
    "    def explore_dataset(self, train_df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyse exploratoire du dataset NLI\n",
    "        \"\"\"\n",
    "        stats = {\n",
    "            'distribution_langues': train_df['language'].value_counts().to_dict(),\n",
    "            'distribution_labels': train_df['label'].value_counts().to_dict(),\n",
    "            'longueur_moyenne': {\n",
    "                'premise': train_df['premise'].str.len().mean(),\n",
    "                'hypothesis': train_df['hypothesis'].str.len().mean()\n",
    "            },\n",
    "            'exemples_par_langue': {}\n",
    "        }\n",
    "        \n",
    "        # Extraire un exemple par langue\n",
    "        for lang in train_df['language'].unique():\n",
    "            exemple = train_df[train_df['language'] == lang].iloc[0]\n",
    "            stats['exemples_par_langue'][lang] = {\n",
    "                'premise': exemple['premise'],\n",
    "                'hypothesis': exemple['hypothesis'],\n",
    "                'label': self.label_map[exemple['label']]\n",
    "            }\n",
    "            \n",
    "        return stats\n",
    "\n",
    "def main():\n",
    "    base_path = \"/Users/ludovicveltz/Documents/Bootcamp_GENAI_2025/Crashcourse/WEEK_6/DAY_1/DATASET\"\n",
    "    explorer = NLIDatasetExplorer(base_path)\n",
    "    \n",
    "    train_df, test_df = explorer.load_datasets()\n",
    "    \n",
    "    if train_df is not None:\n",
    "        stats = explorer.explore_dataset(train_df)\n",
    "        \n",
    "        print(\"\\nüìä Analyse du Dataset NLI Multilingue:\")\n",
    "        \n",
    "        print(\"\\nDistribution par langue:\")\n",
    "        for lang, count in stats['distribution_langues'].items():\n",
    "            print(f\"{lang}: {count} exemples\")\n",
    "        \n",
    "        print(\"\\nDistribution des labels:\")\n",
    "        for label_id, count in stats['distribution_labels'].items():\n",
    "            print(f\"{explorer.label_map[label_id]}: {count} exemples\")\n",
    "        \n",
    "        print(\"\\nLongueur moyenne des textes:\")\n",
    "        print(f\"Premise: {stats['longueur_moyenne']['premise']:.0f} caract√®res\")\n",
    "        print(f\"Hypothesis: {stats['longueur_moyenne']['hypothesis']:.0f} caract√®res\")\n",
    "        \n",
    "        print(\"\\nExemples par langue:\")\n",
    "        for lang, exemple in stats['exemples_par_langue'].items():\n",
    "            print(f\"\\n{lang}:\")\n",
    "            print(f\"Premise: {exemple['premise'][:100]}...\")\n",
    "            print(f\"Hypothesis: {exemple['hypothesis'][:100]}...\")\n",
    "            print(f\"Label: {exemple['label']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des donn√©es...\n",
      "Initialisation du summarizer...\n",
      "Utilisation de : cpu\n",
      "Chargement du tokenizer...\n",
      "Chargement du mod√®le...\n",
      "\n",
      "Traitement des textes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G√©n√©ration des r√©sum√©s: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:18<00:00,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R√©sultats :\n",
      "\n",
      "üìä R√©sultats de la g√©n√©ration :\n",
      "\n",
      "Exemple 1\n",
      "--------------------------------------------------------------------------------\n",
      "Texte Original :\n",
      "–ö—Ç–æ? –û–Ω–∞ —Å–ø—Ä–æ—Å–∏–ª–∞ –µ–≥–æ —Å –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–º –∏–Ω—Ç–µ—Ä–µ—Å–æ–º.\n",
      "\n",
      "R√©sum√© G√©n√©r√© :\n",
      "–Ω–∞ —Å—Ä–æ—Å–∏–ª–∞ –µ–æ —Å –Ω–µ–æ–∏–¥–∞–Ω–Ω–º –∏–Ω—Ç–µ—Ä–µ—Å–æ–º.\n",
      "\n",
      "Hypoth√®se :\n",
      "–û–Ω–∞ —Å–ø—Ä–æ—Å–∏–ª–∞, –∫–∞–∫ —ç—Ç–æ —Å–¥–µ–ª–∞—Ç—å, —Ç–∞–∫ –∫–∞–∫ —Å –µ—ë —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —ç—Ç–æ –∫–∞–∑–∞–ª–æ—Å—å –Ω–µ–≤–æ–∑–º–æ–∂–Ω—ã–º.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Exemple 2\n",
      "--------------------------------------------------------------------------------\n",
      "Texte Original :\n",
      "Others are Zao (in Tohoku) and a number of resorts in Joshin-etsu Kogen National Park in the Japan Alps, where there are now splendid facilities thanks to the 1998 Winter Olympic Games in Nagano.\n",
      "\n",
      "R√©sum√© G√©n√©r√© :\n",
      "a number of resorts in the japanese Alps are now in the area thanks to the 1998 winter games in Nagano.\n",
      "\n",
      "Hypoth√®se :\n",
      "There are a lot of resorts in the national park.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Exemple 3\n",
      "--------------------------------------------------------------------------------\n",
      "Texte Original :\n",
      "trying to keep grass alive during a summer on a piece of ground that big was expensive\n",
      "\n",
      "R√©sum√© G√©n√©r√© :\n",
      "trying to keep grass alive during a summer on a piece of ground that big was expensive.\n",
      "\n",
      "Hypoth√®se :\n",
      "There was no cost in keeping the grass alive in the summer time.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Exemple 4\n",
      "--------------------------------------------------------------------------------\n",
      "Texte Original :\n",
      "so i guess my experience is is just with what we did and and so they didn't really go through the child care route they were able to be home together\n",
      "\n",
      "R√©sum√© G√©n√©r√© :\n",
      "so i guess my experience is just with what we did and so they didn't really go through the child care route they were able to be home together. so they didn't really go through the route they were able to be home together together.\n",
      "\n",
      "Hypoth√®se :\n",
      "They were able to be home rather than having to worry about getting child care.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Exemple 5\n",
      "--------------------------------------------------------------------------------\n",
      "Texte Original :\n",
      "The Journal put the point succinctly to  Is any publicity good publicity?\n",
      "\n",
      "R√©sum√© G√©n√©r√© :\n",
      "the Journal put the point succinctly to Is any publicity good publicity?\n",
      "\n",
      "Hypoth√®se :\n",
      "The Journal asked \"Is this a good political move?\"\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üìà Statistiques :\n",
      "Longueur moyenne des textes originaux : 110 caract√®res\n",
      "Longueur moyenne des r√©sum√©s g√©n√©r√©s : 106 caract√®res\n",
      "Longueur moyenne des hypoth√®ses : 65 caract√®res\n",
      "\n",
      "‚úÖ R√©sultats sauvegard√©s dans t5_summaries_results_light.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercice 2\n",
    "\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "import pandas as pd\n",
    "import gc\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "class T5SummarizerLight:\n",
    "    def __init__(self, model_name: str = \"t5-small\"):\n",
    "        \"\"\"\n",
    "        Version optimis√©e du summarizer T5\n",
    "        \"\"\"\n",
    "        # V√©rification et configuration CUDA\n",
    "        self.device = \"cpu\"  \n",
    "        if torch.cuda.is_available():\n",
    "            # Limitation de la m√©moire CUDA\n",
    "            torch.cuda.set_per_process_memory_fraction(0.7)  # Utilise 70% max de la VRAM\n",
    "            self.device = \"cuda\"\n",
    "        print(f\"Utilisation de : {self.device}\")\n",
    "        \n",
    "        # Param√®tres optimis√©s\n",
    "        self.batch_size = 4  # Batch size r√©duit\n",
    "        self.max_length = 100  # Longueur maximale r√©duite\n",
    "        \n",
    "        # Chargement progressif\n",
    "        print(\"Chargement du tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        print(\"Chargement du mod√®le...\")\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            low_cpu_mem_usage=True,  # Optimisation m√©moire CPU\n",
    "            torch_dtype=torch.float32  # Pr√©cision standard\n",
    "        ).to(self.device)\n",
    "\n",
    "    def clean_memory(self):\n",
    "        \"\"\"Nettoyage m√©moire am√©lior√©\"\"\"\n",
    "        if self.device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    def summarize_single_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Traitement d'un seul texte √† la fois\n",
    "        \"\"\"\n",
    "        try:\n",
    "            input_text = f\"summarize: {text}\"\n",
    "            \n",
    "            # Tokenization avec gestion de la longueur\n",
    "            inputs = self.tokenizer(\n",
    "                input_text,\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # G√©n√©ration avec param√®tres optimis√©s\n",
    "            with torch.no_grad():\n",
    "                output_ids = self.model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    max_length=self.max_length,\n",
    "                    num_beams=2,  # R√©duit pour √©conomiser la m√©moire\n",
    "                    length_penalty=1.5,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "            \n",
    "            summary = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "            self.clean_memory()\n",
    "            return summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du r√©sum√©: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def process_texts(self, texts: List[str], sample_size: int = None) -> List[str]:\n",
    "        \"\"\"\n",
    "        Traitement des textes avec gestion d'√©chantillon\n",
    "        \"\"\"\n",
    "        if sample_size:\n",
    "            texts = texts[:sample_size]\n",
    "        \n",
    "        summaries = []\n",
    "        for text in tqdm(texts, desc=\"G√©n√©ration des r√©sum√©s\"):\n",
    "            summary = self.summarize_single_text(text)\n",
    "            summaries.append(summary)\n",
    "            \n",
    "        return summaries\n",
    "\n",
    "def display_results(results_df):\n",
    "    \"\"\"\n",
    "    Affiche les r√©sultats avec un meilleur formatage\n",
    "    \"\"\"\n",
    "    print(\"\\nüìä R√©sultats de la g√©n√©ration :\\n\")\n",
    "    \n",
    "    for idx, row in results_df.iterrows():\n",
    "        print(f\"Exemple {idx + 1}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"Texte Original :\")\n",
    "        print(f\"{row['Texte_Original']}\\n\")\n",
    "        print(f\"R√©sum√© G√©n√©r√© :\")\n",
    "        print(f\"{row['R√©sum√©_G√©n√©r√©']}\\n\")\n",
    "        print(f\"Hypoth√®se :\")\n",
    "        print(f\"{row['Hypoth√®se']}\\n\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Chemin des donn√©es\n",
    "        base_path = \"/Users/ludovicveltz/Documents/Bootcamp_GENAI_2025/Crashcourse/WEEK_6/DAY_1/DATASET\"\n",
    "        \n",
    "        # Chargement avec gestion d'erreurs\n",
    "        print(\"Chargement des donn√©es...\")\n",
    "        try:\n",
    "            train_df = pd.read_csv(f\"{base_path}/train.csv\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur de chargement des donn√©es: {e}\")\n",
    "            return\n",
    "        \n",
    "        # √âchantillon tr√®s r√©duit pour test\n",
    "        sample_size = 5  # Commencez avec un petit √©chantillon\n",
    "        train_sample = train_df.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "        # Initialisation et traitement\n",
    "        print(\"Initialisation du summarizer...\")\n",
    "        summarizer = T5SummarizerLight()\n",
    "        \n",
    "        print(\"\\nTraitement des textes...\")\n",
    "        premises = train_sample['premise'].tolist()\n",
    "        summaries = summarizer.process_texts(premises)\n",
    "        \n",
    "        # Cr√©ation et sauvegarde des r√©sultats\n",
    "        results_df = pd.DataFrame({\n",
    "            'Texte_Original': premises,\n",
    "            'R√©sum√©_G√©n√©r√©': summaries,\n",
    "            'Hypoth√®se': train_sample['hypothesis'].tolist()\n",
    "        })\n",
    "        \n",
    "        # Modification de l'affichage\n",
    "        pd.set_option('display.max_colwidth', None)\n",
    "        pd.set_option('display.expand_frame_repr', False)  # √âvite le retour √† la ligne automatique\n",
    "        pd.set_option('display.max_rows', None)  # Affiche toutes les lignes\n",
    "\n",
    "        # Pour l'affichage des r√©sultats\n",
    "        print(\"\\nR√©sultats :\")\n",
    "        pd.set_option('display.max_colwidth', None)\n",
    "        display_results(results_df)\n",
    "        \n",
    "        # Statistiques additionnelles\n",
    "        print(\"\\nüìà Statistiques :\")\n",
    "        print(f\"Longueur moyenne des textes originaux : {results_df['Texte_Original'].str.len().mean():.0f} caract√®res\")\n",
    "        print(f\"Longueur moyenne des r√©sum√©s g√©n√©r√©s : {results_df['R√©sum√©_G√©n√©r√©'].str.len().mean():.0f} caract√®res\")\n",
    "        print(f\"Longueur moyenne des hypoth√®ses : {results_df['Hypoth√®se'].str.len().mean():.0f} caract√®res\")\n",
    "        \n",
    "        # Sauvegarde\n",
    "        try:\n",
    "            results_df.to_csv(f\"{base_path}/t5_summaries_results_light.csv\", index=False)\n",
    "            print(\"\\n‚úÖ R√©sultats sauvegard√©s dans t5_summaries_results_light.csv\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Erreur lors de la sauvegarde : {e}\")\n",
    "        \n",
    "    finally:\n",
    "        # Nettoyage final\n",
    "        if 'summarizer' in locals():\n",
    "            summarizer.clean_memory()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des donn√©es...\n",
      "Initialisation du summarizer...\n",
      "Utilisation de : cpu\n",
      "Chargement du tokenizer...\n",
      "Chargement du mod√®le...\n",
      "\n",
      "Traitement des textes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G√©n√©ration des r√©sum√©s: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:18<00:00,  3.64s/it]\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ludovicveltz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "√âvaluation des r√©sultats...\n",
      "\n",
      "üìä R√©sultats de l'√©valuation :\n",
      "\n",
      "M√©triques globales :\n",
      "M√©trique        Score      Interpr√©tation\n",
      "------------------------------------------------------------\n",
      "exact_match     0.0000     Correspondance exacte (tr√®s stricte)\n",
      "rouge1          0.2783     Chevauchement de mots uniques\n",
      "rouge2          0.1167     Chevauchement de bigrammes\n",
      "rougeL          0.2392     Plus longue sous-s√©quence commune\n",
      "bleu            0.0540     Pr√©cision des n-grammes\n",
      "\n",
      "üí° Analyse des r√©sultats :\n",
      "\n",
      "1. Scores faibles attendus car :\n",
      "   - T5 est entra√Æn√© pour le r√©sum√©, pas pour NLI\n",
      "   - Les hypoth√®ses NLI ne sont pas des r√©sum√©s classiques\n",
      "   - La t√¢che demande du raisonnement logique\n",
      "   \n",
      "2. Am√©liorations possibles :\n",
      "   - Fine-tuning de T5 sur des donn√©es NLI\n",
      "   - Adaptation du prompt pour la t√¢che NLI\n",
      "   - Utilisation d'un mod√®le sp√©cialis√© pour NLI\n",
      "    \n",
      "\n",
      "üìù Exemples d√©taill√©s :\n",
      "\n",
      "Exemple 1:\n",
      "G√©n√©r√©    : –Ω–∞ —Å—Ä–æ—Å–∏–ª–∞ –µ–æ —Å –Ω–µ–æ–∏–¥–∞–Ω–Ω–º –∏–Ω—Ç–µ—Ä–µ—Å–æ–º.\n",
      "R√©f√©rence : –û–Ω–∞ —Å–ø—Ä–æ—Å–∏–ª–∞, –∫–∞–∫ —ç—Ç–æ —Å–¥–µ–ª–∞—Ç—å, —Ç–∞–∫ –∫–∞–∫ —Å –µ—ë —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —ç—Ç–æ –∫–∞–∑–∞–ª–æ—Å—å –Ω–µ–≤–æ–∑–º–æ–∂–Ω—ã–º.\n",
      "ROUGE-1   : 0.0000\n",
      "\n",
      "Exemple 2:\n",
      "G√©n√©r√©    : a number of resorts in the japanese Alps are now in the area thanks to the 1998 winter games in Nagano.\n",
      "R√©f√©rence : There are a lot of resorts in the national park.\n",
      "ROUGE-1   : 0.3871\n",
      "\n",
      "Exemple 3:\n",
      "G√©n√©r√©    : trying to keep grass alive during a summer on a piece of ground that big was expensive.\n",
      "R√©f√©rence : There was no cost in keeping the grass alive in the summer time.\n",
      "ROUGE-1   : 0.3333\n"
     ]
    }
   ],
   "source": [
    "# Exercice 3\n",
    "\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "class SummaryEvaluator:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialisation de l'√©valuateur avec diff√©rentes m√©triques\n",
    "        \"\"\"\n",
    "        # Initialisation du scorer ROUGE\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(\n",
    "            ['rouge1', 'rouge2', 'rougeL'],\n",
    "            use_stemmer=True\n",
    "        )\n",
    "        \n",
    "        # Configuration NLTK\n",
    "        try:\n",
    "            nltk.download('punkt')\n",
    "        except:\n",
    "            print(\"‚ö†Ô∏è Attention: Impossible de t√©l√©charger les ressources NLTK\")\n",
    "            \n",
    "        self.smoothing = SmoothingFunction().method1\n",
    "        \n",
    "    def calculate_exact_match(self, generated: str, reference: str) -> float:\n",
    "        \"\"\"\n",
    "        Calcule la correspondance exacte (tr√®s stricte)\n",
    "        \"\"\"\n",
    "        return float(generated.strip() == reference.strip())\n",
    "    \n",
    "    def calculate_rouge_scores(self, generated: str, reference: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calcule les scores ROUGE\n",
    "        \"\"\"\n",
    "        try:\n",
    "            scores = self.rouge_scorer.score(reference, generated)\n",
    "            return {\n",
    "                'rouge1_f': scores['rouge1'].fmeasure,\n",
    "                'rouge2_f': scores['rouge2'].fmeasure,\n",
    "                'rougeL_f': scores['rougeL'].fmeasure\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erreur ROUGE: {e}\")\n",
    "            return {'rouge1_f': 0.0, 'rouge2_f': 0.0, 'rougeL_f': 0.0}\n",
    "    \n",
    "    def calculate_bleu_score(self, generated: str, reference: str) -> float:\n",
    "        \"\"\"\n",
    "        Calcule le score BLEU\n",
    "        \"\"\"\n",
    "        try:\n",
    "            reference_tokens = nltk.word_tokenize(reference)\n",
    "            generated_tokens = nltk.word_tokenize(generated)\n",
    "            return sentence_bleu([reference_tokens], generated_tokens, \n",
    "                               smoothing_function=self.smoothing)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erreur BLEU: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_summaries(self, \n",
    "                         generated_summaries: List[str], \n",
    "                         reference_summaries: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        √âvalue les r√©sum√©s avec plusieurs m√©triques\n",
    "        \"\"\"\n",
    "        metrics = {\n",
    "            'exact_match': [],\n",
    "            'rouge1': [],\n",
    "            'rouge2': [],\n",
    "            'rougeL': [],\n",
    "            'bleu': []\n",
    "        }\n",
    "        \n",
    "        for gen, ref in zip(generated_summaries, reference_summaries):\n",
    "            # Exact match\n",
    "            metrics['exact_match'].append(self.calculate_exact_match(gen, ref))\n",
    "            \n",
    "            # ROUGE scores\n",
    "            rouge_scores = self.calculate_rouge_scores(gen, ref)\n",
    "            metrics['rouge1'].append(rouge_scores['rouge1_f'])\n",
    "            metrics['rouge2'].append(rouge_scores['rouge2_f'])\n",
    "            metrics['rougeL'].append(rouge_scores['rougeL_f'])\n",
    "            \n",
    "            # BLEU score\n",
    "            metrics['bleu'].append(self.calculate_bleu_score(gen, ref))\n",
    "        \n",
    "        # Calcul des moyennes\n",
    "        return {k: np.mean(v) for k, v in metrics.items()}\n",
    "\n",
    "def display_evaluation_results(metrics: Dict[str, float], \n",
    "                             generated_summaries: List[str], \n",
    "                             reference_summaries: List[str]):\n",
    "    \"\"\"\n",
    "    Affiche les r√©sultats d'√©valuation avec explications\n",
    "    \"\"\"\n",
    "    print(\"\\nüìä R√©sultats de l'√©valuation :\")\n",
    "    print(\"\\nM√©triques globales :\")\n",
    "    print(f\"{'M√©trique':<15} {'Score':<10} {'Interpr√©tation'}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    interpretations = {\n",
    "        'exact_match': \"Correspondance exacte (tr√®s stricte)\",\n",
    "        'rouge1': \"Chevauchement de mots uniques\",\n",
    "        'rouge2': \"Chevauchement de bigrammes\",\n",
    "        'rougeL': \"Plus longue sous-s√©quence commune\",\n",
    "        'bleu': \"Pr√©cision des n-grammes\"\n",
    "    }\n",
    "    \n",
    "    for metric, score in metrics.items():\n",
    "        print(f\"{metric:<15} {score:.4f}     {interpretations[metric]}\")\n",
    "    \n",
    "    print(\"\\nüí° Analyse des r√©sultats :\")\n",
    "    print(\"\"\"\n",
    "1. Scores faibles attendus car :\n",
    "   - T5 est entra√Æn√© pour le r√©sum√©, pas pour NLI\n",
    "   - Les hypoth√®ses NLI ne sont pas des r√©sum√©s classiques\n",
    "   - La t√¢che demande du raisonnement logique\n",
    "   \n",
    "2. Am√©liorations possibles :\n",
    "   - Fine-tuning de T5 sur des donn√©es NLI\n",
    "   - Adaptation du prompt pour la t√¢che NLI\n",
    "   - Utilisation d'un mod√®le sp√©cialis√© pour NLI\n",
    "    \"\"\")\n",
    "    \n",
    "    # Exemples d√©taill√©s\n",
    "    print(\"\\nüìù Exemples d√©taill√©s :\")\n",
    "    for i in range(min(3, len(generated_summaries))):\n",
    "        print(f\"\\nExemple {i+1}:\")\n",
    "        print(f\"G√©n√©r√©    : {generated_summaries[i]}\")\n",
    "        print(f\"R√©f√©rence : {reference_summaries[i]}\")\n",
    "        \n",
    "        # Scores individuels\n",
    "        rouge_scores = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\\\n",
    "            .score(reference_summaries[i], generated_summaries[i])\n",
    "        print(f\"ROUGE-1   : {rouge_scores['rouge1'].fmeasure:.4f}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Chemin des donn√©es\n",
    "        base_path = \"/Users/ludovicveltz/Documents/Bootcamp_GENAI_2025/Crashcourse/WEEK_6/DAY_1/DATASET\"\n",
    "        \n",
    "        # Chargement avec gestion d'erreurs\n",
    "        print(\"Chargement des donn√©es...\")\n",
    "        try:\n",
    "            train_df = pd.read_csv(f\"{base_path}/train.csv\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur de chargement des donn√©es: {e}\")\n",
    "            return\n",
    "        \n",
    "        # √âchantillon tr√®s r√©duit pour test\n",
    "        sample_size = 5  # Commencez avec un petit √©chantillon\n",
    "        train_sample = train_df.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "        # Initialisation et traitement\n",
    "        print(\"Initialisation du summarizer...\")\n",
    "        summarizer = T5SummarizerLight()\n",
    "        \n",
    "        print(\"\\nTraitement des textes...\")\n",
    "        premises = train_sample['premise'].tolist()\n",
    "        summaries = summarizer.process_texts(premises)\n",
    "        \n",
    "        # √âvaluation\n",
    "        print(\"\\n√âvaluation des r√©sultats...\")\n",
    "        evaluator = SummaryEvaluator()\n",
    "        metrics = evaluator.evaluate_summaries(summaries, train_sample['hypothesis'].tolist())\n",
    "        \n",
    "        # Affichage des r√©sultats\n",
    "        display_evaluation_results(\n",
    "            metrics,\n",
    "            summaries,\n",
    "            train_sample['hypothesis'].tolist()\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'ex√©cution: {e}\")\n",
    "    finally:\n",
    "        # Nettoyage final\n",
    "        if 'summarizer' in locals():\n",
    "            summarizer.clean_memory()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from evaluate) (2.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from evaluate) (0.29.2)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (8.0.0)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.11.10)\n",
      "Requirement already satisfied: responses<0.19 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from pandas->evaluate) (2021.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des donn√©es...\n",
      "Initialisation du summarizer...\n",
      "Utilisation de : cpu\n",
      "Chargement du tokenizer...\n",
      "Chargement du mod√®le...\n",
      "\n",
      "Traitement des textes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G√©n√©ration des r√©sum√©s: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:18<00:00,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "√âvaluation ROUGE...\n",
      "Initialisation de l'√©valuateur ROUGE...\n",
      "‚úÖ √âvaluateur ROUGE initialis√©\n",
      "\n",
      "üìä √âvaluation des r√©sum√©s...\n",
      "\n",
      "üìà Scores ROUGE moyens :\n",
      "ROUGE-1 : 0.2783\n",
      "ROUGE-2 : 0.1167\n",
      "ROUGE-L : 0.2392\n",
      "\n",
      "üìù Exemples d'√©valuation :\n",
      "\n",
      "Exemple 1:\n",
      "G√©n√©r√©    : –Ω–∞ —Å—Ä–æ—Å–∏–ª–∞ –µ–æ —Å –Ω–µ–æ–∏–¥–∞–Ω–Ω–º –∏–Ω—Ç–µ—Ä–µ—Å–æ–º.\n",
      "R√©f√©rence : –û–Ω–∞ —Å–ø—Ä–æ—Å–∏–ª–∞, –∫–∞–∫ —ç—Ç–æ —Å–¥–µ–ª–∞—Ç—å, —Ç–∞–∫ –∫–∞–∫ —Å –µ—ë —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —ç—Ç–æ –∫–∞–∑–∞–ª–æ—Å—å –Ω–µ–≤–æ–∑–º–æ–∂–Ω—ã–º.\n",
      "Scores    : ROUGE-1=0.0000, ROUGE-2=0.0000, ROUGE-L=0.0000\n",
      "\n",
      "Exemple 2:\n",
      "G√©n√©r√©    : a number of resorts in the japanese Alps are now in the area thanks to the 1998 winter games in Nagano.\n",
      "R√©f√©rence : There are a lot of resorts in the national park.\n",
      "Scores    : ROUGE-1=0.3871, ROUGE-2=0.2069, ROUGE-L=0.3226\n",
      "\n",
      "Exemple 3:\n",
      "G√©n√©r√©    : trying to keep grass alive during a summer on a piece of ground that big was expensive.\n",
      "R√©f√©rence : There was no cost in keeping the grass alive in the summer time.\n",
      "Scores    : ROUGE-1=0.3333, ROUGE-2=0.0714, ROUGE-L=0.2667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercice 4\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "class T5SummarizerLight:\n",
    "    def __init__(self, model_name: str = \"t5-small\"):\n",
    "        \"\"\"\n",
    "        Version optimis√©e du summarizer T5\n",
    "        \"\"\"\n",
    "        # Configuration du device\n",
    "        self.device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.set_per_process_memory_fraction(0.7)\n",
    "            self.device = \"cuda\"\n",
    "        print(f\"Utilisation de : {self.device}\")\n",
    "        \n",
    "        # Param√®tres\n",
    "        self.max_length = 100\n",
    "        \n",
    "        # Chargement du mod√®le\n",
    "        print(\"Chargement du tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        print(\"Chargement du mod√®le...\")\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            low_cpu_mem_usage=True,\n",
    "            torch_dtype=torch.float32\n",
    "        ).to(self.device)\n",
    "\n",
    "    def clean_memory(self):\n",
    "        \"\"\"Nettoyage m√©moire\"\"\"\n",
    "        if self.device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    def process_single_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Traitement d'un seul texte\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Pr√©paration de l'entr√©e\n",
    "            input_text = f\"summarize: {text}\"\n",
    "            inputs = self.tokenizer(\n",
    "                input_text,\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # G√©n√©ration\n",
    "            with torch.no_grad():\n",
    "                output_ids = self.model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    max_length=self.max_length,\n",
    "                    num_beams=2,\n",
    "                    length_penalty=1.5,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "            \n",
    "            summary = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "            self.clean_memory()\n",
    "            return summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erreur de g√©n√©ration: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def process_texts(self, texts: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Traitement d'une liste de textes\n",
    "        \"\"\"\n",
    "        summaries = []\n",
    "        for text in tqdm(texts, desc=\"G√©n√©ration des r√©sum√©s\"):\n",
    "            summary = self.process_single_text(text)\n",
    "            summaries.append(summary)\n",
    "        return summaries\n",
    "\n",
    "class ROUGEEvaluator:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        √âvaluateur ROUGE\n",
    "        \"\"\"\n",
    "        print(\"Initialisation de l'√©valuateur ROUGE...\")\n",
    "        try:\n",
    "            self.scorer = rouge_scorer.RougeScorer(\n",
    "                ['rouge1', 'rouge2', 'rougeL'],\n",
    "                use_stemmer=True\n",
    "            )\n",
    "            try:\n",
    "                nltk.download('punkt', quiet=True)\n",
    "            except:\n",
    "                print(\"‚ö†Ô∏è Note: NLTK punkt non t√©l√©charg√©\")\n",
    "            print(\"‚úÖ √âvaluateur ROUGE initialis√©\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur d'initialisation: {e}\")\n",
    "            raise\n",
    "\n",
    "    def evaluate_summary(self, generated: str, reference: str) -> Dict:\n",
    "        \"\"\"\n",
    "        √âvaluation d'un r√©sum√©\n",
    "        \"\"\"\n",
    "        try:\n",
    "            scores = self.scorer.score(reference, generated)\n",
    "            return {\n",
    "                'rouge1': scores['rouge1'].fmeasure,\n",
    "                'rouge2': scores['rouge2'].fmeasure,\n",
    "                'rougeL': scores['rougeL'].fmeasure\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erreur d'√©valuation: {e}\")\n",
    "            return {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
    "\n",
    "    def evaluate_batch(self, generated_texts: List[str], reference_texts: List[str]) -> Dict:\n",
    "        \"\"\"\n",
    "        √âvaluation d'un lot de r√©sum√©s\n",
    "        \"\"\"\n",
    "        all_scores = []\n",
    "        print(\"\\nüìä √âvaluation des r√©sum√©s...\")\n",
    "        for gen, ref in zip(generated_texts, reference_texts):\n",
    "            scores = self.evaluate_summary(gen, ref)\n",
    "            all_scores.append(scores)\n",
    "        \n",
    "        avg_scores = {\n",
    "            metric: np.mean([s[metric] for s in all_scores])\n",
    "            for metric in ['rouge1', 'rouge2', 'rougeL']\n",
    "        }\n",
    "        \n",
    "        self.display_results(avg_scores, generated_texts, reference_texts)\n",
    "        return avg_scores\n",
    "\n",
    "    def display_results(self, scores: Dict, generated_texts: List[str], reference_texts: List[str]):\n",
    "        \"\"\"\n",
    "        Affichage des r√©sultats\n",
    "        \"\"\"\n",
    "        print(\"\\nüìà Scores ROUGE moyens :\")\n",
    "        print(f\"ROUGE-1 : {scores['rouge1']:.4f}\")\n",
    "        print(f\"ROUGE-2 : {scores['rouge2']:.4f}\")\n",
    "        print(f\"ROUGE-L : {scores['rougeL']:.4f}\")\n",
    "        \n",
    "        print(\"\\nüìù Exemples d'√©valuation :\")\n",
    "        for i in range(min(3, len(generated_texts))):\n",
    "            print(f\"\\nExemple {i+1}:\")\n",
    "            print(f\"G√©n√©r√©    : {generated_texts[i]}\")\n",
    "            print(f\"R√©f√©rence : {reference_texts[i]}\")\n",
    "            scores = self.evaluate_summary(generated_texts[i], reference_texts[i])\n",
    "            print(f\"Scores    : ROUGE-1={scores['rouge1']:.4f}, \"\n",
    "                  f\"ROUGE-2={scores['rouge2']:.4f}, \"\n",
    "                  f\"ROUGE-L={scores['rougeL']:.4f}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        base_path = \"/Users/ludovicveltz/Documents/Bootcamp_GENAI_2025/Crashcourse/WEEK_6/DAY_1/DATASET\"\n",
    "        \n",
    "        print(\"Chargement des donn√©es...\")\n",
    "        train_df = pd.read_csv(f\"{base_path}/train.csv\")\n",
    "        \n",
    "        sample_size = 5\n",
    "        train_sample = train_df.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "        print(\"Initialisation du summarizer...\")\n",
    "        summarizer = T5SummarizerLight()\n",
    "        \n",
    "        print(\"\\nTraitement des textes...\")\n",
    "        premises = train_sample['premise'].tolist()\n",
    "        summaries = summarizer.process_texts(premises)\n",
    "        \n",
    "        print(\"\\n√âvaluation ROUGE...\")\n",
    "        evaluator = ROUGEEvaluator()\n",
    "        scores = evaluator.evaluate_batch(\n",
    "            generated_texts=summaries,\n",
    "            reference_texts=train_sample['hypothesis'].tolist()\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur: {e}\")\n",
    "    finally:\n",
    "        if 'summarizer' in locals():\n",
    "            summarizer.clean_memory()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Lancement des tests de compr√©hension ROUGE...\n",
      "\n",
      "üîç Tests de compr√©hension ROUGE\n",
      "\n",
      "\n",
      "1Ô∏è‚É£ Test de correspondance exacte\n",
      "\n",
      "Texte identique :\n",
      "R√©f√©rence : Le chat noir dort sur le canap√©.\n",
      "Pr√©diction: Le chat noir dort sur le canap√©.\n",
      "\n",
      "Scores :\n",
      "ROUGE-1 : 1.0000\n",
      "ROUGE-2 : 1.0000\n",
      "ROUGE-L : 1.0000\n",
      "\n",
      "üí° Note: Les scores parfaits (1.0) indiquent une correspondance exacte\n",
      "\n",
      "2Ô∏è‚É£ Test de pr√©diction vide\n",
      "\n",
      "Pr√©diction vide :\n",
      "R√©f√©rence : Le chat noir dort.\n",
      "Pr√©diction: [vide]\n",
      "\n",
      "Scores :\n",
      "ROUGE-1 : 0.0000\n",
      "ROUGE-2 : 0.0000\n",
      "ROUGE-L : 0.0000\n",
      "\n",
      "üí° Note: Les scores nuls (0.0) indiquent l'absence de correspondance\n",
      "\n",
      "3Ô∏è‚É£ Test de l'effet du stemming\n",
      "\n",
      "Comparaison avec/sans stemming :\n",
      "R√©f√©rence : Les chats noirs dorment sur les canap√©s.\n",
      "Pr√©diction: Le chat noir dort sur le canap√©.\n",
      "\n",
      "Scores avec stemming :\n",
      "ROUGE-1 : 0.5333\n",
      "ROUGE-2 : 0.1538\n",
      "\n",
      "Scores sans stemming :\n",
      "ROUGE-1 : 0.2667\n",
      "ROUGE-2 : 0.0000\n",
      "\n",
      "üí° Note: Le stemming am√©liore les scores en normalisant les variations morphologiques\n",
      "\n",
      "4Ô∏è‚É£ Test d'analyse des n-grammes\n",
      "\n",
      "Analyse de diff√©rents niveaux de correspondance :\n",
      "\n",
      "Pr√©diction 1:\n",
      "Texte: Le chat noir dort sur le canap√©.\n",
      "ROUGE-1: 0.8750\n",
      "ROUGE-2: 0.7143\n",
      "\n",
      "Pr√©diction 2:\n",
      "Texte: Un chat noir se repose sur le canap√©.\n",
      "ROUGE-1: 0.5882\n",
      "ROUGE-2: 0.4000\n",
      "\n",
      "Pr√©diction 3:\n",
      "Texte: Un f√©lin sombre sommeille sur le sofa.\n",
      "ROUGE-1: 0.2353\n",
      "ROUGE-2: 0.1333\n",
      "\n",
      "üí° Note: ROUGE-2 est plus strict car il v√©rifie les paires de mots cons√©cutifs\n",
      "\n",
      "5Ô∏è‚É£ Test de sym√©trie\n",
      "\n",
      "Test de sym√©trie :\n",
      "Texte 1 : Le chat noir dort.\n",
      "Texte 2 : Le chien noir court.\n",
      "\n",
      "Scores (1 -> 2) :\n",
      "ROUGE-1 : 0.5000\n",
      "ROUGE-2 : 0.0000\n",
      "\n",
      "Scores (2 -> 1) :\n",
      "ROUGE-1 : 0.5000\n",
      "ROUGE-2 : 0.0000\n",
      "\n",
      "üí° Note: Les scores peuvent varier selon l'ordre r√©f√©rence/pr√©diction\n"
     ]
    }
   ],
   "source": [
    "# Exercice 5\n",
    "\n",
    "class ROUGEUnderstanding:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Classe pour comprendre le comportement des scores ROUGE\n",
    "        \"\"\"\n",
    "        self.scorer = rouge_scorer.RougeScorer(\n",
    "            ['rouge1', 'rouge2', 'rougeL'],\n",
    "            use_stemmer=True  # Activation du stemming par d√©faut\n",
    "        )\n",
    "        \n",
    "        # Scorer sans stemming pour comparaison\n",
    "        self.scorer_no_stem = rouge_scorer.RougeScorer(\n",
    "            ['rouge1', 'rouge2', 'rougeL'],\n",
    "            use_stemmer=False\n",
    "        )\n",
    "\n",
    "    def run_all_tests(self):\n",
    "        \"\"\"\n",
    "        Ex√©cute tous les tests de compr√©hension ROUGE\n",
    "        \"\"\"\n",
    "        print(\"\\nüîç Tests de compr√©hension ROUGE\\n\")\n",
    "        \n",
    "        self.test_exact_match()\n",
    "        self.test_null_prediction()\n",
    "        self.test_stemming_effect()\n",
    "        self.test_ngram_analysis()\n",
    "        self.test_symmetry()\n",
    "\n",
    "    def test_exact_match(self):\n",
    "        \"\"\"\n",
    "        Test 1: Correspondance exacte\n",
    "        \"\"\"\n",
    "        print(\"\\n1Ô∏è‚É£ Test de correspondance exacte\")\n",
    "        reference = \"Le chat noir dort sur le canap√©.\"\n",
    "        prediction = \"Le chat noir dort sur le canap√©.\"\n",
    "        \n",
    "        scores = self.scorer.score(reference, prediction)\n",
    "        \n",
    "        print(\"\\nTexte identique :\")\n",
    "        print(f\"R√©f√©rence : {reference}\")\n",
    "        print(f\"Pr√©diction: {prediction}\")\n",
    "        print(\"\\nScores :\")\n",
    "        print(f\"ROUGE-1 : {scores['rouge1'].fmeasure:.4f}\")\n",
    "        print(f\"ROUGE-2 : {scores['rouge2'].fmeasure:.4f}\")\n",
    "        print(f\"ROUGE-L : {scores['rougeL'].fmeasure:.4f}\")\n",
    "        print(\"\\nüí° Note: Les scores parfaits (1.0) indiquent une correspondance exacte\")\n",
    "\n",
    "    def test_null_prediction(self):\n",
    "        \"\"\"\n",
    "        Test 2: Pr√©diction vide\n",
    "        \"\"\"\n",
    "        print(\"\\n2Ô∏è‚É£ Test de pr√©diction vide\")\n",
    "        reference = \"Le chat noir dort.\"\n",
    "        prediction = \"\"\n",
    "        \n",
    "        scores = self.scorer.score(reference, prediction)\n",
    "        \n",
    "        print(\"\\nPr√©diction vide :\")\n",
    "        print(f\"R√©f√©rence : {reference}\")\n",
    "        print(f\"Pr√©diction: [vide]\")\n",
    "        print(\"\\nScores :\")\n",
    "        print(f\"ROUGE-1 : {scores['rouge1'].fmeasure:.4f}\")\n",
    "        print(f\"ROUGE-2 : {scores['rouge2'].fmeasure:.4f}\")\n",
    "        print(f\"ROUGE-L : {scores['rougeL'].fmeasure:.4f}\")\n",
    "        print(\"\\nüí° Note: Les scores nuls (0.0) indiquent l'absence de correspondance\")\n",
    "\n",
    "    def test_stemming_effect(self):\n",
    "        \"\"\"\n",
    "        Test 3: Effet du stemming\n",
    "        \"\"\"\n",
    "        print(\"\\n3Ô∏è‚É£ Test de l'effet du stemming\")\n",
    "        reference = \"Les chats noirs dorment sur les canap√©s.\"\n",
    "        prediction = \"Le chat noir dort sur le canap√©.\"\n",
    "        \n",
    "        # Avec stemming\n",
    "        scores_stem = self.scorer.score(reference, prediction)\n",
    "        # Sans stemming\n",
    "        scores_no_stem = self.scorer_no_stem.score(reference, prediction)\n",
    "        \n",
    "        print(\"\\nComparaison avec/sans stemming :\")\n",
    "        print(f\"R√©f√©rence : {reference}\")\n",
    "        print(f\"Pr√©diction: {prediction}\")\n",
    "        print(\"\\nScores avec stemming :\")\n",
    "        print(f\"ROUGE-1 : {scores_stem['rouge1'].fmeasure:.4f}\")\n",
    "        print(f\"ROUGE-2 : {scores_stem['rouge2'].fmeasure:.4f}\")\n",
    "        print(\"\\nScores sans stemming :\")\n",
    "        print(f\"ROUGE-1 : {scores_no_stem['rouge1'].fmeasure:.4f}\")\n",
    "        print(f\"ROUGE-2 : {scores_no_stem['rouge2'].fmeasure:.4f}\")\n",
    "        print(\"\\nüí° Note: Le stemming am√©liore les scores en normalisant les variations morphologiques\")\n",
    "\n",
    "    def test_ngram_analysis(self):\n",
    "        \"\"\"\n",
    "        Test 4: Analyse des n-grammes\n",
    "        \"\"\"\n",
    "        print(\"\\n4Ô∏è‚É£ Test d'analyse des n-grammes\")\n",
    "        reference = \"Le chat noir dort paisiblement sur le canap√© confortable.\"\n",
    "        predictions = [\n",
    "            \"Le chat noir dort sur le canap√©.\",  # Bonne correspondance\n",
    "            \"Un chat noir se repose sur le canap√©.\",  # Correspondance partielle\n",
    "            \"Un f√©lin sombre sommeille sur le sofa.\"  # Correspondance faible\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nAnalyse de diff√©rents niveaux de correspondance :\")\n",
    "        for i, pred in enumerate(predictions, 1):\n",
    "            scores = self.scorer.score(reference, pred)\n",
    "            print(f\"\\nPr√©diction {i}:\")\n",
    "            print(f\"Texte: {pred}\")\n",
    "            print(f\"ROUGE-1: {scores['rouge1'].fmeasure:.4f}\")\n",
    "            print(f\"ROUGE-2: {scores['rouge2'].fmeasure:.4f}\")\n",
    "        \n",
    "        print(\"\\nüí° Note: ROUGE-2 est plus strict car il v√©rifie les paires de mots cons√©cutifs\")\n",
    "\n",
    "    def test_symmetry(self):\n",
    "        \"\"\"\n",
    "        Test 5: Sym√©trie des scores\n",
    "        \"\"\"\n",
    "        print(\"\\n5Ô∏è‚É£ Test de sym√©trie\")\n",
    "        text1 = \"Le chat noir dort.\"\n",
    "        text2 = \"Le chien noir court.\"\n",
    "        \n",
    "        scores_1_2 = self.scorer.score(text1, text2)\n",
    "        scores_2_1 = self.scorer.score(text2, text1)\n",
    "        \n",
    "        print(\"\\nTest de sym√©trie :\")\n",
    "        print(f\"Texte 1 : {text1}\")\n",
    "        print(f\"Texte 2 : {text2}\")\n",
    "        print(\"\\nScores (1 -> 2) :\")\n",
    "        print(f\"ROUGE-1 : {scores_1_2['rouge1'].fmeasure:.4f}\")\n",
    "        print(f\"ROUGE-2 : {scores_1_2['rouge2'].fmeasure:.4f}\")\n",
    "        print(\"\\nScores (2 -> 1) :\")\n",
    "        print(f\"ROUGE-1 : {scores_2_1['rouge1'].fmeasure:.4f}\")\n",
    "        print(f\"ROUGE-2 : {scores_2_1['rouge2'].fmeasure:.4f}\")\n",
    "        print(\"\\nüí° Note: Les scores peuvent varier selon l'ordre r√©f√©rence/pr√©diction\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Tests de compr√©hension ROUGE\n",
    "        print(\"\\nüéØ Lancement des tests de compr√©hension ROUGE...\")\n",
    "        rouge_understanding = ROUGEUnderstanding()\n",
    "        rouge_understanding.run_all_tests()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors des tests: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m766.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from langdetect) (1.17.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993284 sha256=1ef566991e8afa15f7586f59c134676004ddc227800c31711b752df31cf03d96\n",
      "  Stored in directory: /Users/ludovicveltz/Library/Caches/pip/wheels/d1/c1/d9/7e068de779d863bc8f8fc9467d85e25cfe47fa5051fff1a1bb\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (0.29.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (0.1.96)\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: protobuf 4.25.6\n",
      "Uninstalling protobuf-4.25.6:\n",
      "  Successfully uninstalled protobuf-4.25.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf==3.20.0\n",
      "  Downloading protobuf-3.20.0-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Downloading protobuf-3.20.0-py2.py3-none-any.whl (162 kB)\n",
      "Installing collected packages: protobuf\n",
      "Successfully installed protobuf-3.20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (0.29.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y protobuf\n",
    "!pip install protobuf==3.20.0\n",
    "!pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des donn√©es...\n",
      "üñ•Ô∏è Utilisation de : cpu\n",
      "\n",
      "üîÑ Traitement avec mbart-large-50...\n",
      "\n",
      "üìö Chargement du mod√®le mbart-large-50...\n",
      "‚ùå Erreur lors du chargement de mbart-large-50: Couldn't build proto file into descriptor pool: duplicate file name sentencepiece_model.proto\n",
      "‚ùå Erreur: Couldn't build proto file into descriptor pool: duplicate file name sentencepiece_model.proto\n",
      "üßπ M√©moire nettoy√©e\n"
     ]
    }
   ],
   "source": [
    "# Exercice 5\n",
    "\n",
    "!pip install --upgrade protobuf==3.20.0 sentencepiece --quiet\n",
    "\n",
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, AutoTokenizer, T5ForConditionalGeneration\n",
    "from langdetect import detect\n",
    "import pandas as pd\n",
    "import torch\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "class MultilingualModelComparator:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialisation du comparateur avec mod√®les multilingues\n",
    "        \"\"\"\n",
    "        self.models = {}\n",
    "        self.tokenizers = {}\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(\n",
    "            ['rouge1', 'rouge2', 'rougeL'],\n",
    "            use_stemmer=True\n",
    "        )\n",
    "        print(f\"üñ•Ô∏è Utilisation de : {self.device}\")\n",
    "\n",
    "    def clean_memory(self):\n",
    "        \"\"\"\n",
    "        Nettoyage de la m√©moire GPU/CPU\n",
    "        \"\"\"\n",
    "        try:\n",
    "            for model in self.models.values():\n",
    "                model.cpu()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            print(\"üßπ M√©moire nettoy√©e\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erreur lors du nettoyage m√©moire: {e}\")\n",
    "\n",
    "    def load_model(self, model_name: str):\n",
    "        \"\"\"\n",
    "        Chargement des mod√®les multilingues\n",
    "        \"\"\"\n",
    "        print(f\"\\nüìö Chargement du mod√®le {model_name}...\")\n",
    "        try:\n",
    "            if model_name == \"mbart-large-50\":\n",
    "                self.tokenizers[model_name] = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\")\n",
    "                self.models[model_name] = MBartForConditionalGeneration.from_pretrained(\n",
    "                    \"facebook/mbart-large-50\",\n",
    "                    low_cpu_mem_usage=True\n",
    "                ).to(self.device)\n",
    "            elif \"mt5\" in model_name:\n",
    "                self.tokenizers[model_name] = AutoTokenizer.from_pretrained(f\"google/{model_name}\")\n",
    "                self.models[model_name] = T5ForConditionalGeneration.from_pretrained(\n",
    "                    f\"google/{model_name}\",\n",
    "                    low_cpu_mem_usage=True\n",
    "                ).to(self.device)\n",
    "            print(f\"‚úÖ Mod√®le {model_name} charg√© avec succ√®s\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur lors du chargement de {model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def detect_language_simple(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        D√©tection simple de la langue bas√©e sur les caract√®res\n",
    "        \"\"\"\n",
    "        # D√©tection basique pour le russe (cyrillique)\n",
    "        if any(ord('–∞') <= ord(c) <= ord('—è') for c in text.lower()):\n",
    "            return 'ru'\n",
    "        # D√©tection basique pour le chinois\n",
    "        if any('\\u4e00' <= c <= '\\u9fff' for c in text):\n",
    "            return 'zh'\n",
    "        # Par d√©faut, on suppose de l'anglais\n",
    "        return 'en'\n",
    "\n",
    "\n",
    "    def preprocess_text(self, text: str, model_name: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Pr√©traitement avec d√©tection de langue\n",
    "        \"\"\"\n",
    "        lang = self.detect_language(text)\n",
    "        if model_name == \"mbart-large-50\":\n",
    "            src_lang = f\"xx_{lang}\" if lang != \"unknown\" else \"xx_en\"\n",
    "            return {\n",
    "                \"text\": text,\n",
    "                \"src_lang\": src_lang,\n",
    "                \"tgt_lang\": src_lang  # M√™me langue pour le r√©sum√©\n",
    "            }\n",
    "        return {\"text\": text, \"lang\": lang}\n",
    "\n",
    "    def summarize_with_mbart(self, text: str, model_name: str, lang_info: Dict) -> str:\n",
    "        \"\"\"\n",
    "        G√©n√©ration de r√©sum√© avec mBART\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.tokenizers[model_name].src_lang = lang_info[\"src_lang\"]\n",
    "            inputs = self.tokenizers[model_name](\n",
    "                text,\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.models[model_name].generate(\n",
    "                    inputs.input_ids,\n",
    "                    forced_bos_token_id=self.tokenizers[model_name].lang_code_to_id[lang_info[\"tgt_lang\"]],\n",
    "                    max_length=150,\n",
    "                    num_beams=2,\n",
    "                    length_penalty=1.5,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "\n",
    "            summary = self.tokenizers[model_name].decode(outputs[0], skip_special_tokens=True)\n",
    "            return summary\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erreur mBART: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def summarize_with_mt5(self, text: str, model_name: str, lang_info: Dict) -> str:\n",
    "        \"\"\"\n",
    "        G√©n√©ration de r√©sum√© avec mT5\n",
    "        \"\"\"\n",
    "        try:\n",
    "            input_text = f\"summarize: {text}\"\n",
    "            inputs = self.tokenizers[model_name](\n",
    "                input_text,\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.models[model_name].generate(\n",
    "                    inputs.input_ids,\n",
    "                    max_length=150,\n",
    "                    num_beams=2,\n",
    "                    length_penalty=1.5,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "\n",
    "            summary = self.tokenizers[model_name].decode(outputs[0], skip_special_tokens=True)\n",
    "            return summary\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erreur mT5: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def compare_models(self, texts: List[str], references: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Comparaison des mod√®les multilingues\n",
    "        \"\"\"\n",
    "        models_to_compare = ['mbart-large-50', 'mt5-small', 'mt5-base']\n",
    "        results = []\n",
    "\n",
    "        for model_name in models_to_compare:\n",
    "            print(f\"\\nüîÑ Traitement avec {model_name}...\")\n",
    "            self.load_model(model_name)\n",
    "            \n",
    "            for i, (text, ref) in enumerate(zip(texts, references)):\n",
    "                # Pr√©traitement avec d√©tection de langue\n",
    "                processed = self.preprocess_text(text, model_name)\n",
    "                \n",
    "                # G√©n√©ration du r√©sum√©\n",
    "                if \"mbart\" in model_name:\n",
    "                    summary = self.summarize_with_mbart(text, model_name, processed)\n",
    "                else:\n",
    "                    summary = self.summarize_with_mt5(text, model_name, processed)\n",
    "                \n",
    "                # Calcul des scores ROUGE\n",
    "                rouge_scores = self.compute_rouge_per_row(summary, ref)\n",
    "                \n",
    "                results.append({\n",
    "                    'model': model_name,\n",
    "                    'text_id': i,\n",
    "                    'language': processed.get('lang', processed.get('src_lang')),\n",
    "                    'original': text,\n",
    "                    'reference': ref,\n",
    "                    'generated': summary,\n",
    "                    'rouge1': rouge_scores['rouge1'],\n",
    "                    'rouge2': rouge_scores['rouge2'],\n",
    "                    'rougeL': rouge_scores['rougeL']\n",
    "                })\n",
    "            \n",
    "            self.clean_memory()\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "def display_results(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Affichage am√©lior√© des r√©sultats\n",
    "    \"\"\"\n",
    "    print(\"\\nüìä R√©sultats de la comparaison des mod√®les\\n\")\n",
    "    \n",
    "    # Scores moyens par mod√®le et par langue\n",
    "    print(\"Scores ROUGE moyens par mod√®le et langue :\")\n",
    "    mean_scores = df.groupby(['model', 'language'])[['rouge1', 'rouge2', 'rougeL']].mean()\n",
    "    print(mean_scores)\n",
    "    \n",
    "    # Exemples d√©taill√©s\n",
    "    print(\"\\nüìù Exemples de g√©n√©ration par mod√®le :\")\n",
    "    for model in df['model'].unique():\n",
    "        for lang in df['language'].unique():\n",
    "            sample = df[(df['model'] == model) & (df['language'] == lang)].iloc[0]\n",
    "            print(f\"\\nü§ñ Mod√®le : {model} | Langue : {lang}\")\n",
    "            print(f\"Original  : {sample['original'][:100]}...\")\n",
    "            print(f\"G√©n√©r√©    : {sample['generated']}\")\n",
    "            print(f\"R√©f√©rence : {sample['reference']}\")\n",
    "            print(f\"ROUGE-1   : {sample['rouge1']:.4f}\")\n",
    "            print(f\"ROUGE-2   : {sample['rouge2']:.4f}\")\n",
    "            print(f\"ROUGE-L   : {sample['rougeL']:.4f}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Chargement des donn√©es\n",
    "        base_path = \"/Users/ludovicveltz/Documents/Bootcamp_GENAI_2025/Crashcourse/WEEK_6/DAY_1/DATASET\"\n",
    "        print(\"Chargement des donn√©es...\")\n",
    "        train_df = pd.read_csv(f\"{base_path}/train.csv\")\n",
    "        \n",
    "        # √âchantillon tr√®s r√©duit pour test\n",
    "        sample_size = 2  # R√©duit √† 2 pour test\n",
    "        train_sample = train_df.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "        # Initialisation du comparateur\n",
    "        comparator = MultilingualModelComparator()\n",
    "        \n",
    "        # Comparaison des mod√®les\n",
    "        results_df = comparator.compare_models(\n",
    "            texts=train_sample['premise'].tolist(),\n",
    "            references=train_sample['hypothesis'].tolist()\n",
    "        )\n",
    "        \n",
    "        # Affichage des r√©sultats\n",
    "        display_results(results_df)\n",
    "        \n",
    "        # Sauvegarde des r√©sultats\n",
    "        results_df.to_csv(f\"{base_path}/multilingual_model_comparison_results.csv\", index=False)\n",
    "        print(\"\\n‚úÖ R√©sultats sauvegard√©s dans multilingual_model_comparison_results.csv\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur: {e}\")\n",
    "    finally:\n",
    "        if 'comparator' in locals():\n",
    "            comparator.clean_memory()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
