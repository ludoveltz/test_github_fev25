{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Vérification des fichiers...\n",
      "\n",
      "📊 Structure du Dataset:\n",
      "\n",
      "Colonnes du train.csv:\n",
      "['id', 'premise', 'hypothesis', 'lang_abv', 'language', 'label']\n",
      "\n",
      "Aperçu des premières lignes train.csv:\n",
      "           id                                            premise  \\\n",
      "0  5130fd2cb5  and these comments were considered in formulat...   \n",
      "1  5b72532a0b  These are issues that we wrestle with in pract...   \n",
      "2  3931fbe82a  Des petites choses comme celles-là font une di...   \n",
      "3  5622f0c60b  you know they can't really defend themselves l...   \n",
      "4  86aaa48b45  ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...   \n",
      "\n",
      "                                          hypothesis lang_abv language  label  \n",
      "0  The rules developed in the interim were put to...       en  English      0  \n",
      "1  Practice groups are not permitted to work on t...       en  English      2  \n",
      "2              J'essayais d'accomplir quelque chose.       fr   French      0  \n",
      "3  They can't defend themselves because of their ...       en  English      0  \n",
      "4    เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร       th     Thai      1  \n",
      "\n",
      "Informations sur le dataset train:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12120 entries, 0 to 12119\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          12120 non-null  object\n",
      " 1   premise     12120 non-null  object\n",
      " 2   hypothesis  12120 non-null  object\n",
      " 3   lang_abv    12120 non-null  object\n",
      " 4   language    12120 non-null  object\n",
      " 5   label       12120 non-null  int64 \n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 568.2+ KB\n",
      "None\n",
      "\n",
      "Description statistique train:\n",
      "              label\n",
      "count  12120.000000\n",
      "mean       0.990759\n",
      "std        0.824523\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        1.000000\n",
      "75%        2.000000\n",
      "max        2.000000\n"
     ]
    }
   ],
   "source": [
    "# Exercice 1\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def verify_dataset_structure(base_path: str):\n",
    "    \"\"\"\n",
    "    Vérifie la structure réelle du dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construction des chemins\n",
    "        train_path = os.path.join(base_path, \"train.csv\")\n",
    "        test_path = os.path.join(base_path, \"test.csv\")\n",
    "        \n",
    "        # Vérification de l'existence des fichiers\n",
    "        print(\"🔍 Vérification des fichiers...\")\n",
    "        if not os.path.exists(train_path):\n",
    "            print(f\"❌ Le fichier train.csv n'existe pas dans {train_path}\")\n",
    "            return\n",
    "        if not os.path.exists(test_path):\n",
    "            print(f\"❌ Le fichier test.csv n'existe pas dans {test_path}\")\n",
    "            return\n",
    "            \n",
    "        # Chargement des données pour inspection\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "        \n",
    "        print(\"\\n📊 Structure du Dataset:\")\n",
    "        print(\"\\nColonnes du train.csv:\")\n",
    "        print(train_df.columns.tolist())\n",
    "        \n",
    "        print(\"\\nAperçu des premières lignes train.csv:\")\n",
    "        print(train_df.head())\n",
    "        \n",
    "        print(\"\\nInformations sur le dataset train:\")\n",
    "        print(train_df.info())\n",
    "        \n",
    "        print(\"\\nDescription statistique train:\")\n",
    "        print(train_df.describe())\n",
    "        \n",
    "        return train_df, test_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors de la vérification: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def main():\n",
    "    base_path = \"/Users/ludovicveltz/Documents/Bootcamp_GENAI_2025/Crashcourse/WEEK_6/DAY_1/DATASET\"\n",
    "    train_df, test_df = verify_dataset_structure(base_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Analyse du Dataset NLI Multilingue:\n",
      "\n",
      "Distribution par langue:\n",
      "Arabic: 18 exemples\n",
      "Bulgarian: 18 exemples\n",
      "Chinese: 18 exemples\n",
      "English: 18 exemples\n",
      "French: 18 exemples\n",
      "German: 18 exemples\n",
      "Greek: 18 exemples\n",
      "Hindi: 18 exemples\n",
      "Russian: 18 exemples\n",
      "Spanish: 18 exemples\n",
      "Swahili: 18 exemples\n",
      "Thai: 18 exemples\n",
      "Turkish: 18 exemples\n",
      "Urdu: 18 exemples\n",
      "Vietnamese: 18 exemples\n",
      "\n",
      "Distribution des labels:\n",
      "Contradiction: 90 exemples\n",
      "Neutre: 90 exemples\n",
      "Implication: 90 exemples\n",
      "\n",
      "Longueur moyenne des textes:\n",
      "Premise: 101 caractères\n",
      "Hypothesis: 51 caractères\n",
      "\n",
      "Exemples par langue:\n",
      "\n",
      "Arabic:\n",
      "Premise: كان للتلاميذ بمثابة ناصح، ومعلم، وقسيس، وعم، وصديق حقيقي....\n",
      "Hypothesis: لقد كان مفيدًا جدًا لطلابه....\n",
      "Label: Contradiction\n",
      "\n",
      "Bulgarian:\n",
      "Premise: Като член на Вътрешния кръг ще получите избрано място по време на Конвенцията и специални покани за ...\n",
      "Hypothesis: Членовете на Вътрешния кръг получават различни бонуси....\n",
      "Label: Contradiction\n",
      "\n",
      "Chinese:\n",
      "Premise: 我今天早上到那里, 呃，我忘了是我问了一个问题还是他进来了, 随便吧。...\n",
      "Hypothesis: 今天早上我来了，他也来了。...\n",
      "Label: Contradiction\n",
      "\n",
      "English:\n",
      "Premise: The Star reports that actress Jodie Foster is pregnant through artificial insemination....\n",
      "Hypothesis: It has been reported by The Star that actress Jodie Foster is pregnant through artificial inseminati...\n",
      "Label: Contradiction\n",
      "\n",
      "French:\n",
      "Premise: Avant que tu ne me donnes une fessée, pourquoi ne me laisserais-tu pas juste avoir un grand verre de...\n",
      "Hypothesis: Je voudrais un verre de lait au chocolat....\n",
      "Label: Contradiction\n",
      "\n",
      "German:\n",
      "Premise: Der Zugang zu unserem Gelände wird für jeden mit einem Computer und einem Modem geöffnet....\n",
      "Hypothesis: Die Leute brauchen sowohl einen Computer als auch ein Modem, um das Gelände zu betreten....\n",
      "Label: Contradiction\n",
      "\n",
      "Greek:\n",
      "Premise: Εν τω μεταξύ, το Caldas de Monchique είναι ένα καλό μέρος για ένα πικνίκ και μια βόλτα στο δάσος....\n",
      "Hypothesis: Υπάρχουν δάση γύρω από το Caldas de Monchique....\n",
      "Label: Contradiction\n",
      "\n",
      "Hindi:\n",
      "Premise: मैं देख रहा हूँ, महोदय, कि आप परिस्थितियों को अभी तक नहीं समझे है ।...\n",
      "Hypothesis: मई तुझपे विश्वास नहीं करता साडी हालत समझ लो...\n",
      "Label: Contradiction\n",
      "\n",
      "Russian:\n",
      "Premise: Первая группа NYUD ESU вошла в вестибюль Северной башни на Западной улице и подготовилась к подъему ...\n",
      "Hypothesis: В 09:15 башня еще стояла....\n",
      "Label: Contradiction\n",
      "\n",
      "Spanish:\n",
      "Premise: Las metáforas animales originales son prácticamente destruídas con palabras que no hacen referencia ...\n",
      "Hypothesis: Las metáforas de animales prácticamente han desaparecido....\n",
      "Label: Contradiction\n",
      "\n",
      "Swahili:\n",
      "Premise: Mnamo mwaka wa 1868 na kupitishwa kwa Marekebisho ya Kumi na nne, tulikuwa tumezingatia kizingiti ch...\n",
      "Hypothesis: Marekebisho ya kumi na nne yalianzishwa mwaka wa 1868....\n",
      "Label: Contradiction\n",
      "\n",
      "Thai:\n",
      "Premise: ท้ายที่สุดฉันต้องการที่จะกล่าวถึงปัญหาของการระดมทุน...\n",
      "Hypothesis: ฉันต้องการพูดคุยเกี่ยวกับการระดมทุน...\n",
      "Label: Contradiction\n",
      "\n",
      "Turkish:\n",
      "Premise: Şövalye, atına, Latin caballus atıyla bağlıdır....\n",
      "Hypothesis: Şövalyenin adı, atıyla olan bağına işaret ediyor....\n",
      "Label: Contradiction\n",
      "\n",
      "Urdu:\n",
      "Premise: مہداہر نے اپنے مطلوبہ خطاب کو نیویارک شہر، میرٹوت ہوٹل کے طور پر دیا، لیکن اس کے بجائے کسی اورنیو یا...\n",
      "Hypothesis: مدھار نے اس ہوٹل میں نہیں قیام نہیں کیا  جس کے بارے میں اس نے کہا تھا۔...\n",
      "Label: Contradiction\n",
      "\n",
      "Vietnamese:\n",
      "Premise: Tài sản tích lũy có thể tạo ra thu nhập dưới hình thức lãi suất và cổ tức và những khoản này có thể ...\n",
      "Hypothesis: Bạn có thể tạo thu nhập với tài sản....\n",
      "Label: Contradiction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3h/ppgwfhhj1rz7_5v8s2zf1pwr0000gn/T/ipykernel_78859/3709443932.py:23: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train_sample = (train_df.groupby(['language', 'label'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "class NLIDatasetExplorer:\n",
    "    def __init__(self, base_path: str):\n",
    "        \"\"\"\n",
    "        Initialisation de l'explorateur pour le dataset NLI multilingue\n",
    "        \"\"\"\n",
    "        self.base_path = base_path\n",
    "        self.label_map = {\n",
    "            0: \"Contradiction\",\n",
    "            1: \"Neutre\",\n",
    "            2: \"Implication\"\n",
    "        }\n",
    "        \n",
    "    def load_datasets(self, train_sample_size: int = 100, test_sample_size: int = 50):\n",
    "        try:\n",
    "            train_df = pd.read_csv(os.path.join(self.base_path, \"train.csv\"))\n",
    "            test_df = pd.read_csv(os.path.join(self.base_path, \"test.csv\"))\n",
    "            \n",
    "            # Correction du groupby deprecated\n",
    "            train_sample = (train_df.groupby(['language', 'label'])\n",
    "                        .apply(lambda x: x.sample(min(len(x), train_sample_size//len(train_df['language'].unique()))))\n",
    "                        .reset_index(drop=True))\n",
    "            \n",
    "            return train_sample, test_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur: {e}\")\n",
    "            return None, None\n",
    "\n",
    "\n",
    "    def explore_dataset(self, train_df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyse exploratoire du dataset NLI\n",
    "        \"\"\"\n",
    "        stats = {\n",
    "            'distribution_langues': train_df['language'].value_counts().to_dict(),\n",
    "            'distribution_labels': train_df['label'].value_counts().to_dict(),\n",
    "            'longueur_moyenne': {\n",
    "                'premise': train_df['premise'].str.len().mean(),\n",
    "                'hypothesis': train_df['hypothesis'].str.len().mean()\n",
    "            },\n",
    "            'exemples_par_langue': {}\n",
    "        }\n",
    "        \n",
    "        # Extraire un exemple par langue\n",
    "        for lang in train_df['language'].unique():\n",
    "            exemple = train_df[train_df['language'] == lang].iloc[0]\n",
    "            stats['exemples_par_langue'][lang] = {\n",
    "                'premise': exemple['premise'],\n",
    "                'hypothesis': exemple['hypothesis'],\n",
    "                'label': self.label_map[exemple['label']]\n",
    "            }\n",
    "            \n",
    "        return stats\n",
    "\n",
    "def main():\n",
    "    base_path = \"/Users/ludovicveltz/Documents/Bootcamp_GENAI_2025/Crashcourse/WEEK_6/DAY_1/DATASET\"\n",
    "    explorer = NLIDatasetExplorer(base_path)\n",
    "    \n",
    "    train_df, test_df = explorer.load_datasets()\n",
    "    \n",
    "    if train_df is not None:\n",
    "        stats = explorer.explore_dataset(train_df)\n",
    "        \n",
    "        print(\"\\n📊 Analyse du Dataset NLI Multilingue:\")\n",
    "        \n",
    "        print(\"\\nDistribution par langue:\")\n",
    "        for lang, count in stats['distribution_langues'].items():\n",
    "            print(f\"{lang}: {count} exemples\")\n",
    "        \n",
    "        print(\"\\nDistribution des labels:\")\n",
    "        for label_id, count in stats['distribution_labels'].items():\n",
    "            print(f\"{explorer.label_map[label_id]}: {count} exemples\")\n",
    "        \n",
    "        print(\"\\nLongueur moyenne des textes:\")\n",
    "        print(f\"Premise: {stats['longueur_moyenne']['premise']:.0f} caractères\")\n",
    "        print(f\"Hypothesis: {stats['longueur_moyenne']['hypothesis']:.0f} caractères\")\n",
    "        \n",
    "        print(\"\\nExemples par langue:\")\n",
    "        for lang, exemple in stats['exemples_par_langue'].items():\n",
    "            print(f\"\\n{lang}:\")\n",
    "            print(f\"Premise: {exemple['premise'][:100]}...\")\n",
    "            print(f\"Hypothesis: {exemple['hypothesis'][:100]}...\")\n",
    "            print(f\"Label: {exemple['label']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des données...\n",
      "Initialisation du summarizer...\n",
      "Utilisation de : cpu\n",
      "Chargement du tokenizer...\n",
      "Chargement du modèle...\n",
      "\n",
      "Traitement des textes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Génération des résumés: 100%|██████████| 5/5 [00:18<00:00,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Résultats :\n",
      "\n",
      "📊 Résultats de la génération :\n",
      "\n",
      "Exemple 1\n",
      "--------------------------------------------------------------------------------\n",
      "Texte Original :\n",
      "Кто? Она спросила его с неожиданным интересом.\n",
      "\n",
      "Résumé Généré :\n",
      "на сросила ео с неоиданнм интересом.\n",
      "\n",
      "Hypothèse :\n",
      "Она спросила, как это сделать, так как с её точки зрения это казалось невозможным.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Exemple 2\n",
      "--------------------------------------------------------------------------------\n",
      "Texte Original :\n",
      "Others are Zao (in Tohoku) and a number of resorts in Joshin-etsu Kogen National Park in the Japan Alps, where there are now splendid facilities thanks to the 1998 Winter Olympic Games in Nagano.\n",
      "\n",
      "Résumé Généré :\n",
      "a number of resorts in the japanese Alps are now in the area thanks to the 1998 winter games in Nagano.\n",
      "\n",
      "Hypothèse :\n",
      "There are a lot of resorts in the national park.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Exemple 3\n",
      "--------------------------------------------------------------------------------\n",
      "Texte Original :\n",
      "trying to keep grass alive during a summer on a piece of ground that big was expensive\n",
      "\n",
      "Résumé Généré :\n",
      "trying to keep grass alive during a summer on a piece of ground that big was expensive.\n",
      "\n",
      "Hypothèse :\n",
      "There was no cost in keeping the grass alive in the summer time.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Exemple 4\n",
      "--------------------------------------------------------------------------------\n",
      "Texte Original :\n",
      "so i guess my experience is is just with what we did and and so they didn't really go through the child care route they were able to be home together\n",
      "\n",
      "Résumé Généré :\n",
      "so i guess my experience is just with what we did and so they didn't really go through the child care route they were able to be home together. so they didn't really go through the route they were able to be home together together.\n",
      "\n",
      "Hypothèse :\n",
      "They were able to be home rather than having to worry about getting child care.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Exemple 5\n",
      "--------------------------------------------------------------------------------\n",
      "Texte Original :\n",
      "The Journal put the point succinctly to  Is any publicity good publicity?\n",
      "\n",
      "Résumé Généré :\n",
      "the Journal put the point succinctly to Is any publicity good publicity?\n",
      "\n",
      "Hypothèse :\n",
      "The Journal asked \"Is this a good political move?\"\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "📈 Statistiques :\n",
      "Longueur moyenne des textes originaux : 110 caractères\n",
      "Longueur moyenne des résumés générés : 106 caractères\n",
      "Longueur moyenne des hypothèses : 65 caractères\n",
      "\n",
      "✅ Résultats sauvegardés dans t5_summaries_results_light.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercice 2\n",
    "\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "import pandas as pd\n",
    "import gc\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "class T5SummarizerLight:\n",
    "    def __init__(self, model_name: str = \"t5-small\"):\n",
    "        \"\"\"\n",
    "        Version optimisée du summarizer T5\n",
    "        \"\"\"\n",
    "        # Vérification et configuration CUDA\n",
    "        self.device = \"cpu\"  \n",
    "        if torch.cuda.is_available():\n",
    "            # Limitation de la mémoire CUDA\n",
    "            torch.cuda.set_per_process_memory_fraction(0.7)  # Utilise 70% max de la VRAM\n",
    "            self.device = \"cuda\"\n",
    "        print(f\"Utilisation de : {self.device}\")\n",
    "        \n",
    "        # Paramètres optimisés\n",
    "        self.batch_size = 4  # Batch size réduit\n",
    "        self.max_length = 100  # Longueur maximale réduite\n",
    "        \n",
    "        # Chargement progressif\n",
    "        print(\"Chargement du tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        print(\"Chargement du modèle...\")\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            low_cpu_mem_usage=True,  # Optimisation mémoire CPU\n",
    "            torch_dtype=torch.float32  # Précision standard\n",
    "        ).to(self.device)\n",
    "\n",
    "    def clean_memory(self):\n",
    "        \"\"\"Nettoyage mémoire amélioré\"\"\"\n",
    "        if self.device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    def summarize_single_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Traitement d'un seul texte à la fois\n",
    "        \"\"\"\n",
    "        try:\n",
    "            input_text = f\"summarize: {text}\"\n",
    "            \n",
    "            # Tokenization avec gestion de la longueur\n",
    "            inputs = self.tokenizer(\n",
    "                input_text,\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Génération avec paramètres optimisés\n",
    "            with torch.no_grad():\n",
    "                output_ids = self.model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    max_length=self.max_length,\n",
    "                    num_beams=2,  # Réduit pour économiser la mémoire\n",
    "                    length_penalty=1.5,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "            \n",
    "            summary = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "            self.clean_memory()\n",
    "            return summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du résumé: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def process_texts(self, texts: List[str], sample_size: int = None) -> List[str]:\n",
    "        \"\"\"\n",
    "        Traitement des textes avec gestion d'échantillon\n",
    "        \"\"\"\n",
    "        if sample_size:\n",
    "            texts = texts[:sample_size]\n",
    "        \n",
    "        summaries = []\n",
    "        for text in tqdm(texts, desc=\"Génération des résumés\"):\n",
    "            summary = self.summarize_single_text(text)\n",
    "            summaries.append(summary)\n",
    "            \n",
    "        return summaries\n",
    "\n",
    "def display_results(results_df):\n",
    "    \"\"\"\n",
    "    Affiche les résultats avec un meilleur formatage\n",
    "    \"\"\"\n",
    "    print(\"\\n📊 Résultats de la génération :\\n\")\n",
    "    \n",
    "    for idx, row in results_df.iterrows():\n",
    "        print(f\"Exemple {idx + 1}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"Texte Original :\")\n",
    "        print(f\"{row['Texte_Original']}\\n\")\n",
    "        print(f\"Résumé Généré :\")\n",
    "        print(f\"{row['Résumé_Généré']}\\n\")\n",
    "        print(f\"Hypothèse :\")\n",
    "        print(f\"{row['Hypothèse']}\\n\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Chemin des données\n",
    "        base_path = \"/Users/ludovicveltz/Documents/Bootcamp_GENAI_2025/Crashcourse/WEEK_6/DAY_1/DATASET\"\n",
    "        \n",
    "        # Chargement avec gestion d'erreurs\n",
    "        print(\"Chargement des données...\")\n",
    "        try:\n",
    "            train_df = pd.read_csv(f\"{base_path}/train.csv\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur de chargement des données: {e}\")\n",
    "            return\n",
    "        \n",
    "        # Échantillon très réduit pour test\n",
    "        sample_size = 5  # Commencez avec un petit échantillon\n",
    "        train_sample = train_df.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "        # Initialisation et traitement\n",
    "        print(\"Initialisation du summarizer...\")\n",
    "        summarizer = T5SummarizerLight()\n",
    "        \n",
    "        print(\"\\nTraitement des textes...\")\n",
    "        premises = train_sample['premise'].tolist()\n",
    "        summaries = summarizer.process_texts(premises)\n",
    "        \n",
    "        # Création et sauvegarde des résultats\n",
    "        results_df = pd.DataFrame({\n",
    "            'Texte_Original': premises,\n",
    "            'Résumé_Généré': summaries,\n",
    "            'Hypothèse': train_sample['hypothesis'].tolist()\n",
    "        })\n",
    "        \n",
    "        # Modification de l'affichage\n",
    "        pd.set_option('display.max_colwidth', None)\n",
    "        pd.set_option('display.expand_frame_repr', False)  # Évite le retour à la ligne automatique\n",
    "        pd.set_option('display.max_rows', None)  # Affiche toutes les lignes\n",
    "\n",
    "        # Pour l'affichage des résultats\n",
    "        print(\"\\nRésultats :\")\n",
    "        pd.set_option('display.max_colwidth', None)\n",
    "        display_results(results_df)\n",
    "        \n",
    "        # Statistiques additionnelles\n",
    "        print(\"\\n📈 Statistiques :\")\n",
    "        print(f\"Longueur moyenne des textes originaux : {results_df['Texte_Original'].str.len().mean():.0f} caractères\")\n",
    "        print(f\"Longueur moyenne des résumés générés : {results_df['Résumé_Généré'].str.len().mean():.0f} caractères\")\n",
    "        print(f\"Longueur moyenne des hypothèses : {results_df['Hypothèse'].str.len().mean():.0f} caractères\")\n",
    "        \n",
    "        # Sauvegarde\n",
    "        try:\n",
    "            results_df.to_csv(f\"{base_path}/t5_summaries_results_light.csv\", index=False)\n",
    "            print(\"\\n✅ Résultats sauvegardés dans t5_summaries_results_light.csv\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Erreur lors de la sauvegarde : {e}\")\n",
    "        \n",
    "    finally:\n",
    "        # Nettoyage final\n",
    "        if 'summarizer' in locals():\n",
    "            summarizer.clean_memory()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des données...\n",
      "Initialisation du summarizer...\n",
      "Utilisation de : cpu\n",
      "Chargement du tokenizer...\n",
      "Chargement du modèle...\n",
      "\n",
      "Traitement des textes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Génération des résumés: 100%|██████████| 5/5 [00:18<00:00,  3.64s/it]\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ludovicveltz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Évaluation des résultats...\n",
      "\n",
      "📊 Résultats de l'évaluation :\n",
      "\n",
      "Métriques globales :\n",
      "Métrique        Score      Interprétation\n",
      "------------------------------------------------------------\n",
      "exact_match     0.0000     Correspondance exacte (très stricte)\n",
      "rouge1          0.2783     Chevauchement de mots uniques\n",
      "rouge2          0.1167     Chevauchement de bigrammes\n",
      "rougeL          0.2392     Plus longue sous-séquence commune\n",
      "bleu            0.0540     Précision des n-grammes\n",
      "\n",
      "💡 Analyse des résultats :\n",
      "\n",
      "1. Scores faibles attendus car :\n",
      "   - T5 est entraîné pour le résumé, pas pour NLI\n",
      "   - Les hypothèses NLI ne sont pas des résumés classiques\n",
      "   - La tâche demande du raisonnement logique\n",
      "   \n",
      "2. Améliorations possibles :\n",
      "   - Fine-tuning de T5 sur des données NLI\n",
      "   - Adaptation du prompt pour la tâche NLI\n",
      "   - Utilisation d'un modèle spécialisé pour NLI\n",
      "    \n",
      "\n",
      "📝 Exemples détaillés :\n",
      "\n",
      "Exemple 1:\n",
      "Généré    : на сросила ео с неоиданнм интересом.\n",
      "Référence : Она спросила, как это сделать, так как с её точки зрения это казалось невозможным.\n",
      "ROUGE-1   : 0.0000\n",
      "\n",
      "Exemple 2:\n",
      "Généré    : a number of resorts in the japanese Alps are now in the area thanks to the 1998 winter games in Nagano.\n",
      "Référence : There are a lot of resorts in the national park.\n",
      "ROUGE-1   : 0.3871\n",
      "\n",
      "Exemple 3:\n",
      "Généré    : trying to keep grass alive during a summer on a piece of ground that big was expensive.\n",
      "Référence : There was no cost in keeping the grass alive in the summer time.\n",
      "ROUGE-1   : 0.3333\n"
     ]
    }
   ],
   "source": [
    "# Exercice 3\n",
    "\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "class SummaryEvaluator:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialisation de l'évaluateur avec différentes métriques\n",
    "        \"\"\"\n",
    "        # Initialisation du scorer ROUGE\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(\n",
    "            ['rouge1', 'rouge2', 'rougeL'],\n",
    "            use_stemmer=True\n",
    "        )\n",
    "        \n",
    "        # Configuration NLTK\n",
    "        try:\n",
    "            nltk.download('punkt')\n",
    "        except:\n",
    "            print(\"⚠️ Attention: Impossible de télécharger les ressources NLTK\")\n",
    "            \n",
    "        self.smoothing = SmoothingFunction().method1\n",
    "        \n",
    "    def calculate_exact_match(self, generated: str, reference: str) -> float:\n",
    "        \"\"\"\n",
    "        Calcule la correspondance exacte (très stricte)\n",
    "        \"\"\"\n",
    "        return float(generated.strip() == reference.strip())\n",
    "    \n",
    "    def calculate_rouge_scores(self, generated: str, reference: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calcule les scores ROUGE\n",
    "        \"\"\"\n",
    "        try:\n",
    "            scores = self.rouge_scorer.score(reference, generated)\n",
    "            return {\n",
    "                'rouge1_f': scores['rouge1'].fmeasure,\n",
    "                'rouge2_f': scores['rouge2'].fmeasure,\n",
    "                'rougeL_f': scores['rougeL'].fmeasure\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Erreur ROUGE: {e}\")\n",
    "            return {'rouge1_f': 0.0, 'rouge2_f': 0.0, 'rougeL_f': 0.0}\n",
    "    \n",
    "    def calculate_bleu_score(self, generated: str, reference: str) -> float:\n",
    "        \"\"\"\n",
    "        Calcule le score BLEU\n",
    "        \"\"\"\n",
    "        try:\n",
    "            reference_tokens = nltk.word_tokenize(reference)\n",
    "            generated_tokens = nltk.word_tokenize(generated)\n",
    "            return sentence_bleu([reference_tokens], generated_tokens, \n",
    "                               smoothing_function=self.smoothing)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Erreur BLEU: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_summaries(self, \n",
    "                         generated_summaries: List[str], \n",
    "                         reference_summaries: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Évalue les résumés avec plusieurs métriques\n",
    "        \"\"\"\n",
    "        metrics = {\n",
    "            'exact_match': [],\n",
    "            'rouge1': [],\n",
    "            'rouge2': [],\n",
    "            'rougeL': [],\n",
    "            'bleu': []\n",
    "        }\n",
    "        \n",
    "        for gen, ref in zip(generated_summaries, reference_summaries):\n",
    "            # Exact match\n",
    "            metrics['exact_match'].append(self.calculate_exact_match(gen, ref))\n",
    "            \n",
    "            # ROUGE scores\n",
    "            rouge_scores = self.calculate_rouge_scores(gen, ref)\n",
    "            metrics['rouge1'].append(rouge_scores['rouge1_f'])\n",
    "            metrics['rouge2'].append(rouge_scores['rouge2_f'])\n",
    "            metrics['rougeL'].append(rouge_scores['rougeL_f'])\n",
    "            \n",
    "            # BLEU score\n",
    "            metrics['bleu'].append(self.calculate_bleu_score(gen, ref))\n",
    "        \n",
    "        # Calcul des moyennes\n",
    "        return {k: np.mean(v) for k, v in metrics.items()}\n",
    "\n",
    "def display_evaluation_results(metrics: Dict[str, float], \n",
    "                             generated_summaries: List[str], \n",
    "                             reference_summaries: List[str]):\n",
    "    \"\"\"\n",
    "    Affiche les résultats d'évaluation avec explications\n",
    "    \"\"\"\n",
    "    print(\"\\n📊 Résultats de l'évaluation :\")\n",
    "    print(\"\\nMétriques globales :\")\n",
    "    print(f\"{'Métrique':<15} {'Score':<10} {'Interprétation'}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    interpretations = {\n",
    "        'exact_match': \"Correspondance exacte (très stricte)\",\n",
    "        'rouge1': \"Chevauchement de mots uniques\",\n",
    "        'rouge2': \"Chevauchement de bigrammes\",\n",
    "        'rougeL': \"Plus longue sous-séquence commune\",\n",
    "        'bleu': \"Précision des n-grammes\"\n",
    "    }\n",
    "    \n",
    "    for metric, score in metrics.items():\n",
    "        print(f\"{metric:<15} {score:.4f}     {interpretations[metric]}\")\n",
    "    \n",
    "    print(\"\\n💡 Analyse des résultats :\")\n",
    "    print(\"\"\"\n",
    "1. Scores faibles attendus car :\n",
    "   - T5 est entraîné pour le résumé, pas pour NLI\n",
    "   - Les hypothèses NLI ne sont pas des résumés classiques\n",
    "   - La tâche demande du raisonnement logique\n",
    "   \n",
    "2. Améliorations possibles :\n",
    "   - Fine-tuning de T5 sur des données NLI\n",
    "   - Adaptation du prompt pour la tâche NLI\n",
    "   - Utilisation d'un modèle spécialisé pour NLI\n",
    "    \"\"\")\n",
    "    \n",
    "    # Exemples détaillés\n",
    "    print(\"\\n📝 Exemples détaillés :\")\n",
    "    for i in range(min(3, len(generated_summaries))):\n",
    "        print(f\"\\nExemple {i+1}:\")\n",
    "        print(f\"Généré    : {generated_summaries[i]}\")\n",
    "        print(f\"Référence : {reference_summaries[i]}\")\n",
    "        \n",
    "        # Scores individuels\n",
    "        rouge_scores = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\\\n",
    "            .score(reference_summaries[i], generated_summaries[i])\n",
    "        print(f\"ROUGE-1   : {rouge_scores['rouge1'].fmeasure:.4f}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Chemin des données\n",
    "        base_path = \"/Users/ludovicveltz/Documents/Bootcamp_GENAI_2025/Crashcourse/WEEK_6/DAY_1/DATASET\"\n",
    "        \n",
    "        # Chargement avec gestion d'erreurs\n",
    "        print(\"Chargement des données...\")\n",
    "        try:\n",
    "            train_df = pd.read_csv(f\"{base_path}/train.csv\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur de chargement des données: {e}\")\n",
    "            return\n",
    "        \n",
    "        # Échantillon très réduit pour test\n",
    "        sample_size = 5  # Commencez avec un petit échantillon\n",
    "        train_sample = train_df.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "        # Initialisation et traitement\n",
    "        print(\"Initialisation du summarizer...\")\n",
    "        summarizer = T5SummarizerLight()\n",
    "        \n",
    "        print(\"\\nTraitement des textes...\")\n",
    "        premises = train_sample['premise'].tolist()\n",
    "        summaries = summarizer.process_texts(premises)\n",
    "        \n",
    "        # Évaluation\n",
    "        print(\"\\nÉvaluation des résultats...\")\n",
    "        evaluator = SummaryEvaluator()\n",
    "        metrics = evaluator.evaluate_summaries(summaries, train_sample['hypothesis'].tolist())\n",
    "        \n",
    "        # Affichage des résultats\n",
    "        display_evaluation_results(\n",
    "            metrics,\n",
    "            summaries,\n",
    "            train_sample['hypothesis'].tolist()\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'exécution: {e}\")\n",
    "    finally:\n",
    "        # Nettoyage final\n",
    "        if 'summarizer' in locals():\n",
    "            summarizer.clean_memory()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from evaluate) (2.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from evaluate) (0.29.2)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (8.0.0)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.11.10)\n",
      "Requirement already satisfied: responses<0.19 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from pandas->evaluate) (2021.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des données...\n",
      "Initialisation du summarizer...\n",
      "Utilisation de : cpu\n",
      "Chargement du tokenizer...\n",
      "Chargement du modèle...\n",
      "\n",
      "Traitement des textes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Génération des résumés: 100%|██████████| 5/5 [00:18<00:00,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Évaluation ROUGE...\n",
      "Initialisation de l'évaluateur ROUGE...\n",
      "✅ Évaluateur ROUGE initialisé\n",
      "\n",
      "📊 Évaluation des résumés...\n",
      "\n",
      "📈 Scores ROUGE moyens :\n",
      "ROUGE-1 : 0.2783\n",
      "ROUGE-2 : 0.1167\n",
      "ROUGE-L : 0.2392\n",
      "\n",
      "📝 Exemples d'évaluation :\n",
      "\n",
      "Exemple 1:\n",
      "Généré    : на сросила ео с неоиданнм интересом.\n",
      "Référence : Она спросила, как это сделать, так как с её точки зрения это казалось невозможным.\n",
      "Scores    : ROUGE-1=0.0000, ROUGE-2=0.0000, ROUGE-L=0.0000\n",
      "\n",
      "Exemple 2:\n",
      "Généré    : a number of resorts in the japanese Alps are now in the area thanks to the 1998 winter games in Nagano.\n",
      "Référence : There are a lot of resorts in the national park.\n",
      "Scores    : ROUGE-1=0.3871, ROUGE-2=0.2069, ROUGE-L=0.3226\n",
      "\n",
      "Exemple 3:\n",
      "Généré    : trying to keep grass alive during a summer on a piece of ground that big was expensive.\n",
      "Référence : There was no cost in keeping the grass alive in the summer time.\n",
      "Scores    : ROUGE-1=0.3333, ROUGE-2=0.0714, ROUGE-L=0.2667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercice 4\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "class T5SummarizerLight:\n",
    "    def __init__(self, model_name: str = \"t5-small\"):\n",
    "        \"\"\"\n",
    "        Version optimisée du summarizer T5\n",
    "        \"\"\"\n",
    "        # Configuration du device\n",
    "        self.device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.set_per_process_memory_fraction(0.7)\n",
    "            self.device = \"cuda\"\n",
    "        print(f\"Utilisation de : {self.device}\")\n",
    "        \n",
    "        # Paramètres\n",
    "        self.max_length = 100\n",
    "        \n",
    "        # Chargement du modèle\n",
    "        print(\"Chargement du tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        print(\"Chargement du modèle...\")\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            low_cpu_mem_usage=True,\n",
    "            torch_dtype=torch.float32\n",
    "        ).to(self.device)\n",
    "\n",
    "    def clean_memory(self):\n",
    "        \"\"\"Nettoyage mémoire\"\"\"\n",
    "        if self.device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    def process_single_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Traitement d'un seul texte\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Préparation de l'entrée\n",
    "            input_text = f\"summarize: {text}\"\n",
    "            inputs = self.tokenizer(\n",
    "                input_text,\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Génération\n",
    "            with torch.no_grad():\n",
    "                output_ids = self.model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    max_length=self.max_length,\n",
    "                    num_beams=2,\n",
    "                    length_penalty=1.5,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "            \n",
    "            summary = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "            self.clean_memory()\n",
    "            return summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Erreur de génération: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def process_texts(self, texts: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Traitement d'une liste de textes\n",
    "        \"\"\"\n",
    "        summaries = []\n",
    "        for text in tqdm(texts, desc=\"Génération des résumés\"):\n",
    "            summary = self.process_single_text(text)\n",
    "            summaries.append(summary)\n",
    "        return summaries\n",
    "\n",
    "class ROUGEEvaluator:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Évaluateur ROUGE\n",
    "        \"\"\"\n",
    "        print(\"Initialisation de l'évaluateur ROUGE...\")\n",
    "        try:\n",
    "            self.scorer = rouge_scorer.RougeScorer(\n",
    "                ['rouge1', 'rouge2', 'rougeL'],\n",
    "                use_stemmer=True\n",
    "            )\n",
    "            try:\n",
    "                nltk.download('punkt', quiet=True)\n",
    "            except:\n",
    "                print(\"⚠️ Note: NLTK punkt non téléchargé\")\n",
    "            print(\"✅ Évaluateur ROUGE initialisé\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur d'initialisation: {e}\")\n",
    "            raise\n",
    "\n",
    "    def evaluate_summary(self, generated: str, reference: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Évaluation d'un résumé\n",
    "        \"\"\"\n",
    "        try:\n",
    "            scores = self.scorer.score(reference, generated)\n",
    "            return {\n",
    "                'rouge1': scores['rouge1'].fmeasure,\n",
    "                'rouge2': scores['rouge2'].fmeasure,\n",
    "                'rougeL': scores['rougeL'].fmeasure\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Erreur d'évaluation: {e}\")\n",
    "            return {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
    "\n",
    "    def evaluate_batch(self, generated_texts: List[str], reference_texts: List[str]) -> Dict:\n",
    "        \"\"\"\n",
    "        Évaluation d'un lot de résumés\n",
    "        \"\"\"\n",
    "        all_scores = []\n",
    "        print(\"\\n📊 Évaluation des résumés...\")\n",
    "        for gen, ref in zip(generated_texts, reference_texts):\n",
    "            scores = self.evaluate_summary(gen, ref)\n",
    "            all_scores.append(scores)\n",
    "        \n",
    "        avg_scores = {\n",
    "            metric: np.mean([s[metric] for s in all_scores])\n",
    "            for metric in ['rouge1', 'rouge2', 'rougeL']\n",
    "        }\n",
    "        \n",
    "        self.display_results(avg_scores, generated_texts, reference_texts)\n",
    "        return avg_scores\n",
    "\n",
    "    def display_results(self, scores: Dict, generated_texts: List[str], reference_texts: List[str]):\n",
    "        \"\"\"\n",
    "        Affichage des résultats\n",
    "        \"\"\"\n",
    "        print(\"\\n📈 Scores ROUGE moyens :\")\n",
    "        print(f\"ROUGE-1 : {scores['rouge1']:.4f}\")\n",
    "        print(f\"ROUGE-2 : {scores['rouge2']:.4f}\")\n",
    "        print(f\"ROUGE-L : {scores['rougeL']:.4f}\")\n",
    "        \n",
    "        print(\"\\n📝 Exemples d'évaluation :\")\n",
    "        for i in range(min(3, len(generated_texts))):\n",
    "            print(f\"\\nExemple {i+1}:\")\n",
    "            print(f\"Généré    : {generated_texts[i]}\")\n",
    "            print(f\"Référence : {reference_texts[i]}\")\n",
    "            scores = self.evaluate_summary(generated_texts[i], reference_texts[i])\n",
    "            print(f\"Scores    : ROUGE-1={scores['rouge1']:.4f}, \"\n",
    "                  f\"ROUGE-2={scores['rouge2']:.4f}, \"\n",
    "                  f\"ROUGE-L={scores['rougeL']:.4f}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        base_path = \"/Users/ludovicveltz/Documents/Bootcamp_GENAI_2025/Crashcourse/WEEK_6/DAY_1/DATASET\"\n",
    "        \n",
    "        print(\"Chargement des données...\")\n",
    "        train_df = pd.read_csv(f\"{base_path}/train.csv\")\n",
    "        \n",
    "        sample_size = 5\n",
    "        train_sample = train_df.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "        print(\"Initialisation du summarizer...\")\n",
    "        summarizer = T5SummarizerLight()\n",
    "        \n",
    "        print(\"\\nTraitement des textes...\")\n",
    "        premises = train_sample['premise'].tolist()\n",
    "        summaries = summarizer.process_texts(premises)\n",
    "        \n",
    "        print(\"\\nÉvaluation ROUGE...\")\n",
    "        evaluator = ROUGEEvaluator()\n",
    "        scores = evaluator.evaluate_batch(\n",
    "            generated_texts=summaries,\n",
    "            reference_texts=train_sample['hypothesis'].tolist()\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur: {e}\")\n",
    "    finally:\n",
    "        if 'summarizer' in locals():\n",
    "            summarizer.clean_memory()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Lancement des tests de compréhension ROUGE...\n",
      "\n",
      "🔍 Tests de compréhension ROUGE\n",
      "\n",
      "\n",
      "1️⃣ Test de correspondance exacte\n",
      "\n",
      "Texte identique :\n",
      "Référence : Le chat noir dort sur le canapé.\n",
      "Prédiction: Le chat noir dort sur le canapé.\n",
      "\n",
      "Scores :\n",
      "ROUGE-1 : 1.0000\n",
      "ROUGE-2 : 1.0000\n",
      "ROUGE-L : 1.0000\n",
      "\n",
      "💡 Note: Les scores parfaits (1.0) indiquent une correspondance exacte\n",
      "\n",
      "2️⃣ Test de prédiction vide\n",
      "\n",
      "Prédiction vide :\n",
      "Référence : Le chat noir dort.\n",
      "Prédiction: [vide]\n",
      "\n",
      "Scores :\n",
      "ROUGE-1 : 0.0000\n",
      "ROUGE-2 : 0.0000\n",
      "ROUGE-L : 0.0000\n",
      "\n",
      "💡 Note: Les scores nuls (0.0) indiquent l'absence de correspondance\n",
      "\n",
      "3️⃣ Test de l'effet du stemming\n",
      "\n",
      "Comparaison avec/sans stemming :\n",
      "Référence : Les chats noirs dorment sur les canapés.\n",
      "Prédiction: Le chat noir dort sur le canapé.\n",
      "\n",
      "Scores avec stemming :\n",
      "ROUGE-1 : 0.5333\n",
      "ROUGE-2 : 0.1538\n",
      "\n",
      "Scores sans stemming :\n",
      "ROUGE-1 : 0.2667\n",
      "ROUGE-2 : 0.0000\n",
      "\n",
      "💡 Note: Le stemming améliore les scores en normalisant les variations morphologiques\n",
      "\n",
      "4️⃣ Test d'analyse des n-grammes\n",
      "\n",
      "Analyse de différents niveaux de correspondance :\n",
      "\n",
      "Prédiction 1:\n",
      "Texte: Le chat noir dort sur le canapé.\n",
      "ROUGE-1: 0.8750\n",
      "ROUGE-2: 0.7143\n",
      "\n",
      "Prédiction 2:\n",
      "Texte: Un chat noir se repose sur le canapé.\n",
      "ROUGE-1: 0.5882\n",
      "ROUGE-2: 0.4000\n",
      "\n",
      "Prédiction 3:\n",
      "Texte: Un félin sombre sommeille sur le sofa.\n",
      "ROUGE-1: 0.2353\n",
      "ROUGE-2: 0.1333\n",
      "\n",
      "💡 Note: ROUGE-2 est plus strict car il vérifie les paires de mots consécutifs\n",
      "\n",
      "5️⃣ Test de symétrie\n",
      "\n",
      "Test de symétrie :\n",
      "Texte 1 : Le chat noir dort.\n",
      "Texte 2 : Le chien noir court.\n",
      "\n",
      "Scores (1 -> 2) :\n",
      "ROUGE-1 : 0.5000\n",
      "ROUGE-2 : 0.0000\n",
      "\n",
      "Scores (2 -> 1) :\n",
      "ROUGE-1 : 0.5000\n",
      "ROUGE-2 : 0.0000\n",
      "\n",
      "💡 Note: Les scores peuvent varier selon l'ordre référence/prédiction\n"
     ]
    }
   ],
   "source": [
    "# Exercice 5\n",
    "\n",
    "class ROUGEUnderstanding:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Classe pour comprendre le comportement des scores ROUGE\n",
    "        \"\"\"\n",
    "        self.scorer = rouge_scorer.RougeScorer(\n",
    "            ['rouge1', 'rouge2', 'rougeL'],\n",
    "            use_stemmer=True  # Activation du stemming par défaut\n",
    "        )\n",
    "        \n",
    "        # Scorer sans stemming pour comparaison\n",
    "        self.scorer_no_stem = rouge_scorer.RougeScorer(\n",
    "            ['rouge1', 'rouge2', 'rougeL'],\n",
    "            use_stemmer=False\n",
    "        )\n",
    "\n",
    "    def run_all_tests(self):\n",
    "        \"\"\"\n",
    "        Exécute tous les tests de compréhension ROUGE\n",
    "        \"\"\"\n",
    "        print(\"\\n🔍 Tests de compréhension ROUGE\\n\")\n",
    "        \n",
    "        self.test_exact_match()\n",
    "        self.test_null_prediction()\n",
    "        self.test_stemming_effect()\n",
    "        self.test_ngram_analysis()\n",
    "        self.test_symmetry()\n",
    "\n",
    "    def test_exact_match(self):\n",
    "        \"\"\"\n",
    "        Test 1: Correspondance exacte\n",
    "        \"\"\"\n",
    "        print(\"\\n1️⃣ Test de correspondance exacte\")\n",
    "        reference = \"Le chat noir dort sur le canapé.\"\n",
    "        prediction = \"Le chat noir dort sur le canapé.\"\n",
    "        \n",
    "        scores = self.scorer.score(reference, prediction)\n",
    "        \n",
    "        print(\"\\nTexte identique :\")\n",
    "        print(f\"Référence : {reference}\")\n",
    "        print(f\"Prédiction: {prediction}\")\n",
    "        print(\"\\nScores :\")\n",
    "        print(f\"ROUGE-1 : {scores['rouge1'].fmeasure:.4f}\")\n",
    "        print(f\"ROUGE-2 : {scores['rouge2'].fmeasure:.4f}\")\n",
    "        print(f\"ROUGE-L : {scores['rougeL'].fmeasure:.4f}\")\n",
    "        print(\"\\n💡 Note: Les scores parfaits (1.0) indiquent une correspondance exacte\")\n",
    "\n",
    "    def test_null_prediction(self):\n",
    "        \"\"\"\n",
    "        Test 2: Prédiction vide\n",
    "        \"\"\"\n",
    "        print(\"\\n2️⃣ Test de prédiction vide\")\n",
    "        reference = \"Le chat noir dort.\"\n",
    "        prediction = \"\"\n",
    "        \n",
    "        scores = self.scorer.score(reference, prediction)\n",
    "        \n",
    "        print(\"\\nPrédiction vide :\")\n",
    "        print(f\"Référence : {reference}\")\n",
    "        print(f\"Prédiction: [vide]\")\n",
    "        print(\"\\nScores :\")\n",
    "        print(f\"ROUGE-1 : {scores['rouge1'].fmeasure:.4f}\")\n",
    "        print(f\"ROUGE-2 : {scores['rouge2'].fmeasure:.4f}\")\n",
    "        print(f\"ROUGE-L : {scores['rougeL'].fmeasure:.4f}\")\n",
    "        print(\"\\n💡 Note: Les scores nuls (0.0) indiquent l'absence de correspondance\")\n",
    "\n",
    "    def test_stemming_effect(self):\n",
    "        \"\"\"\n",
    "        Test 3: Effet du stemming\n",
    "        \"\"\"\n",
    "        print(\"\\n3️⃣ Test de l'effet du stemming\")\n",
    "        reference = \"Les chats noirs dorment sur les canapés.\"\n",
    "        prediction = \"Le chat noir dort sur le canapé.\"\n",
    "        \n",
    "        # Avec stemming\n",
    "        scores_stem = self.scorer.score(reference, prediction)\n",
    "        # Sans stemming\n",
    "        scores_no_stem = self.scorer_no_stem.score(reference, prediction)\n",
    "        \n",
    "        print(\"\\nComparaison avec/sans stemming :\")\n",
    "        print(f\"Référence : {reference}\")\n",
    "        print(f\"Prédiction: {prediction}\")\n",
    "        print(\"\\nScores avec stemming :\")\n",
    "        print(f\"ROUGE-1 : {scores_stem['rouge1'].fmeasure:.4f}\")\n",
    "        print(f\"ROUGE-2 : {scores_stem['rouge2'].fmeasure:.4f}\")\n",
    "        print(\"\\nScores sans stemming :\")\n",
    "        print(f\"ROUGE-1 : {scores_no_stem['rouge1'].fmeasure:.4f}\")\n",
    "        print(f\"ROUGE-2 : {scores_no_stem['rouge2'].fmeasure:.4f}\")\n",
    "        print(\"\\n💡 Note: Le stemming améliore les scores en normalisant les variations morphologiques\")\n",
    "\n",
    "    def test_ngram_analysis(self):\n",
    "        \"\"\"\n",
    "        Test 4: Analyse des n-grammes\n",
    "        \"\"\"\n",
    "        print(\"\\n4️⃣ Test d'analyse des n-grammes\")\n",
    "        reference = \"Le chat noir dort paisiblement sur le canapé confortable.\"\n",
    "        predictions = [\n",
    "            \"Le chat noir dort sur le canapé.\",  # Bonne correspondance\n",
    "            \"Un chat noir se repose sur le canapé.\",  # Correspondance partielle\n",
    "            \"Un félin sombre sommeille sur le sofa.\"  # Correspondance faible\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nAnalyse de différents niveaux de correspondance :\")\n",
    "        for i, pred in enumerate(predictions, 1):\n",
    "            scores = self.scorer.score(reference, pred)\n",
    "            print(f\"\\nPrédiction {i}:\")\n",
    "            print(f\"Texte: {pred}\")\n",
    "            print(f\"ROUGE-1: {scores['rouge1'].fmeasure:.4f}\")\n",
    "            print(f\"ROUGE-2: {scores['rouge2'].fmeasure:.4f}\")\n",
    "        \n",
    "        print(\"\\n💡 Note: ROUGE-2 est plus strict car il vérifie les paires de mots consécutifs\")\n",
    "\n",
    "    def test_symmetry(self):\n",
    "        \"\"\"\n",
    "        Test 5: Symétrie des scores\n",
    "        \"\"\"\n",
    "        print(\"\\n5️⃣ Test de symétrie\")\n",
    "        text1 = \"Le chat noir dort.\"\n",
    "        text2 = \"Le chien noir court.\"\n",
    "        \n",
    "        scores_1_2 = self.scorer.score(text1, text2)\n",
    "        scores_2_1 = self.scorer.score(text2, text1)\n",
    "        \n",
    "        print(\"\\nTest de symétrie :\")\n",
    "        print(f\"Texte 1 : {text1}\")\n",
    "        print(f\"Texte 2 : {text2}\")\n",
    "        print(\"\\nScores (1 -> 2) :\")\n",
    "        print(f\"ROUGE-1 : {scores_1_2['rouge1'].fmeasure:.4f}\")\n",
    "        print(f\"ROUGE-2 : {scores_1_2['rouge2'].fmeasure:.4f}\")\n",
    "        print(\"\\nScores (2 -> 1) :\")\n",
    "        print(f\"ROUGE-1 : {scores_2_1['rouge1'].fmeasure:.4f}\")\n",
    "        print(f\"ROUGE-2 : {scores_2_1['rouge2'].fmeasure:.4f}\")\n",
    "        print(\"\\n💡 Note: Les scores peuvent varier selon l'ordre référence/prédiction\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Tests de compréhension ROUGE\n",
    "        print(\"\\n🎯 Lancement des tests de compréhension ROUGE...\")\n",
    "        rouge_understanding = ROUGEUnderstanding()\n",
    "        rouge_understanding.run_all_tests()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors des tests: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m766.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from langdetect) (1.17.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993284 sha256=1ef566991e8afa15f7586f59c134676004ddc227800c31711b752df31cf03d96\n",
      "  Stored in directory: /Users/ludovicveltz/Library/Caches/pip/wheels/d1/c1/d9/7e068de779d863bc8f8fc9467d85e25cfe47fa5051fff1a1bb\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (0.29.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (0.1.96)\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: protobuf 4.25.6\n",
      "Uninstalling protobuf-4.25.6:\n",
      "  Successfully uninstalled protobuf-4.25.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf==3.20.0\n",
      "  Downloading protobuf-3.20.0-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Downloading protobuf-3.20.0-py2.py3-none-any.whl (162 kB)\n",
      "Installing collected packages: protobuf\n",
      "Successfully installed protobuf-3.20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (0.29.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y protobuf\n",
    "!pip install protobuf==3.20.0\n",
    "!pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des données...\n",
      "🖥️ Utilisation de : cpu\n",
      "\n",
      "🔄 Traitement avec mbart-large-50...\n",
      "\n",
      "📚 Chargement du modèle mbart-large-50...\n",
      "❌ Erreur lors du chargement de mbart-large-50: Couldn't build proto file into descriptor pool: duplicate file name sentencepiece_model.proto\n",
      "❌ Erreur: Couldn't build proto file into descriptor pool: duplicate file name sentencepiece_model.proto\n",
      "🧹 Mémoire nettoyée\n"
     ]
    }
   ],
   "source": [
    "# Exercice 5\n",
    "\n",
    "!pip install --upgrade protobuf==3.20.0 sentencepiece --quiet\n",
    "\n",
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, AutoTokenizer, T5ForConditionalGeneration\n",
    "from langdetect import detect\n",
    "import pandas as pd\n",
    "import torch\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "class MultilingualModelComparator:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialisation du comparateur avec modèles multilingues\n",
    "        \"\"\"\n",
    "        self.models = {}\n",
    "        self.tokenizers = {}\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(\n",
    "            ['rouge1', 'rouge2', 'rougeL'],\n",
    "            use_stemmer=True\n",
    "        )\n",
    "        print(f\"🖥️ Utilisation de : {self.device}\")\n",
    "\n",
    "    def clean_memory(self):\n",
    "        \"\"\"\n",
    "        Nettoyage de la mémoire GPU/CPU\n",
    "        \"\"\"\n",
    "        try:\n",
    "            for model in self.models.values():\n",
    "                model.cpu()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            print(\"🧹 Mémoire nettoyée\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Erreur lors du nettoyage mémoire: {e}\")\n",
    "\n",
    "    def load_model(self, model_name: str):\n",
    "        \"\"\"\n",
    "        Chargement des modèles multilingues\n",
    "        \"\"\"\n",
    "        print(f\"\\n📚 Chargement du modèle {model_name}...\")\n",
    "        try:\n",
    "            if model_name == \"mbart-large-50\":\n",
    "                self.tokenizers[model_name] = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\")\n",
    "                self.models[model_name] = MBartForConditionalGeneration.from_pretrained(\n",
    "                    \"facebook/mbart-large-50\",\n",
    "                    low_cpu_mem_usage=True\n",
    "                ).to(self.device)\n",
    "            elif \"mt5\" in model_name:\n",
    "                self.tokenizers[model_name] = AutoTokenizer.from_pretrained(f\"google/{model_name}\")\n",
    "                self.models[model_name] = T5ForConditionalGeneration.from_pretrained(\n",
    "                    f\"google/{model_name}\",\n",
    "                    low_cpu_mem_usage=True\n",
    "                ).to(self.device)\n",
    "            print(f\"✅ Modèle {model_name} chargé avec succès\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur lors du chargement de {model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def detect_language_simple(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Détection simple de la langue basée sur les caractères\n",
    "        \"\"\"\n",
    "        # Détection basique pour le russe (cyrillique)\n",
    "        if any(ord('а') <= ord(c) <= ord('я') for c in text.lower()):\n",
    "            return 'ru'\n",
    "        # Détection basique pour le chinois\n",
    "        if any('\\u4e00' <= c <= '\\u9fff' for c in text):\n",
    "            return 'zh'\n",
    "        # Par défaut, on suppose de l'anglais\n",
    "        return 'en'\n",
    "\n",
    "\n",
    "    def preprocess_text(self, text: str, model_name: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Prétraitement avec détection de langue\n",
    "        \"\"\"\n",
    "        lang = self.detect_language(text)\n",
    "        if model_name == \"mbart-large-50\":\n",
    "            src_lang = f\"xx_{lang}\" if lang != \"unknown\" else \"xx_en\"\n",
    "            return {\n",
    "                \"text\": text,\n",
    "                \"src_lang\": src_lang,\n",
    "                \"tgt_lang\": src_lang  # Même langue pour le résumé\n",
    "            }\n",
    "        return {\"text\": text, \"lang\": lang}\n",
    "\n",
    "    def summarize_with_mbart(self, text: str, model_name: str, lang_info: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Génération de résumé avec mBART\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.tokenizers[model_name].src_lang = lang_info[\"src_lang\"]\n",
    "            inputs = self.tokenizers[model_name](\n",
    "                text,\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.models[model_name].generate(\n",
    "                    inputs.input_ids,\n",
    "                    forced_bos_token_id=self.tokenizers[model_name].lang_code_to_id[lang_info[\"tgt_lang\"]],\n",
    "                    max_length=150,\n",
    "                    num_beams=2,\n",
    "                    length_penalty=1.5,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "\n",
    "            summary = self.tokenizers[model_name].decode(outputs[0], skip_special_tokens=True)\n",
    "            return summary\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Erreur mBART: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def summarize_with_mt5(self, text: str, model_name: str, lang_info: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Génération de résumé avec mT5\n",
    "        \"\"\"\n",
    "        try:\n",
    "            input_text = f\"summarize: {text}\"\n",
    "            inputs = self.tokenizers[model_name](\n",
    "                input_text,\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.models[model_name].generate(\n",
    "                    inputs.input_ids,\n",
    "                    max_length=150,\n",
    "                    num_beams=2,\n",
    "                    length_penalty=1.5,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "\n",
    "            summary = self.tokenizers[model_name].decode(outputs[0], skip_special_tokens=True)\n",
    "            return summary\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Erreur mT5: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def compare_models(self, texts: List[str], references: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Comparaison des modèles multilingues\n",
    "        \"\"\"\n",
    "        models_to_compare = ['mbart-large-50', 'mt5-small', 'mt5-base']\n",
    "        results = []\n",
    "\n",
    "        for model_name in models_to_compare:\n",
    "            print(f\"\\n🔄 Traitement avec {model_name}...\")\n",
    "            self.load_model(model_name)\n",
    "            \n",
    "            for i, (text, ref) in enumerate(zip(texts, references)):\n",
    "                # Prétraitement avec détection de langue\n",
    "                processed = self.preprocess_text(text, model_name)\n",
    "                \n",
    "                # Génération du résumé\n",
    "                if \"mbart\" in model_name:\n",
    "                    summary = self.summarize_with_mbart(text, model_name, processed)\n",
    "                else:\n",
    "                    summary = self.summarize_with_mt5(text, model_name, processed)\n",
    "                \n",
    "                # Calcul des scores ROUGE\n",
    "                rouge_scores = self.compute_rouge_per_row(summary, ref)\n",
    "                \n",
    "                results.append({\n",
    "                    'model': model_name,\n",
    "                    'text_id': i,\n",
    "                    'language': processed.get('lang', processed.get('src_lang')),\n",
    "                    'original': text,\n",
    "                    'reference': ref,\n",
    "                    'generated': summary,\n",
    "                    'rouge1': rouge_scores['rouge1'],\n",
    "                    'rouge2': rouge_scores['rouge2'],\n",
    "                    'rougeL': rouge_scores['rougeL']\n",
    "                })\n",
    "            \n",
    "            self.clean_memory()\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "def display_results(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Affichage amélioré des résultats\n",
    "    \"\"\"\n",
    "    print(\"\\n📊 Résultats de la comparaison des modèles\\n\")\n",
    "    \n",
    "    # Scores moyens par modèle et par langue\n",
    "    print(\"Scores ROUGE moyens par modèle et langue :\")\n",
    "    mean_scores = df.groupby(['model', 'language'])[['rouge1', 'rouge2', 'rougeL']].mean()\n",
    "    print(mean_scores)\n",
    "    \n",
    "    # Exemples détaillés\n",
    "    print(\"\\n📝 Exemples de génération par modèle :\")\n",
    "    for model in df['model'].unique():\n",
    "        for lang in df['language'].unique():\n",
    "            sample = df[(df['model'] == model) & (df['language'] == lang)].iloc[0]\n",
    "            print(f\"\\n🤖 Modèle : {model} | Langue : {lang}\")\n",
    "            print(f\"Original  : {sample['original'][:100]}...\")\n",
    "            print(f\"Généré    : {sample['generated']}\")\n",
    "            print(f\"Référence : {sample['reference']}\")\n",
    "            print(f\"ROUGE-1   : {sample['rouge1']:.4f}\")\n",
    "            print(f\"ROUGE-2   : {sample['rouge2']:.4f}\")\n",
    "            print(f\"ROUGE-L   : {sample['rougeL']:.4f}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Chargement des données\n",
    "        base_path = \"/Users/ludovicveltz/Documents/Bootcamp_GENAI_2025/Crashcourse/WEEK_6/DAY_1/DATASET\"\n",
    "        print(\"Chargement des données...\")\n",
    "        train_df = pd.read_csv(f\"{base_path}/train.csv\")\n",
    "        \n",
    "        # Échantillon très réduit pour test\n",
    "        sample_size = 2  # Réduit à 2 pour test\n",
    "        train_sample = train_df.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "        # Initialisation du comparateur\n",
    "        comparator = MultilingualModelComparator()\n",
    "        \n",
    "        # Comparaison des modèles\n",
    "        results_df = comparator.compare_models(\n",
    "            texts=train_sample['premise'].tolist(),\n",
    "            references=train_sample['hypothesis'].tolist()\n",
    "        )\n",
    "        \n",
    "        # Affichage des résultats\n",
    "        display_results(results_df)\n",
    "        \n",
    "        # Sauvegarde des résultats\n",
    "        results_df.to_csv(f\"{base_path}/multilingual_model_comparison_results.csv\", index=False)\n",
    "        print(\"\\n✅ Résultats sauvegardés dans multilingual_model_comparison_results.csv\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur: {e}\")\n",
    "    finally:\n",
    "        if 'comparator' in locals():\n",
    "            comparator.clean_memory()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
