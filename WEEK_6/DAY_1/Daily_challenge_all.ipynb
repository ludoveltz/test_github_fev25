{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_6ae5c th {\n",
       "  font-weight: bold;\n",
       "  text-align: left;\n",
       "  background-color: #2c3e50;\n",
       "  color: white;\n",
       "  padding: 12px;\n",
       "  font-size: 14px;\n",
       "}\n",
       "#T_6ae5c td {\n",
       "  padding: 12px;\n",
       "  white-space: pre-wrap;\n",
       "  border-bottom: 1px solid #ddd;\n",
       "  line-height: 1.5;\n",
       "  font-size: 13px;\n",
       "}\n",
       "#T_6ae5c_row0_col0, #T_6ae5c_row0_col1, #T_6ae5c_row0_col2, #T_6ae5c_row1_col0, #T_6ae5c_row1_col1, #T_6ae5c_row1_col2, #T_6ae5c_row2_col0, #T_6ae5c_row2_col1, #T_6ae5c_row2_col2, #T_6ae5c_row3_col0, #T_6ae5c_row3_col1, #T_6ae5c_row3_col2, #T_6ae5c_row4_col0, #T_6ae5c_row4_col1, #T_6ae5c_row4_col2, #T_6ae5c_row5_col0, #T_6ae5c_row5_col1, #T_6ae5c_row5_col2, #T_6ae5c_row6_col0, #T_6ae5c_row6_col1, #T_6ae5c_row6_col2 {\n",
       "  white-space: pre-wrap;\n",
       "  text-align: left;\n",
       "  padding: 12px;\n",
       "  background-color: #ffffff;\n",
       "  vertical-align: top;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_6ae5c\">\n",
       "  <caption>Comparaison Détaillée BERT vs XLM-RoBERTa</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_6ae5c_level0_col0\" class=\"col_heading level0 col0\" >Caractéristiques</th>\n",
       "      <th id=\"T_6ae5c_level0_col1\" class=\"col_heading level0 col1\" >BERT</th>\n",
       "      <th id=\"T_6ae5c_level0_col2\" class=\"col_heading level0 col2\" >XLM-RoBERTa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6ae5c_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_6ae5c_row0_col0\" class=\"data row0 col0\" >Architecture de Base</td>\n",
       "      <td id=\"T_6ae5c_row0_col1\" class=\"data row0 col1\" >• Architecture Transformer Encoder\n",
       "• Attention bidirectionnelle\n",
       "• MLM et NSP comme objectifs\n",
       "• 12/24 couches selon version</td>\n",
       "      <td id=\"T_6ae5c_row0_col2\" class=\"data row0 col2\" >• Architecture RoBERTa optimisée\n",
       "• Attention améliorée\n",
       "• Sans NSP\n",
       "• 12/24 couches selon version</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ae5c_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_6ae5c_row1_col0\" class=\"data row1 col0\" >Tokenisation</td>\n",
       "      <td id=\"T_6ae5c_row1_col1\" class=\"data row1 col1\" >• WordPiece tokenization\n",
       "• Vocabulaire fixe de 30k tokens\n",
       "• Gestion sous-mots\n",
       "• Tokens spéciaux ([CLS], [SEP])</td>\n",
       "      <td id=\"T_6ae5c_row1_col2\" class=\"data row1 col2\" >• SentencePiece tokenization\n",
       "• Vocabulaire partagé 250k tokens\n",
       "• Meilleure gestion cross-langue\n",
       "• Tokens universels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ae5c_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_6ae5c_row2_col0\" class=\"data row2 col0\" >Capacités Multilingues</td>\n",
       "      <td id=\"T_6ae5c_row2_col1\" class=\"data row2 col1\" >• Versions monolingues principales\n",
       "• mBERT pour multi-langue\n",
       "• Performance variable selon langue\n",
       "• Adapté aux langues principales</td>\n",
       "      <td id=\"T_6ae5c_row2_col2\" class=\"data row2 col2\" >• Natif multilingue\n",
       "• 100+ langues supportées\n",
       "• Performance consistante\n",
       "• Transfer learning efficace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ae5c_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_6ae5c_row3_col0\" class=\"data row3 col0\" >Taille du Vocabulaire</td>\n",
       "      <td id=\"T_6ae5c_row3_col1\" class=\"data row3 col1\" >• 30,522 tokens (base)\n",
       "• Vocabulaire par langue\n",
       "• Optimisé par domaine\n",
       "• Extensions possibles</td>\n",
       "      <td id=\"T_6ae5c_row3_col2\" class=\"data row3 col2\" >• 250,002 tokens (universel)\n",
       "• Vocabulaire commun\n",
       "• Couverture large\n",
       "• Efficacité cross-langue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ae5c_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_6ae5c_row4_col0\" class=\"data row4 col0\" >Pré-entraînement</td>\n",
       "      <td id=\"T_6ae5c_row4_col1\" class=\"data row4 col1\" >• Wikipedia + BookCorpus\n",
       "• 3.3B mots\n",
       "• Approche MLM classique\n",
       "• Next Sentence Prediction</td>\n",
       "      <td id=\"T_6ae5c_row4_col2\" class=\"data row4 col2\" >• CC-100 dataset\n",
       "• 2.5TB données multilingues\n",
       "• Dynamic masking\n",
       "• Sans NSP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ae5c_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_6ae5c_row5_col0\" class=\"data row5 col0\" >Versions Disponibles</td>\n",
       "      <td id=\"T_6ae5c_row5_col1\" class=\"data row5 col1\" >• bert-base-uncased (110M)\n",
       "• bert-large-uncased (340M)\n",
       "• bert-base-multilingual\n",
       "• Versions spécialisées</td>\n",
       "      <td id=\"T_6ae5c_row5_col2\" class=\"data row5 col2\" >• xlm-roberta-base (270M)\n",
       "• xlm-roberta-large (550M)\n",
       "• Versions fine-tunées\n",
       "• Adaptations spécifiques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ae5c_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_6ae5c_row6_col0\" class=\"data row6 col0\" >Use Cases Optimaux</td>\n",
       "      <td id=\"T_6ae5c_row6_col1\" class=\"data row6 col1\" >• Classification de texte\n",
       "• Analyse de sentiment\n",
       "• QA monolingue\n",
       "• Tâches spécifiques langue</td>\n",
       "      <td id=\"T_6ae5c_row6_col2\" class=\"data row6 col2\" >• Classification multilingue\n",
       "• Zero-shot cross-langue\n",
       "• Transfer learning\n",
       "• Applications globales</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x10583a490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Tableau comparatif BERT vs XLM-RoBERTa\n",
    "data_models = {\n",
    "    'Caractéristiques': [\n",
    "        'Architecture de Base',\n",
    "        'Tokenisation',\n",
    "        'Capacités Multilingues',\n",
    "        'Taille du Vocabulaire',\n",
    "        'Pré-entraînement',\n",
    "        'Versions Disponibles',\n",
    "        'Use Cases Optimaux'\n",
    "    ],\n",
    "    'BERT': [\n",
    "        \"• Architecture Transformer Encoder\\n\"\n",
    "        \"• Attention bidirectionnelle\\n\"\n",
    "        \"• MLM et NSP comme objectifs\\n\"\n",
    "        \"• 12/24 couches selon version\",\n",
    "        \n",
    "        \"• WordPiece tokenization\\n\"\n",
    "        \"• Vocabulaire fixe de 30k tokens\\n\"\n",
    "        \"• Gestion sous-mots\\n\"\n",
    "        \"• Tokens spéciaux ([CLS], [SEP])\",\n",
    "        \n",
    "        \"• Versions monolingues principales\\n\"\n",
    "        \"• mBERT pour multi-langue\\n\"\n",
    "        \"• Performance variable selon langue\\n\"\n",
    "        \"• Adapté aux langues principales\",\n",
    "        \n",
    "        \"• 30,522 tokens (base)\\n\"\n",
    "        \"• Vocabulaire par langue\\n\"\n",
    "        \"• Optimisé par domaine\\n\"\n",
    "        \"• Extensions possibles\",\n",
    "        \n",
    "        \"• Wikipedia + BookCorpus\\n\"\n",
    "        \"• 3.3B mots\\n\"\n",
    "        \"• Approche MLM classique\\n\"\n",
    "        \"• Next Sentence Prediction\",\n",
    "        \n",
    "        \"• bert-base-uncased (110M)\\n\"\n",
    "        \"• bert-large-uncased (340M)\\n\"\n",
    "        \"• bert-base-multilingual\\n\"\n",
    "        \"• Versions spécialisées\",\n",
    "        \n",
    "        \"• Classification de texte\\n\"\n",
    "        \"• Analyse de sentiment\\n\"\n",
    "        \"• QA monolingue\\n\"\n",
    "        \"• Tâches spécifiques langue\"\n",
    "    ],\n",
    "    'XLM-RoBERTa': [\n",
    "        \"• Architecture RoBERTa optimisée\\n\"\n",
    "        \"• Attention améliorée\\n\"\n",
    "        \"• Sans NSP\\n\"\n",
    "        \"• 12/24 couches selon version\",\n",
    "        \n",
    "        \"• SentencePiece tokenization\\n\"\n",
    "        \"• Vocabulaire partagé 250k tokens\\n\"\n",
    "        \"• Meilleure gestion cross-langue\\n\"\n",
    "        \"• Tokens universels\",\n",
    "        \n",
    "        \"• Natif multilingue\\n\"\n",
    "        \"• 100+ langues supportées\\n\"\n",
    "        \"• Performance consistante\\n\"\n",
    "        \"• Transfer learning efficace\",\n",
    "        \n",
    "        \"• 250,002 tokens (universel)\\n\"\n",
    "        \"• Vocabulaire commun\\n\"\n",
    "        \"• Couverture large\\n\"\n",
    "        \"• Efficacité cross-langue\",\n",
    "        \n",
    "        \"• CC-100 dataset\\n\"\n",
    "        \"• 2.5TB données multilingues\\n\"\n",
    "        \"• Dynamic masking\\n\"\n",
    "        \"• Sans NSP\",\n",
    "        \n",
    "        \"• xlm-roberta-base (270M)\\n\"\n",
    "        \"• xlm-roberta-large (550M)\\n\"\n",
    "        \"• Versions fine-tunées\\n\"\n",
    "        \"• Adaptations spécifiques\",\n",
    "        \n",
    "        \"• Classification multilingue\\n\"\n",
    "        \"• Zero-shot cross-langue\\n\"\n",
    "        \"• Transfer learning\\n\"\n",
    "        \"• Applications globales\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_models = pd.DataFrame(data_models)\n",
    "\n",
    "# Style du tableau\n",
    "styles = [\n",
    "    dict(selector=\"th\", props=[(\"font-weight\", \"bold\"),\n",
    "                              (\"text-align\", \"left\"),\n",
    "                              (\"background-color\", \"#2c3e50\"),\n",
    "                              (\"color\", \"white\"),\n",
    "                              (\"padding\", \"12px\"),\n",
    "                              (\"font-size\", \"14px\")]),\n",
    "    dict(selector=\"td\", props=[(\"padding\", \"12px\"),\n",
    "                              (\"white-space\", \"pre-wrap\"),\n",
    "                              (\"border-bottom\", \"1px solid #ddd\"),\n",
    "                              (\"line-height\", \"1.5\"),\n",
    "                              (\"font-size\", \"13px\")])\n",
    "]\n",
    "\n",
    "styled_df_models = df_models.style\\\n",
    "    .set_table_styles(styles)\\\n",
    "    .set_properties(**{\n",
    "        'white-space': 'pre-wrap',\n",
    "        'text-align': 'left',\n",
    "        'padding': '12px',\n",
    "        'background-color': '#ffffff',\n",
    "        'vertical-align': 'top'\n",
    "    })\\\n",
    "    .set_caption(\"Comparaison Détaillée BERT vs XLM-RoBERTa\")\n",
    "\n",
    "display(styled_df_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT tokens: ['l', \"'\", 'intelligence', 'art', '##ific', '##iel', '##le', 'transform', '##e', 'notre', 'monde', '.']\n",
      "XLM-RoBERTa tokens: ['▁L', \"'\", 'intelligence', '▁artifici', 'elle', '▁transform', 'e', '▁notre', '▁monde', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, XLMRobertaTokenizer\n",
    "\n",
    "# Exemple de texte\n",
    "text = \"L'intelligence artificielle transforme notre monde.\"\n",
    "\n",
    "# BERT Tokenization\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_tokens = bert_tokenizer.tokenize(text)\n",
    "print(\"BERT tokens:\", bert_tokens)\n",
    "\n",
    "# XLM-RoBERTa Tokenization\n",
    "xlm_tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "xlm_tokens = xlm_tokenizer.tokenize(text)\n",
    "print(\"XLM-RoBERTa tokens:\", xlm_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b4a5a th {\n",
       "  font-weight: bold;\n",
       "  text-align: left;\n",
       "  background-color: #2c3e50;\n",
       "  color: white;\n",
       "  padding: 12px;\n",
       "  font-size: 14px;\n",
       "}\n",
       "#T_b4a5a td {\n",
       "  padding: 12px;\n",
       "  white-space: pre-wrap;\n",
       "  border-bottom: 1px solid #ddd;\n",
       "  line-height: 1.5;\n",
       "  font-size: 13px;\n",
       "}\n",
       "#T_b4a5a_row0_col0, #T_b4a5a_row0_col1, #T_b4a5a_row0_col2, #T_b4a5a_row0_col3, #T_b4a5a_row1_col0, #T_b4a5a_row1_col1, #T_b4a5a_row1_col2, #T_b4a5a_row1_col3 {\n",
       "  white-space: pre-wrap;\n",
       "  text-align: left;\n",
       "  padding: 12px;\n",
       "  background-color: #ffffff;\n",
       "  vertical-align: top;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b4a5a\">\n",
       "  <caption>Analyse des Résultats de Tokenisation</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b4a5a_level0_col0\" class=\"col_heading level0 col0\" >Modèle</th>\n",
       "      <th id=\"T_b4a5a_level0_col1\" class=\"col_heading level0 col1\" >Tokens Générés</th>\n",
       "      <th id=\"T_b4a5a_level0_col2\" class=\"col_heading level0 col2\" >Particularités Observées</th>\n",
       "      <th id=\"T_b4a5a_level0_col3\" class=\"col_heading level0 col3\" >Implications pour S&I</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b4a5a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_b4a5a_row0_col0\" class=\"data row0 col0\" >BERT</td>\n",
       "      <td id=\"T_b4a5a_row0_col1\" class=\"data row0 col1\" >• l\n",
       "• '\n",
       "• intelligence\n",
       "• art\n",
       "• ##ific\n",
       "• ##iel\n",
       "• ##le\n",
       "• transform\n",
       "• ##e\n",
       "• notre\n",
       "• monde\n",
       "• .</td>\n",
       "      <td id=\"T_b4a5a_row0_col2\" class=\"data row0 col2\" >• Découpage plus granulaire\n",
       "• Utilisation de '##' pour les sous-mots\n",
       "• Décomposition de 'artificielle' en 4 tokens\n",
       "• Conservation de la casse d'origine</td>\n",
       "      <td id=\"T_b4a5a_row0_col3\" class=\"data row0 col3\" >• Optimal pour analyse fine du français juridique\n",
       "• Meilleur pour détection nuances techniques\n",
       "• Adapté aux documents spécialisés\n",
       "• Précision accrue pour l'analyse détaillée</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b4a5a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_b4a5a_row1_col0\" class=\"data row1 col0\" >XLM-RoBERTa</td>\n",
       "      <td id=\"T_b4a5a_row1_col1\" class=\"data row1 col1\" >• ▁L\n",
       "• '\n",
       "• intelligence\n",
       "• ▁artifici\n",
       "• elle\n",
       "• ▁transform\n",
       "• e\n",
       "• ▁notre\n",
       "• ▁monde\n",
       "• .</td>\n",
       "      <td id=\"T_b4a5a_row1_col2\" class=\"data row1 col2\" >• Utilisation de '▁' pour marquer les débuts de mots\n",
       "• Tokenisation plus naturelle\n",
       "• Meilleure gestion des mots composés\n",
       "• Moins de fragmentation</td>\n",
       "      <td id=\"T_b4a5a_row1_col3\" class=\"data row1 col3\" >• Idéal pour contenu multilingue\n",
       "• Meilleur pour créativité cross-culturelle\n",
       "• Adapté aux projets internationaux\n",
       "• Flexible pour différents contextes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x105835e20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Création d'un tableau comparatif des résultats\n",
    "data_tokens = {\n",
    "    'Modèle': [\n",
    "        'BERT',\n",
    "        'XLM-RoBERTa'\n",
    "    ],\n",
    "    'Tokens Générés': [\n",
    "        \"• l\\n\"\n",
    "        \"• '\\n\"\n",
    "        \"• intelligence\\n\"\n",
    "        \"• art\\n\"\n",
    "        \"• ##ific\\n\"\n",
    "        \"• ##iel\\n\"\n",
    "        \"• ##le\\n\"\n",
    "        \"• transform\\n\"\n",
    "        \"• ##e\\n\"\n",
    "        \"• notre\\n\"\n",
    "        \"• monde\\n\"\n",
    "        \"• .\",\n",
    "        \n",
    "        \"• ▁L\\n\"\n",
    "        \"• '\\n\"\n",
    "        \"• intelligence\\n\"\n",
    "        \"• ▁artifici\\n\"\n",
    "        \"• elle\\n\"\n",
    "        \"• ▁transform\\n\"\n",
    "        \"• e\\n\"\n",
    "        \"• ▁notre\\n\"\n",
    "        \"• ▁monde\\n\"\n",
    "        \"• .\"\n",
    "    ],\n",
    "    'Particularités Observées': [\n",
    "        \"• Découpage plus granulaire\\n\"\n",
    "        \"• Utilisation de '##' pour les sous-mots\\n\"\n",
    "        \"• Décomposition de 'artificielle' en 4 tokens\\n\"\n",
    "        \"• Conservation de la casse d'origine\",\n",
    "        \n",
    "        \"• Utilisation de '▁' pour marquer les débuts de mots\\n\"\n",
    "        \"• Tokenisation plus naturelle\\n\"\n",
    "        \"• Meilleure gestion des mots composés\\n\"\n",
    "        \"• Moins de fragmentation\"\n",
    "    ],\n",
    "    'Implications pour S&I': [\n",
    "        \"• Optimal pour analyse fine du français juridique\\n\"\n",
    "        \"• Meilleur pour détection nuances techniques\\n\"\n",
    "        \"• Adapté aux documents spécialisés\\n\"\n",
    "        \"• Précision accrue pour l'analyse détaillée\",\n",
    "        \n",
    "        \"• Idéal pour contenu multilingue\\n\"\n",
    "        \"• Meilleur pour créativité cross-culturelle\\n\"\n",
    "        \"• Adapté aux projets internationaux\\n\"\n",
    "        \"• Flexible pour différents contextes\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_tokens = pd.DataFrame(data_tokens)\n",
    "\n",
    "# Style du tableau\n",
    "styles = [\n",
    "    dict(selector=\"th\", props=[(\"font-weight\", \"bold\"),\n",
    "                              (\"text-align\", \"left\"),\n",
    "                              (\"background-color\", \"#2c3e50\"),\n",
    "                              (\"color\", \"white\"),\n",
    "                              (\"padding\", \"12px\"),\n",
    "                              (\"font-size\", \"14px\")]),\n",
    "    dict(selector=\"td\", props=[(\"padding\", \"12px\"),\n",
    "                              (\"white-space\", \"pre-wrap\"),\n",
    "                              (\"border-bottom\", \"1px solid #ddd\"),\n",
    "                              (\"line-height\", \"1.5\"),\n",
    "                              (\"font-size\", \"13px\")])\n",
    "]\n",
    "\n",
    "styled_df_tokens = df_tokens.style\\\n",
    "    .set_table_styles(styles)\\\n",
    "    .set_properties(**{\n",
    "        'white-space': 'pre-wrap',\n",
    "        'text-align': 'left',\n",
    "        'padding': '12px',\n",
    "        'background-color': '#ffffff',\n",
    "        'vertical-align': 'top'\n",
    "    })\\\n",
    "    .set_caption(\"Analyse des Résultats de Tokenisation\")\n",
    "\n",
    "display(styled_df_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Tokenization Simple:\n",
      "\n",
      "BERT tokens: ['le', 'deep', 'learning', 'revolution', '##ne', 'l', \"'\", 'ia', '.']\n",
      "BERT input_ids: tensor([[  101,  3393,  2784,  4083,  4329,  2638,  1048,  1005, 24264,  1012,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])\n",
      "BERT attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "\n",
      "XLM-RoBERTa tokens: ['▁Le', '▁deep', '▁learning', '▁révolution', 'ne', '▁l', \"'\", 'IA', '.']\n",
      "XLM-RoBERTa input_ids: tensor([[     0,    636,  53894,  52080, 195839,     86,     96,     25,  17481,\n",
      "              5,      2,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1]])\n",
      "XLM-RoBERTa attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "\n",
      "2. Tokenization Paire de Phrases (BERT):\n",
      "Tokens: ['l', \"'\", 'app', '##rent', '##issa', '##ge', 'auto', '##mat', '##ique', 'est', 'pu', '##issa', '##nt', '.', 'il', 'nec', '##ess', '##ite', 'beau', '##co', '##up', 'de', 'don', '##nee', '##s', '.']\n",
      "Token type IDs: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, XLMRobertaTokenizer\n",
    "\n",
    "# 1. Tokenization simple\n",
    "text = \"Le deep learning révolutionne l'IA.\"\n",
    "\n",
    "# BERT\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_output = bert_tokenizer.encode_plus(\n",
    "    text,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding='max_length',\n",
    "    max_length=32,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# XLM-RoBERTa\n",
    "xlm_tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "xlm_output = xlm_tokenizer.encode_plus(\n",
    "    text,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding='max_length',\n",
    "    max_length=32,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# 2. Tokenization de deux phrases\n",
    "text_pair = [\"L'apprentissage automatique est puissant.\", \"Il nécessite beaucoup de données.\"]\n",
    "\n",
    "bert_pair = bert_tokenizer.encode_plus(\n",
    "    text_pair[0],\n",
    "    text_pair[1],\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding='max_length',\n",
    "    max_length=32,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"1. Tokenization Simple:\")\n",
    "print(\"\\nBERT tokens:\", bert_tokenizer.tokenize(text))\n",
    "print(\"BERT input_ids:\", bert_output['input_ids'])\n",
    "print(\"BERT attention_mask:\", bert_output['attention_mask'])\n",
    "\n",
    "print(\"\\nXLM-RoBERTa tokens:\", xlm_tokenizer.tokenize(text))\n",
    "print(\"XLM-RoBERTa input_ids:\", xlm_output['input_ids'])\n",
    "print(\"XLM-RoBERTa attention_mask:\", xlm_output['attention_mask'])\n",
    "\n",
    "print(\"\\n2. Tokenization Paire de Phrases (BERT):\")\n",
    "print(\"Tokens:\", bert_tokenizer.tokenize(text_pair[0] + \" \" + text_pair[1]))\n",
    "print(\"Token type IDs:\", bert_pair['token_type_ids'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points Clés :\n",
    "\n",
    "encode_plus() retourne:\n",
    "- input_ids: IDs numériques des tokens\n",
    "- attention_mask: Masque pour tokens réels vs padding\n",
    "- token_type_ids: Identification des phrases (pour paires)\n",
    "\n",
    "\n",
    "Composants importants:\n",
    "- add_special_tokens: Ajoute [CLS], [SEP]\n",
    "- padding: Uniformise la longueur\n",
    "- max_length: Longueur maximale\n",
    "- truncation: Coupe si trop long\n",
    "- return_tensors: Format de sortie\n",
    "\n",
    "\n",
    "Différences BERT vs XLM-RoBERTa:\n",
    "- Tokenization différente\n",
    "- Gestion différente des tokens spéciaux\n",
    "- Vocabulaire différent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf_m1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Démonstration avec BERT:\n",
      "\n",
      "Informations sur le tokenizer:\n",
      "Taille du vocabulaire: 30522\n",
      "Tokens spéciaux: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "\n",
      "Structure des données préparées:\n",
      "Shape des input_ids: torch.Size([3, 32])\n",
      "Shape du masque d'attention: torch.Size([3, 32])\n",
      "\n",
      "Vérification du décodage:\n",
      "\n",
      "Texte original 1: L'intelligence artificielle transforme notre monde.\n",
      "Tokens: ['[CLS]', 'l', \"'\", 'intelligence', 'art', '##ific', '##iel', '##le', 'transform', '##e', 'notre', 'monde', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Masque d'attention: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Texte original 2: Les modèles de langage évoluent rapidement.\n",
      "Tokens: ['[CLS]', 'les', 'model', '##es', 'de', 'lang', '##age', 'ev', '##ol', '##uen', '##t', 'rapid', '##ement', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Masque d'attention: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Texte original 3: L'apprentissage profond ouvre de nouvelles possibilités.\n",
      "Tokens: ['[CLS]', 'l', \"'\", 'app', '##rent', '##issa', '##ge', 'prof', '##ond', 'ou', '##vre', 'de', 'nouvelle', '##s', 'po', '##ssi', '##bil', '##ites', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Masque d'attention: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, XLMRobertaTokenizer\n",
    "import torch\n",
    "\n",
    "# 1. Initialisation des tokenizers\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "xlm_tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# 2. Exemple de données\n",
    "textes = [\n",
    "    \"L'intelligence artificielle transforme notre monde.\",\n",
    "    \"Les modèles de langage évoluent rapidement.\",\n",
    "    \"L'apprentissage profond ouvre de nouvelles possibilités.\"  \n",
    "]\n",
    "\n",
    "# 3. Préparation des données avec encode_plus\n",
    "def preparer_donnees(texts, tokenizer, max_length=32):\n",
    "    encodages = tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        add_special_tokens=True,        # Ajoute [CLS]/[SEP] ou <s>/<\\s>\n",
    "        padding='max_length',           # Padding jusqu'à max_length\n",
    "        max_length=max_length,          # Longueur maximale\n",
    "        truncation=True,                # Tronque si nécessaire\n",
    "        return_attention_mask=True,     # Génère le masque d'attention\n",
    "        return_tensors='pt'             # Retourne des tenseurs PyTorch\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nInformations sur le tokenizer:\")\n",
    "    print(f\"Taille du vocabulaire: {tokenizer.vocab_size}\")\n",
    "    print(f\"Tokens spéciaux: {tokenizer.special_tokens_map}\")\n",
    "    \n",
    "    return encodages\n",
    "\n",
    "# 4. Démonstration avec BERT\n",
    "print(\"Démonstration avec BERT:\")\n",
    "bert_encodages = preparer_donnees(textes, bert_tokenizer)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"\\nStructure des données préparées:\")\n",
    "print(f\"Shape des input_ids: {bert_encodages['input_ids'].shape}\")\n",
    "print(f\"Shape du masque d'attention: {bert_encodages['attention_mask'].shape}\")\n",
    "\n",
    "# 5. Exemple de décodage pour vérification\n",
    "print(\"\\nVérification du décodage:\")\n",
    "for i, text in enumerate(textes):\n",
    "    print(f\"\\nTexte original {i+1}: {text}\")\n",
    "    tokens = bert_tokenizer.convert_ids_to_tokens(bert_encodages['input_ids'][i])\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Masque d'attention: {bert_encodages['attention_mask'][i].tolist()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Paramètres clés d'encode_plus():\n",
      "            Paramètre                                      Description                              Impact\n",
      "   add_special_tokens       Ajoute les tokens de début/fin de séquence   Structure correcte pour le modèle\n",
      "              padding Stratégie de padding (none, max_length, longest)        Uniformisation des longueurs\n",
      "           max_length                    Longueur maximale de séquence Contrôle de la taille des séquences\n",
      "           truncation        Activation de la troncature si nécessaire       Gestion des textes trop longs\n",
      "return_attention_mask                 Génération du masque d'attention         Gestion correcte du padding\n"
     ]
    }
   ],
   "source": [
    "# Tableau des paramètres clés\n",
    "import pandas as pd\n",
    "\n",
    "parametres = {\n",
    "    'Paramètre': [\n",
    "        'add_special_tokens',\n",
    "        'padding',\n",
    "        'max_length',\n",
    "        'truncation',\n",
    "        'return_attention_mask'\n",
    "    ],\n",
    "    'Description': [\n",
    "        'Ajoute les tokens de début/fin de séquence',\n",
    "        'Stratégie de padding (none, max_length, longest)',\n",
    "        'Longueur maximale de séquence',\n",
    "        'Activation de la troncature si nécessaire',\n",
    "        'Génération du masque d\\'attention'\n",
    "    ],\n",
    "    'Impact': [\n",
    "        'Structure correcte pour le modèle',\n",
    "        'Uniformisation des longueurs',\n",
    "        'Contrôle de la taille des séquences',\n",
    "        'Gestion des textes trop longs',\n",
    "        'Gestion correcte du padding'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_params = pd.DataFrame(parametres)\n",
    "print(\"\\nParamètres clés d'encode_plus():\")\n",
    "print(df_params.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes disponibles dans le dataset:\n",
      "['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
      "\n",
      "Aperçu des premières lignes du dataset:\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "\n",
      "Structure du dataset:\n",
      "Nombre de lignes: 891\n",
      "Nombre de colonnes: 12\n",
      "\n",
      "Informations sur les colonnes:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n",
      "\n",
      "Statistiques descriptives:\n",
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n",
      "\n",
      "Récapitulatif détaillé des colonnes:\n",
      "                 Colonne     Type  Non-Null Count  Unique Values\n",
      "PassengerId  PassengerId    int64             891            891\n",
      "Survived        Survived    int64             891              2\n",
      "Pclass            Pclass    int64             891              3\n",
      "Name                Name   object             891            891\n",
      "Sex                  Sex   object             891              2\n",
      "Age                  Age  float64             714             88\n",
      "SibSp              SibSp    int64             891              7\n",
      "Parch              Parch    int64             891              7\n",
      "Ticket            Ticket   object             891            681\n",
      "Fare                Fare  float64             891            248\n",
      "Cabin              Cabin   object             204            147\n",
      "Embarked        Embarked   object             889              3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Chargement du dataset\n",
    "df = pd.read_csv('/Users/ludovicveltz/Documents/Bootcamp_GENAI_2025/Crashcourse/WEEK_2/DAY_2/DATASET/train.csv')\n",
    "\n",
    "# 2. Affichage des colonnes disponibles\n",
    "print(\"Colonnes disponibles dans le dataset:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# 3. Affichage des premières lignes\n",
    "print(\"\\nAperçu des premières lignes du dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# 4. Affichage de la structure du dataset\n",
    "print(\"\\nStructure du dataset:\")\n",
    "print(f\"Nombre de lignes: {df.shape[0]}\")\n",
    "print(f\"Nombre de colonnes: {df.shape[1]}\")\n",
    "\n",
    "# 5. Information sur les colonnes\n",
    "print(\"\\nInformations sur les colonnes:\")\n",
    "print(df.info())\n",
    "\n",
    "# 6. Statistiques descriptives\n",
    "print(\"\\nStatistiques descriptives:\")\n",
    "print(df.describe())\n",
    "\n",
    "# 7. Création d'un tableau récapitulatif des colonnes basé sur les colonnes réellement présentes\n",
    "colonnes_info = pd.DataFrame({\n",
    "    'Colonne': df.columns,\n",
    "    'Type': df.dtypes,\n",
    "    'Non-Null Count': df.count(),\n",
    "    'Unique Values': [df[col].nunique() for col in df.columns]\n",
    "})\n",
    "\n",
    "print(\"\\nRécapitulatif détaillé des colonnes:\")\n",
    "print(colonnes_info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure du Dataset Titanic pour Classification:\n",
      "\n",
      "Colonnes principales pour l'entraînement:\n",
      "    Feature                                   Description     Type  \\\n",
      "0    Pclass     Classe du passager (1,2,3) - catégorielle    int64   \n",
      "1       Sex              Genre du passager - catégorielle   object   \n",
      "2       Age                   Âge du passager - numérique  float64   \n",
      "3     SibSp   Nombre de siblings/époux à bord - numérique    int64   \n",
      "4     Parch  Nombre de parents/enfants à bord - numérique    int64   \n",
      "5      Fare                    Prix du billet - numérique  float64   \n",
      "6  Embarked    Port d'embarquement (C,Q,S) - catégorielle   object   \n",
      "\n",
      "   Valeurs Manquantes  \n",
      "0                   0  \n",
      "1                   0  \n",
      "2                 177  \n",
      "3                   0  \n",
      "4                   0  \n",
      "5                   0  \n",
      "6                   2  \n",
      "\n",
      "Variable Cible:\n",
      "Survived (0: Non survécu, 1: Survécu)\n",
      "\n",
      "Distribution de la variable cible:\n",
      "Survived\n",
      "0    549\n",
      "1    342\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Pourcentage de survie:\n",
      "Survived\n",
      "0    61.6\n",
      "1    38.4\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Aperçu des données numériques:\n",
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n",
      "\n",
      "Points clés pour la préparation du modèle:\n",
      "1. Nombre total d'observations: 891\n",
      "2. Nombre de features: 7\n",
      "3. Traitement nécessaire:\n",
      "   - Encodage des variables catégorielles (Sex, Embarked)\n",
      "   - Gestion des valeurs manquantes (Age, Embarked)\n",
      "   - Normalisation des variables numériques (Age, Fare)\n",
      "   - Possible feature engineering (ex: extraction titre du Name)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Chargement et exploration du dataset Titanic\n",
    "df = pd.read_csv('/Users/ludovicveltz/Documents/Bootcamp_GENAI_2025/Crashcourse/WEEK_2/DAY_2/DATASET/train.csv')\n",
    "\n",
    "# 2. Structure des données pour la classification\n",
    "print(\"Structure du Dataset Titanic pour Classification:\")\n",
    "print(\"\\nColonnes principales pour l'entraînement:\")\n",
    "features_info = {\n",
    "    'Feature': [\n",
    "        'Pclass',\n",
    "        'Sex',\n",
    "        'Age',\n",
    "        'SibSp',\n",
    "        'Parch',\n",
    "        'Fare',\n",
    "        'Embarked'\n",
    "    ],\n",
    "    'Description': [\n",
    "        'Classe du passager (1,2,3) - catégorielle',\n",
    "        'Genre du passager - catégorielle',\n",
    "        'Âge du passager - numérique',\n",
    "        'Nombre de siblings/époux à bord - numérique',\n",
    "        'Nombre de parents/enfants à bord - numérique',\n",
    "        'Prix du billet - numérique',\n",
    "        'Port d\\'embarquement (C,Q,S) - catégorielle'\n",
    "    ],\n",
    "    'Type': [\n",
    "        'int64',\n",
    "        'object',\n",
    "        'float64',\n",
    "        'int64',\n",
    "        'int64',\n",
    "        'float64',\n",
    "        'object'\n",
    "    ],\n",
    "    'Valeurs Manquantes': [\n",
    "        df['Pclass'].isnull().sum(),\n",
    "        df['Sex'].isnull().sum(),\n",
    "        df['Age'].isnull().sum(),\n",
    "        df['SibSp'].isnull().sum(),\n",
    "        df['Parch'].isnull().sum(),\n",
    "        df['Fare'].isnull().sum(),\n",
    "        df['Embarked'].isnull().sum()\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_features = pd.DataFrame(features_info)\n",
    "print(df_features)\n",
    "\n",
    "print(\"\\nVariable Cible:\")\n",
    "print(\"Survived (0: Non survécu, 1: Survécu)\")\n",
    "print(\"\\nDistribution de la variable cible:\")\n",
    "print(df['Survived'].value_counts())\n",
    "print(\"\\nPourcentage de survie:\")\n",
    "print(df['Survived'].value_counts(normalize=True).round(3) * 100)\n",
    "\n",
    "print(\"\\nAperçu des données numériques:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Points clés pour la préparation du modèle\n",
    "print(\"\\nPoints clés pour la préparation du modèle:\")\n",
    "print(f\"1. Nombre total d'observations: {df.shape[0]}\")\n",
    "print(f\"2. Nombre de features: {len(features_info['Feature'])}\")\n",
    "print(\"3. Traitement nécessaire:\")\n",
    "print(\"   - Encodage des variables catégorielles (Sex, Embarked)\")\n",
    "print(\"   - Gestion des valeurs manquantes (Age, Embarked)\")\n",
    "print(\"   - Normalisation des variables numériques (Age, Fare)\")\n",
    "print(\"   - Possible feature engineering (ex: extraction titre du Name)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Encodage des variables catégorielles:\n",
      "   - Sex encodé (male=1, female=0)\n",
      "   - Embarked one-hot encodé\n",
      "\n",
      "2. Gestion des valeurs manquantes:\n",
      "   - Age: 0 valeurs manquantes restantes\n",
      "   - Embarked: 0 valeurs manquantes restantes\n",
      "\n",
      "3. Normalisation des variables numériques:\n",
      "   - Age et Fare normalisés\n",
      "\n",
      "4. Feature engineering:\n",
      "   - Titres extraits des noms\n",
      "   - Feature FamilySize créée\n",
      "\n",
      "Dataset prêt pour l'entraînement!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Chargement du dataset Titanic\n",
    "df = pd.read_csv('/Users/ludovicveltz/Documents/Bootcamp_GENAI_2025/Crashcourse/WEEK_2/DAY_2/DATASET/train.csv')\n",
    "\n",
    "# 1. Encodage des variables catégorielles\n",
    "print(\"1. Encodage des variables catégorielles:\")\n",
    "# Sex\n",
    "le = LabelEncoder()\n",
    "df['Sex'] = le.fit_transform(df['Sex'])\n",
    "print(\"   - Sex encodé (male=1, female=0)\")\n",
    "\n",
    "# Embarked\n",
    "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])  # Remplir d'abord les valeurs manquantes\n",
    "df_embarked = pd.get_dummies(df['Embarked'], prefix='Embarked')\n",
    "print(\"   - Embarked one-hot encodé\")\n",
    "\n",
    "# 2. Gestion des valeurs manquantes\n",
    "print(\"\\n2. Gestion des valeurs manquantes:\")\n",
    "# Age\n",
    "age_median = df['Age'].median()\n",
    "df['Age'] = df['Age'].fillna(age_median)\n",
    "print(f\"   - Age: {df['Age'].isnull().sum()} valeurs manquantes restantes\")\n",
    "print(f\"   - Embarked: {df['Embarked'].isnull().sum()} valeurs manquantes restantes\")\n",
    "\n",
    "# 3. Normalisation des variables numériques\n",
    "print(\"\\n3. Normalisation des variables numériques:\")\n",
    "scaler = StandardScaler()\n",
    "df[['Age', 'Fare']] = scaler.fit_transform(df[['Age', 'Fare']])\n",
    "print(\"   - Age et Fare normalisés\")\n",
    "\n",
    "# 4. Feature engineering\n",
    "print(\"\\n4. Feature engineering:\")\n",
    "# Extraction du titre\n",
    "df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "print(\"   - Titres extraits des noms\")\n",
    "\n",
    "# Feature familiale\n",
    "df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "print(\"   - Feature FamilySize créée\")\n",
    "\n",
    "print(\"\\nDataset prêt pour l'entraînement!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information sur la validation croisée stratifiée:\n",
      "Nombre de folds: 5\n",
      "\n",
      "Distribution des classes dans chaque fold:\n",
      "\n",
      "Fold 1:\n",
      "Train set:\n",
      "Survived\n",
      "0    0.617\n",
      "1    0.383\n",
      "Name: proportion, dtype: float64\n",
      "Validation set:\n",
      "Survived\n",
      "0    0.615\n",
      "1    0.385\n",
      "Name: proportion, dtype: float64\n",
      "Taille train: 712, Taille validation: 179\n",
      "\n",
      "Fold 2:\n",
      "Train set:\n",
      "Survived\n",
      "0    0.616\n",
      "1    0.384\n",
      "Name: proportion, dtype: float64\n",
      "Validation set:\n",
      "Survived\n",
      "0    0.618\n",
      "1    0.382\n",
      "Name: proportion, dtype: float64\n",
      "Taille train: 713, Taille validation: 178\n",
      "\n",
      "Fold 3:\n",
      "Train set:\n",
      "Survived\n",
      "0    0.616\n",
      "1    0.384\n",
      "Name: proportion, dtype: float64\n",
      "Validation set:\n",
      "Survived\n",
      "0    0.618\n",
      "1    0.382\n",
      "Name: proportion, dtype: float64\n",
      "Taille train: 713, Taille validation: 178\n",
      "\n",
      "Fold 4:\n",
      "Train set:\n",
      "Survived\n",
      "0    0.616\n",
      "1    0.384\n",
      "Name: proportion, dtype: float64\n",
      "Validation set:\n",
      "Survived\n",
      "0    0.618\n",
      "1    0.382\n",
      "Name: proportion, dtype: float64\n",
      "Taille train: 713, Taille validation: 178\n",
      "\n",
      "Fold 5:\n",
      "Train set:\n",
      "Survived\n",
      "0    0.617\n",
      "1    0.383\n",
      "Name: proportion, dtype: float64\n",
      "Validation set:\n",
      "Survived\n",
      "0    0.612\n",
      "1    0.388\n",
      "Name: proportion, dtype: float64\n",
      "Taille train: 713, Taille validation: 178\n",
      "\n",
      "Vérification de la stratification:\n",
      "Distribution globale des classes:\n",
      "Survived\n",
      "0    0.616\n",
      "1    0.384\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 1. Préparation des données\n",
    "df = pd.read_csv('/Users/ludovicveltz/Documents/Bootcamp_GENAI_2025/Crashcourse/WEEK_2/DAY_2/DATASET/train.csv')\n",
    "\n",
    "# Sélection des features après prétraitement\n",
    "X = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']].copy()\n",
    "y = df['Survived']\n",
    "\n",
    "# Application du prétraitement basique pour l'exemple\n",
    "# Encodage de Sex et Embarked\n",
    "X['Sex'] = (X['Sex'] == 'female').astype(int)\n",
    "X['Embarked'] = X['Embarked'].fillna('S')\n",
    "X = pd.get_dummies(X, columns=['Embarked'])\n",
    "\n",
    "# Gestion des valeurs manquantes de Age\n",
    "X['Age'] = X['Age'].fillna(X['Age'].median())\n",
    "\n",
    "# 2. Création des folds avec StratifiedKFold\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# 3. Création des listes pour stocker les indices\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "\n",
    "# 4. Split et stockage des indices\n",
    "for train_idx, val_idx in skf.split(X, y):\n",
    "    train_indices.append(train_idx)\n",
    "    val_indices.append(val_idx)\n",
    "\n",
    "# 5. Affichage des informations sur les folds\n",
    "print(\"Information sur la validation croisée stratifiée:\")\n",
    "print(f\"Nombre de folds: {n_splits}\")\n",
    "print(\"\\nDistribution des classes dans chaque fold:\")\n",
    "\n",
    "for fold in range(n_splits):\n",
    "    print(f\"\\nFold {fold + 1}:\")\n",
    "    print(\"Train set:\")\n",
    "    print(y.iloc[train_indices[fold]].value_counts(normalize=True).round(3))\n",
    "    print(\"Validation set:\")\n",
    "    print(y.iloc[val_indices[fold]].value_counts(normalize=True).round(3))\n",
    "    print(f\"Taille train: {len(train_indices[fold])}, Taille validation: {len(val_indices[fold])}\")\n",
    "\n",
    "# 6. Vérification de la stratification\n",
    "print(\"\\nVérification de la stratification:\")\n",
    "print(\"Distribution globale des classes:\")\n",
    "print(y.value_counts(normalize=True).round(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
