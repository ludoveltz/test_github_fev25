{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédiction initiale: 36.4\n",
      "Perte: 1180.98\n",
      "Poids mis à jour: [ 2.544 39.18 ]\n",
      "Biais mis à jour: 10.486\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Initialisation des données\n",
    "x = np.array([4, 80])  # heures étudiées et score précédent\n",
    "w = np.array([0.6, 0.3])  # poids initiaux\n",
    "b = 10  # biais initial\n",
    "\n",
    "# 2. Propagation avant\n",
    "def forward_propagation(x, w, b):\n",
    "    z = np.dot(x, w) + b\n",
    "    return z\n",
    "\n",
    "# 3. Calcul de la prédiction\n",
    "y_pred = forward_propagation(x, w, b)\n",
    "y_true = 85\n",
    "\n",
    "# Détaillons les calculs :\n",
    "# y_pred = (4 * 0.6) + (80 * 0.3) + 10\n",
    "# y_pred = 2.4 + 24 + 10\n",
    "# y_pred = 36.4\n",
    "\n",
    "# 4. Calcul de la perte (MSE)\n",
    "loss = 0.5 * (y_true - y_pred) ** 2\n",
    "# loss = 0.5 * (85 - 36.4)² = 1182.98\n",
    "\n",
    "# 5. Calcul des gradients\n",
    "grad_w = -(y_true - y_pred) * x\n",
    "grad_b = -(y_true - y_pred)\n",
    "\n",
    "# 6. Mise à jour des poids et du biais\n",
    "learning_rate = 0.01\n",
    "w_new = w - learning_rate * grad_w\n",
    "b_new = b - learning_rate * grad_b\n",
    "\n",
    "# 7. Affichage des résultats\n",
    "print(\"Prédiction initiale:\", y_pred)\n",
    "print(\"Perte:\", loss)\n",
    "print(\"Poids mis à jour:\", w_new)\n",
    "print(\"Biais mis à jour:\", b_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explication du processus de descente de gradient :\n",
    "1. La prédiction initiale (36.4) est loin de la valeur réelle (85)\n",
    "2. Les gradients indiquent la direction de la plus forte augmentation de l'erreur\n",
    "3. En soustrayant les gradients (multipliés par le learning rate), nous :\n",
    "- Augmentons les poids si la prédiction est trop basse\n",
    "- Diminuons les poids si la prédiction est trop haute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cas 1 : Learning Rate faible (0.001)\n",
    "learning_rate = 0.001\n",
    "# Les changements sont plus lents mais plus stables\n",
    "# Exemple : w_new ≈ w - 0.001 * grad_w\n",
    "# Changements très graduels\n",
    "\n",
    "# Cas 2 : Learning Rate élevé (0.1)\n",
    "learning_rate = 0.1\n",
    "# Les changements sont plus rapides mais risqués\n",
    "# Exemple : w_new ≈ w - 0.1 * grad_w\n",
    "# Peut \"sauter\" la solution optimale\n",
    "\n",
    "\n",
    "# Cas 1 : Poids faibles\n",
    "w = np.array([0.1, 0.1])\n",
    "# Prédiction initiale plus basse\n",
    "# Gradients plus importants au début\n",
    "# Apprentissage plus progressif\n",
    "\n",
    "# Cas 2 : Poids élevés\n",
    "w = np.array([1.0, 1.0])\n",
    "# Prédiction initiale plus élevée\n",
    "# Risque de saturation\n",
    "# Gradients potentiellement plus grands\n",
    "\n",
    "Effets sur l'apprentissage :\n",
    "\n",
    "Learning Rate :\n",
    "Trop petit : apprentissage très lent\n",
    "Trop grand : instabilité, dépassement de l'optimum\n",
    "Optimal : convergence rapide et stable\n",
    "\n",
    "Poids Initiaux :\n",
    "Trop petits : apprentissage lent au début\n",
    "Trop grands : risque de surapprentissage initial\n",
    "Bien choisis : convergence plus rapide vers la solution\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
